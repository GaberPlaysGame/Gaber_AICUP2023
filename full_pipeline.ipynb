{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline\n",
        "python: 3.8.*\n",
        "\n",
        "use ```Ctrl + ]``` to collapse all section :)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download our starter pack (3~5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown 1Xq2Fv6UGA1pc25pF0qwEc_l7Fa5jPP6p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown --folder 1T6jpOtdf_i6XNYA6F_lqU4mRRh1xYPcl\n",
        "!mv baseline/* ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown --folder 1hnVYEgN-gYzFCeBZo8cbKjGLBP-YTnTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MSEcenjVrL"
      },
      "source": [
        "## PART 1. Document retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the environment and import all library we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niqu9pLajYC_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "from functools import partial\n",
        "\n",
        "# 3rd party libs\n",
        "import hanlp\n",
        "import opencc\n",
        "import pandas as pd\n",
        "from hanlp.components.pipeline import Pipeline\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# our own libs\n",
        "from utils import load_json\n",
        "from hw3_utils import jsonl_dir_to_df\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from TCSP import read_stopwords_list\n",
        "\n",
        "stopwords = read_stopwords_list()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_test_private = \"data/private_test_data.jsonl\"\n",
        "file_train_0316 = \"data/public_train_0316.jsonl\"\n",
        "file_train_0522 = \"data/public_train_0522.jsonl\"\n",
        "\n",
        "TRAIN_DATA_1 = load_json(file_train_0316)\n",
        "TRAIN_DATA_2 = load_json(file_train_0522)\n",
        "# TEST_DATA_PUBLIC = load_json(\"data/public_test.jsonl\")\n",
        "TEST_DATA_PRIVATE = load_json(\"data/private_test_data.jsonl\")\n",
        "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data class for type hinting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Claim:\n",
        "    data: str\n",
        "\n",
        "@dataclass\n",
        "class AnnotationID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class EvidenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class PageTitle:\n",
        "    title: str\n",
        "\n",
        "@dataclass\n",
        "class SentenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class Evidence:\n",
        "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3NU01DnjKp-"
      },
      "outputs": [],
      "source": [
        "def do_st_corrections(text: str) -> str:\n",
        "    simplified = CONVERTER_T2S.convert(text)\n",
        "\n",
        "    return CONVERTER_S2T.convert(simplified)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_nps_hanlp(\n",
        "    predictor: Pipeline,\n",
        "    d: Dict[str, Union[int, Claim, Evidence]],\n",
        ") -> List[str]:\n",
        "    claim = d[\"claim\"]\n",
        "    tree = predictor(claim)[\"con\"]\n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "\n",
        "    return nps"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_precision(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> float:\n",
        "    precision = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        # Extract all ground truth of titles of the wikipedia pages\n",
        "        # evidence[2] refers to the title of the wikipedia page\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        if len(predicted_pages) != 0:\n",
        "            precision += len(hits) / len(predicted_pages)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Macro precision\n",
        "    print(f\"Precision: {precision / count}\")\n",
        "    return precision / count\n",
        "\n",
        "\n",
        "def calculate_recall(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> float:\n",
        "    recall = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        recall += len(hits) / len(gt_pages)\n",
        "        count += 1\n",
        "\n",
        "    print(f\"Recall: {recall / count}\")\n",
        "    return recall / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_f1(precision: float, recall: float) -> float:\n",
        "    f1 = 2*(precision*recall)/(precision+recall)\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "    return f1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_doc(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    suffix: str = \"\",\n",
        "    num_pred_doc: int = 5,\n",
        "    col_name = \"predicted_pages\"\n",
        ") -> None:\n",
        "    with open(\n",
        "        f\"data/{mode}_doc{num_pred_doc}{suffix}.jsonl\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        for i, d in enumerate(data):\n",
        "            d[col_name] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jieba\n",
        "jieba.set_dictionary('data/jieba_dict/dict.txt.big')\n",
        "jieba.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text: str, stopwords: list) -> str:\n",
        "    tokens = jieba.cut(text)\n",
        "\n",
        "    return \" \".join([w for w in tokens if w not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_path = \"data/wiki-pages\"\n",
        "min_wiki_length = 10\n",
        "topk = 50\n",
        "min_df = 1\n",
        "max_df = 0.8\n",
        "use_idf = True\n",
        "sublinear_tf = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_cache = \"wiki\"\n",
        "target_column = \"text\"\n",
        "\n",
        "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
        "if wiki_cache_path.exists():\n",
        "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
        "else:\n",
        "    def text_split(line: str) -> list:\n",
        "        import re\n",
        "        line = re.sub(r\"[0-9]+\\t\", \"\", line)\n",
        "        lines = line.split(\"\\n\")\n",
        "        lines = list(filter(None, lines))\n",
        "        return lines\n",
        "    # You need to download `wiki-pages.zip` from the AICUP website\n",
        "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
        "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
        "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
        "\n",
        "    # tokenize the text and keep the result in a new column `processed_text`\n",
        "    wiki_pages[\"lines\"] = wiki_pages[\"lines\"].parallel_apply(text_split)\n",
        "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
        "        partial(tokenize, stopwords=stopwords)\n",
        "    )\n",
        "    # save the result to a pickle file\n",
        "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_pages = wiki_pages[\n",
        "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tfidf Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = wiki_pages[\"processed_text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gensim.models\n",
        "# w2vmodel = gensim.models.Word2Vec.load(\"models/w2v.zh.300/word2vec.model\")\n",
        "# w2v = dict(zip(w2vmodel.wv.index_to_key, w2vmodel.wv.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from collections import defaultdict\n",
        "# class TfidfEmbeddingVectorizer(object):\n",
        "#     def __init__(self, word2vec, size=300):\n",
        "#         self.word2vec = word2vec\n",
        "#         self.word2weight = None\n",
        "#         self.dim = size\n",
        "    \n",
        "#     def fit(self, X):\n",
        "#         tfidf = TfidfVectorizer(\n",
        "#             min_df=min_df,\n",
        "#             max_df=max_df,\n",
        "#             use_idf=use_idf,\n",
        "#             sublinear_tf=sublinear_tf,\n",
        "#             dtype=np.float64,\n",
        "#             analyzer=lambda x: x\n",
        "#         )\n",
        "#         tfidf.fit(X)\n",
        "#         # if a word was never seen - it must be at least as infrequent\n",
        "#         # as any of the known words - so the default idf is the max of \n",
        "#         # known idf's\n",
        "#         max_idf = max(tfidf.idf_)\n",
        "#         self.word2weight = defaultdict(\n",
        "#             lambda: max_idf,\n",
        "#             [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         return np.array([\n",
        "#                 np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "#                          for w in words if w in self.word2vec] or\n",
        "#                         [np.zeros(self.dim)], axis=0)\n",
        "#                 for words in X\n",
        "#             ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vectorizer = TfidfEmbeddingVectorizer(w2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=min_df,\n",
        "    max_df=max_df,\n",
        "    use_idf=use_idf,\n",
        "    sublinear_tf=sublinear_tf,\n",
        "    # dtype=np.float64,\n",
        "    ngram_range=(1,2),\n",
        "    # norm=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = vectorizer.fit_transform(corpus)\n",
        "# X = vectorizer.fit(corpus).transform(corpus)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentence BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "sbert_model = SentenceTransformer('uer/sbert-base-chinese-nli', device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool = sbert_model.start_multi_process_pool()\n",
        "print(pool)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for document retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def get_pred_pages(\n",
        "#         series_data: pd.Series, \n",
        "#         ) -> Set[Dict[int, str]]:\n",
        "#     import wikipedia\n",
        "#     import re\n",
        "#     import opencc\n",
        "#     import pandas as pd\n",
        "\n",
        "#     from TCSP import read_stopwords_list\n",
        "#     stopwords = read_stopwords_list()\n",
        "\n",
        "#     import numpy as np\n",
        "#     from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#     from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#     wikipedia.set_lang(\"zh\")\n",
        "#     CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "#     CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "#     def do_st_corrections(text: str) -> str:\n",
        "#         simplified = CONVERTER_T2S.convert(text)\n",
        "#         return CONVERTER_S2T.convert(simplified)\n",
        "\n",
        "#     results = []\n",
        "#     tmp_muji = []\n",
        "#     # wiki_page: its index showned in claim\n",
        "#     mapping = {}\n",
        "#     claim = series_data[\"claim\"]\n",
        "#     nps = series_data[\"hanlp_results\"]\n",
        "#     first_wiki_term = []\n",
        "#     repeated_mention = []\n",
        "#     quote_search = []\n",
        "\n",
        "#     def clean_claim(claim) -> str:     # Clean claim function because hanlp has error when conducting cons\n",
        "#         def multiple_replacer(*kv):\n",
        "#             replace_dict = dict(kv)\n",
        "#             replace_func = lambda match: replace_dict[match.group(0)]\n",
        "#             pattern = re.compile(\"|\".join([re.escape(k) for k, v in kv]), re.M)\n",
        "#             return lambda string: pattern.sub(replace_func, string) \n",
        "#         def multiple_replace(string, *kv):\n",
        "#             return multiple_replacer(*replace_dict)(claim)\n",
        "\n",
        "#         replace_dict = (\" \", \"\"), (\"牠\", \"它\"), (\"（\", \"(\"), (\"）\", \")\"), (\"，\", \",\"), (\"、\", \",\"), (\"群\", \"羣\"), (\"“\", \"\\\"\"), (\"”\", \"\\\"\"), (\"「\", \"“\"), (\"」\", \"”\")\n",
        "#         claim = multiple_replace(claim, *replace_dict)\n",
        "#         claim = claim.lower()\n",
        "#         return claim\n",
        "\n",
        "#     claim = clean_claim(claim)\n",
        "\n",
        "#     def post_processing(np, page, loc):\n",
        "#         page = do_st_corrections(page)\n",
        "#         page = page.replace(\" \", \"_\")\n",
        "#         page = page.replace(\"-\", \"\")\n",
        "#         search_pos = claim.find(np)\n",
        "#         if search_pos != -1:\n",
        "#             if page in results:\n",
        "#                 repeated_mention.append(page)\n",
        "#                 # results.insert(0, results.pop(results.index(page)))     # Fresh page to front if it was mention before\n",
        "#             else:\n",
        "#                 results.append(page)\n",
        "#             if loc == 0:\n",
        "#                 pass\n",
        "#                 # print(f\"Add: {page}, at page direct search, np={np}\")\n",
        "#             elif loc == 1:\n",
        "#                 pass\n",
        "#                 # print(f\"Add: {page}, at match, new term={np}\")\n",
        "#             mapping[page] = search_pos\n",
        "#             tmp_muji.append(np)\n",
        "\n",
        "#     def if_page_exists(page: str) -> bool:\n",
        "#         import requests\n",
        "#         url_base = \"https://zh.wikipedia.org/wiki/\"\n",
        "#         new_url = [url_base + page, url_base + page.upper()]\n",
        "#         for url in new_url:\n",
        "#             r = requests.head(url)\n",
        "#             if r.status_code == 200:\n",
        "#                 return True\n",
        "#             else:\n",
        "#                 continue\n",
        "#         return False\n",
        "    \n",
        "#     def clean_time_format(np: str):\n",
        "#         if (matched := re.search(r\"\\d+年\", np)) != None:\n",
        "#             return True\n",
        "#         if (matched := re.search(r\"\\d+月\\d+日\", np)) != None:\n",
        "#             return True\n",
        "#         if (matched := re.search(r\"\\d+小時\", np)) != None:\n",
        "#             return True\n",
        "#         if (matched := re.search(r\"\\d+天\", np)) != None:\n",
        "#             return True\n",
        "#         if (matched := re.search(r\"\\d+世紀\", np)) != None:\n",
        "#             return True\n",
        "#         if (matched := re.search(r\"\\d+年代\", np)) != None:\n",
        "#             return True\n",
        "#         return False\n",
        "    \n",
        "#     def tokenize(text: str, stopwords: list) -> str:\n",
        "#         import jieba\n",
        "#         \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
        "\n",
        "#         Args:\n",
        "#             text (str): claim or wikipedia article\n",
        "#             stopwords (list): common words that contribute little to the meaning of a sentence\n",
        "\n",
        "#         Returns:\n",
        "#             str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
        "#         \"\"\"\n",
        "\n",
        "#         tokens = jieba.cut(text)\n",
        "\n",
        "#         return \" \".join([w for w in tokens if w not in stopwords])\n",
        "\n",
        "#     for i, np in enumerate(nps):\n",
        "#         # print(f\"searching {np}\")\n",
        "#         quote_dup = False\n",
        "#         if np in stopwords:         # 如果包含停用詞\n",
        "#             continue\n",
        "#         if clean_time_format(np):   # 如果包含時間\n",
        "#             continue\n",
        "        \n",
        "#         # Ignore parsing among quotation mark, for example, if《仲夏夜之夢》exists, ignore「仲夏夜」and「夢」\n",
        "#         for search in quote_search:\n",
        "#             if search.find(np) != -1:\n",
        "#                 quote_dup = True\n",
        "#         if quote_dup == True:\n",
        "#             continue\n",
        "\n",
        "#         # Delete Bookname Mark, Quote Mark\n",
        "#         np_no_quote = re.sub(r\"《|》|〈|〉|【|】|「|」|『|』|（|）\", \"\", np)\n",
        "#         if np != np_no_quote:\n",
        "#             quote_search.append(np_no_quote)\n",
        "#             np = np_no_quote\n",
        "\n",
        "#         # Simplified Traditional Chinese Correction\n",
        "#         wiki_search_results = [\n",
        "#             do_st_corrections(w) for w in wikipedia.search(np)\n",
        "#         ]\n",
        "\n",
        "#         # Directly Search by Redirection\n",
        "#             # Check if a page exists\n",
        "#         if (if_page_exists(np)):\n",
        "#             try:\n",
        "#                 page = do_st_corrections(wikipedia.page(title=np).title)\n",
        "#                 if page == np:\n",
        "#                     # print(f\"Found, np={np}, page={page}\")\n",
        "#                     post_processing(np=np, page=page, loc=0)\n",
        "#                 else:\n",
        "#                     # print(f\"Redirect, np={np}, page={page}\")\n",
        "#                     post_processing(np=np, page=page, loc=0)\n",
        "#             except wikipedia.DisambiguationError as diserr:\n",
        "#                 page = do_st_corrections(wikipedia.search(np)[0])\n",
        "#                 if page == np:\n",
        "#                     # print(f\"Disambig, np={np}, page={page}\")\n",
        "#                     post_processing(np=np, page=page, loc=0)\n",
        "#             except wikipedia.PageError as pageerr:\n",
        "#                 pass\n",
        "\n",
        "#         # Remove the wiki page's description in brackets\n",
        "#         wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "#         wiki_df = pd.DataFrame({\n",
        "#             \"wiki_set\": wiki_set,\n",
        "#             \"wiki_results\": wiki_search_results\n",
        "#         })\n",
        "\n",
        "#         # Elements in wiki_set --> index\n",
        "#         # Extracting only the first element is one way to avoid extracting\n",
        "#         # too many of the similar wiki pages\n",
        "#         grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
        "#         candidates = grouped_df[\"wiki_results\"].tolist()\n",
        "#         # muji refers to wiki_set\n",
        "#         muji = grouped_df.index.tolist()\n",
        "\n",
        "#         for prefix, term in zip(muji, candidates):\n",
        "#             if prefix not in tmp_muji:  #忽略掉括號，如果括號有重複的話。假設如果有\" 1 (數字)\", 則\"1 (符號)\" 會被忽略\n",
        "#                 matched = False\n",
        "\n",
        "#                 # Take at least one term from the first noun phrase\n",
        "#                 if i == 0:\n",
        "#                     first_wiki_term.append(term)\n",
        "\n",
        "#                 # try:\n",
        "#                 #     print(term)\n",
        "#                 #     term_idx = wiki_pages.index[wiki_pages['id'] == do_st_corrections(term.replace(\" \", \"_\").replace(\"-\", \"\"))].tolist()[0]\n",
        "#                 #     processed_tokens = wiki_pages['processed_text'][term_idx]\n",
        "#                 #     processed_text_vector = vectorizer.transform([processed_tokens])\n",
        "#                 #     sim_score = cosine_similarity(processed_text_vector, claim_vector)[0][0]\n",
        "#                 #     if sim_score > 0.25: # 0.25 is hyperparam\n",
        "#                 #         score_mapping[term] = sim_score\n",
        "#                 #         print(sim_score, term)\n",
        "#                 # except IndexError:\n",
        "#                 #     pass\n",
        "#                 # except wikipedia.DisambiguationError:\n",
        "#                 #     pass\n",
        "#                 # except wikipedia.PageError:\n",
        "#                 #     pass\n",
        "\n",
        "#                 # Walrus operator :=\n",
        "#                 # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
        "#                 # Through these filters, we are trying to figure out if the term\n",
        "#                 # is within the claim\n",
        "#                 if (((new_term := term) in claim) or\n",
        "#                     ((new_term := term) in claim.replace(\" \", \"\")) or\n",
        "#                     ((new_term := term.replace(\"·\", \"\")) in claim) or                                   # 過濾人名\n",
        "#                     ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term)) in claim) or                          # 過濾空格 / 消歧義\n",
        "#                     ((new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[0]) in claim and       # 消歧義與括號內皆有在裡面\n",
        "#                      (new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[1]) in claim) or\n",
        "#                     ((new_term := term.replace(\"-\", \" \")) in claim) or                                  # 過濾槓號\n",
        "#                     ((new_term := term.lower()) in claim) or                                            # 過濾大小寫\n",
        "#                     ((new_term := term.lower().replace(\"-\", \"\")) in claim) or                           # 過濾大小寫及槓號\n",
        "#                     ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term.lower().replace(\"-\", \"\"))) in claim)    # 過濾大小寫、槓號及消歧義\n",
        "#                     ):\n",
        "#                     matched = True\n",
        "#                     # print(new_term, term)\n",
        "\n",
        "#                 # 人名匹配\n",
        "#                 elif \"·\" in term:\n",
        "#                     splitted = term.split(\"·\")\n",
        "#                     if \"·\" not in claim:        # 要求claim顯示的不為全名，不然都需要全名\n",
        "#                         for split in splitted:\n",
        "#                             if (new_term := split) in claim:\n",
        "#                                 matched = True\n",
        "#                                 break\n",
        "\n",
        "#                 if matched:\n",
        "#                     post_processing(np=new_term, page=term, loc=1)\n",
        "\n",
        "#     # score_results = sorted(score_mapping, key=score_mapping.get)[:-5]\n",
        "\n",
        "#     # 8 is a hyperparameter\n",
        "#     if len(results) > 8:\n",
        "#         assert -1 not in mapping.values()\n",
        "#         # print(\"長度大於8\", results)\n",
        "\n",
        "#         results = repeated_mention + sorted(mapping, key=mapping.get)[:8]\n",
        "#         results = list(set(results))            # remove duplicates\n",
        "#         # print(\"排序後\", results)\n",
        "#     if len(results) < 1:\n",
        "#         results = first_wiki_term\n",
        "#         # print(\"第一搜尋結果\", results)\n",
        "    \n",
        "#     print(results)\n",
        "#     return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_search(\n",
        "        series_data: pd.Series, \n",
        "        ):\n",
        "    import wikipedia\n",
        "    import re\n",
        "    import opencc\n",
        "    import pandas as pd\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    wikipedia.set_lang(\"zh\")\n",
        "    CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "    CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "    def do_st_corrections(text: str) -> str:\n",
        "        simplified = CONVERTER_T2S.convert(text)\n",
        "        return CONVERTER_S2T.convert(simplified)\n",
        "    \n",
        "    def if_page_exists(page: str) -> bool:\n",
        "        import requests\n",
        "        url_base = \"https://zh.wikipedia.org/wiki/\"\n",
        "        new_url = [url_base + page, url_base + page.upper()]\n",
        "        for url in new_url:\n",
        "            r = requests.head(url)\n",
        "            if r.status_code == 200:\n",
        "                return True\n",
        "            else:\n",
        "                continue\n",
        "        return False\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    results = []\n",
        "    direct_results = []\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "    nps.append(claim)\n",
        "\n",
        "    def post_processing(page):\n",
        "        page = do_st_corrections(page)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "\n",
        "    for i, np in enumerate(nps):\n",
        "        # print(f\"searching {np}\")\n",
        "\n",
        "        if (if_page_exists(np)):\n",
        "            try:\n",
        "                page = do_st_corrections(wikipedia.page(title=np).title)\n",
        "                if page == np:\n",
        "                    # print(f\"Found, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "                else:\n",
        "                    # print(f\"Redirect, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "            except wikipedia.DisambiguationError as diserr:\n",
        "                for option in diserr.options:\n",
        "                    option = do_st_corrections(option)\n",
        "                    if new_option := re.sub(r\"\\s\\(\\S+\\)\", \"\", option) in claim:\n",
        "                        # print(f\"Disambig, np={np}, page={option}, claim={claim}\")\n",
        "                        post_processing(option)\n",
        "                        direct_results.append(option)\n",
        "                    post_processing(option)\n",
        "                    results.append(option)\n",
        "                page = do_st_corrections(wikipedia.search(np)[0])\n",
        "                if page == np:\n",
        "                    # print(f\"Disambig, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "            except wikipedia.PageError as pageerr:\n",
        "                pass\n",
        "\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np)\n",
        "        ]\n",
        "\n",
        "        for term in wiki_search_results:\n",
        "            if (((new_term := term) in claim) or\n",
        "                ((new_term := term) in claim.replace(\" \", \"\")) or\n",
        "                ((new_term := term.replace(\"·\", \"\")) in claim) or                                   # 過濾人名\n",
        "                ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term)) in claim) or                          # 過濾空格 / 消歧義\n",
        "                ((new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[0]) in claim and       # 消歧義與括號內皆有在裡面\n",
        "                    (new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[1]) in claim) or\n",
        "                ((new_term := term.replace(\"-\", \" \")) in claim) or                                  # 過濾槓號\n",
        "                ((new_term := term.lower()) in claim) or                                            # 過濾大小寫\n",
        "                ((new_term := term.lower().replace(\"-\", \"\")) in claim) or                           # 過濾大小寫及槓號\n",
        "                ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term.lower().replace(\"-\", \"\"))) in claim)    # 過濾大小寫、槓號及消歧義\n",
        "                ):\n",
        "                post_processing(term)\n",
        "                direct_results.append(term)\n",
        "            # if prefix not in tmp_muji:  #忽略掉括號，如果括號有重複的話。假設如果有\" 1 (數字)\", 則\"1 (符號)\" 會被忽略\n",
        "            post_processing(term)\n",
        "            results.append(term)\n",
        "\n",
        "    direct_results = list(set(direct_results))\n",
        "    results = list(set(results))            # remove duplicates\n",
        "    series_data[\"predicted_pages\"] = results\n",
        "    series_data[\"direct_match\"] = direct_results\n",
        "\n",
        "    return series_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for j in range(0,1):\n",
        "#     import wikipedia\n",
        "#     import re\n",
        "#     wikipedia.set_lang(\"zh\")\n",
        "#     np = \"ddfas\"\n",
        "\n",
        "#     CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "#     CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "\n",
        "#     results = []\n",
        "#     direct_results = []\n",
        "    \n",
        "#     def do_st_corrections(text: str) -> str:\n",
        "#         simplified = CONVERTER_T2S.convert(text)\n",
        "#         return CONVERTER_S2T.convert(simplified)\n",
        "    \n",
        "#     def if_page_exists(page: str) -> bool:\n",
        "#         import requests\n",
        "#         url_base = \"https://zh.wikipedia.org/wiki/\"\n",
        "#         new_url = [url_base + page, url_base + page.upper()]\n",
        "#         for url in new_url:\n",
        "#             r = requests.head(url)\n",
        "#             if r.status_code == 200:\n",
        "#                 return True\n",
        "#             else:\n",
        "#                 continue\n",
        "#         return False\n",
        "    \n",
        "#     def post_processing(page):\n",
        "#         page = do_st_corrections(page)\n",
        "#         page = page.replace(\" \", \"_\")\n",
        "#         page = page.replace(\"-\", \"\")\n",
        "\n",
        "#     if (if_page_exists(np)):\n",
        "#         try:\n",
        "#             page = do_st_corrections(wikipedia.page(title=np).title)\n",
        "#             if page == np:\n",
        "#                 # print(f\"Found, np={np}, page={page}, claim={claim}\")\n",
        "#                 post_processing(page)\n",
        "#                 direct_results.append(page)\n",
        "#             else:\n",
        "#                 # print(f\"Redirect, np={np}, page={page}, claim={claim}\")\n",
        "#                 post_processing(page)\n",
        "#                 direct_results.append(page)\n",
        "#         except wikipedia.DisambiguationError as diserr:\n",
        "#             for option in diserr.options:\n",
        "#                 option = do_st_corrections(option)\n",
        "#                 if new_option := re.sub(r\"\\s\\(\\S+\\)\", \"\", option) in np:\n",
        "#                     # print(f\"Disambig, np={np}, page={option}, claim={claim}\")\n",
        "#                     post_processing(option)\n",
        "#                     direct_results.append(option)\n",
        "#                 post_processing(option)\n",
        "#                 results.append(option)\n",
        "#             page = do_st_corrections(wikipedia.search(np)[0])\n",
        "#             if page == np:\n",
        "#                 # print(f\"Disambig, np={np}, page={page}, claim={claim}\")\n",
        "#                 post_processing(page)\n",
        "#                 direct_results.append(page)\n",
        "#         except wikipedia.PageError as pageerr:\n",
        "#             pass\n",
        "\n",
        "#     # Simplified Traditional Chinese Correction\n",
        "#     wiki_search_results = [\n",
        "#         do_st_corrections(w) for w in wikipedia.search(np)\n",
        "#     ]\n",
        "\n",
        "#     for term in wiki_search_results:\n",
        "#         if (((new_term := term) in np) or\n",
        "#             ((new_term := term) in np.replace(\" \", \"\")) or\n",
        "#             ((new_term := term.replace(\"·\", \"\")) in np) or                                   # 過濾人名\n",
        "#             ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term)) in np) or                          # 過濾空格 / 消歧義\n",
        "#             ((new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[0]) in np and       # 消歧義與括號內皆有在裡面\n",
        "#                 (new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[1]) in np) or\n",
        "#             ((new_term := term.replace(\"-\", \" \")) in np) or                                  # 過濾槓號\n",
        "#             ((new_term := term.lower()) in np) or                                            # 過濾大小寫\n",
        "#             ((new_term := term.lower().replace(\"-\", \"\")) in np) or                           # 過濾大小寫及槓號\n",
        "#             ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term.lower().replace(\"-\", \"\"))) in np)    # 過濾大小寫、槓號及消歧義\n",
        "#             ):\n",
        "#             post_processing(term)\n",
        "#             direct_results.append(term)\n",
        "#         # if prefix not in tmp_muji:  #忽略掉括號，如果括號有重複的話。假設如果有\" 1 (數字)\", 則\"1 (符號)\" 會被忽略\n",
        "#         post_processing(term)\n",
        "#         results.append(term)\n",
        "\n",
        "#     direct_results = list(set(direct_results))\n",
        "#     results = list(set(results))            # remove duplicates\n",
        "#     print(results)\n",
        "#     print(direct_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_sbert(\n",
        "    series_data: pd.Series, \n",
        "    tokenizing_method: callable,\n",
        "    sbert_model: SentenceTransformer,\n",
        "    # wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        "    threshold: float,\n",
        "    i: int,\n",
        "    mode: str = \"train\",\n",
        "    suffix: str = \"0316\",\n",
        ") -> set:\n",
        "    # Disable huggingface tokenizor parallelism warning\n",
        "    import os\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
        "\n",
        "    import torch.cuda as cuda\n",
        "    cuda.empty_cache()\n",
        "    \n",
        "    # Parameters:\n",
        "    THRESHOLD_LOWEST = 0.6\n",
        "    THRESHOLD_SIM_LINE = threshold\n",
        "    WEIGHT_SIM_ID = 0.05    # The lower it is, the higher sim_id is when it directly matches claim.\n",
        "    \n",
        "    def sim_score_eval(sim_line, sim_id):\n",
        "        if len(claim) > 15:\n",
        "            if sim_line > THRESHOLD_SIM_LINE:\n",
        "                res = 2*(1.1*sim_line*1.1*sim_id)/(1.1*sim_line+1.1*sim_id)\n",
        "            else:\n",
        "                res = 0\n",
        "        else:\n",
        "            res = sim_id\n",
        "        \n",
        "        return res\n",
        "    \n",
        "    def post_processing(page) -> str:\n",
        "        import opencc\n",
        "        CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "        CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "        simplified = CONVERTER_T2S.convert(page)\n",
        "        page = CONVERTER_S2T.convert(simplified)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        return page\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    search_list = series_data[\"predicted_pages\"]\n",
        "    direct_search = series_data[\"direct_match\"]\n",
        "    results = []\n",
        "    mapping = {}\n",
        "    df_res = []\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    emb_claim_tok = sbert_model.encode(tokens)\n",
        "    emb_claim = sbert_model.encode(claim)\n",
        "\n",
        "    search_list = [post_processing(id) for id in search_list]\n",
        "    '''\n",
        "    if series_data[\"label\"] != \"NOT ENOUGH INFO\":\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in series_data[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "    else:\n",
        "        gt_pages = set([])\n",
        "    '''\n",
        "\n",
        "    for search_id in search_list:\n",
        "        # print(search_id)\n",
        "        search_series = wiki_pages.loc[wiki_pages['id'] == search_id]\n",
        "        if search_series.empty:\n",
        "            continue\n",
        "        try:\n",
        "            for temp in search_series[\"lines\"]:\n",
        "                search_lines = temp\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if len(search_lines) == 0:\n",
        "             continue\n",
        "        search_id_tok = tokenizing_method(search_id)\n",
        "        emb_id = sbert_model.encode(search_id_tok)\n",
        "        sim_id = util.pytorch_cos_sim(emb_id, emb_claim).numpy()\n",
        "        sim_id = sim_id[0][0]\n",
        "        new_sim_id = 0\n",
        "        if search_id in direct_search:\n",
        "            if sim_id > 0:\n",
        "                new_sim_id = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "            else:\n",
        "                sim_id = 0\n",
        "                new_sim_id = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "        else:\n",
        "            new_sim_id = sim_id\n",
        "\n",
        "        sim_score = 0\n",
        "        sim_line = 0\n",
        "        sim_line_b = 0\n",
        "\n",
        "        # Multiple GPU Support:\n",
        "            # embs = sbert_model.encode_multi_process(search_lines, pool=pool)\n",
        "            # for emb in embs:\n",
        "            #     sim = util.pytorch_cos_sim(emb, emb_claim).numpy()\n",
        "            #     sim = sim[0][0]\n",
        "            #     sim_line = max(sim, sim_line)\n",
        "\n",
        "            # search_lines_tok = [tokenizing_method(line) for line in search_lines]\n",
        "            # embs = sbert_model.encode_multi_process(search_lines_tok, pool=pool)\n",
        "            # for emb in embs:\n",
        "            #     sim = util.pytorch_cos_sim(emb, emb_claim_tok).numpy()\n",
        "            #     sim = sim[0][0]\n",
        "            #     sim_line = max(sim, sim_line)\n",
        "\n",
        "        # Single GPU Support:\n",
        "        for search_line in search_lines:\n",
        "            emb = sbert_model.encode(search_line)\n",
        "            sim = util.pytorch_cos_sim(emb, emb_claim).numpy()\n",
        "            sim = sim[0][0]\n",
        "            sim_line = max(sim, sim_line)\n",
        "\n",
        "            search_lines_tok = tokenizing_method(search_line)\n",
        "            emb = sbert_model.encode(search_lines_tok)\n",
        "            sim = util.pytorch_cos_sim(emb, emb_claim).numpy()\n",
        "            sim = sim[0][0]\n",
        "            sim_line = max(sim, sim_line)\n",
        "\n",
        "        if sim_line > THRESHOLD_SIM_LINE:\n",
        "            sim_line = max(sim_line, sim_line_b)\n",
        "            sim_line_b = sim_line\n",
        "            sim_score = sim_score_eval(sim_line, new_sim_id)\n",
        "            sim_score = max(sim_score, sim_line_b)\n",
        "            # print(sim_score, sim_line, search_id)\n",
        "            if sim_score > THRESHOLD_LOWEST:\n",
        "                search_id = post_processing(search_id)\n",
        "                if search_id in mapping:\n",
        "                    mapping[search_id] = max(sim_score, mapping[search_id])\n",
        "                else:\n",
        "                    mapping[search_id] = sim_score\n",
        "        data = (claim, search_id, sim_id, new_sim_id, sim_line, sim_score)\n",
        "        df_res.append(data)\n",
        "\n",
        "    mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "    # print(mapping_sorted[:topk])\n",
        "    DIFF = 0.125\n",
        "    for k, v in mapping_sorted:\n",
        "        THRESHOLD_TOP = v\n",
        "        break\n",
        "    if len(mapping_sorted) >= topk:\n",
        "        results = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "    else:\n",
        "        results = [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "    if not results:\n",
        "        results = [k for k, v in mapping_sorted][:topk]\n",
        "    if not results:\n",
        "        results = series_data[\"direct_match\"]\n",
        "    if not results:\n",
        "        results = series_data[\"predicted_pages\"][:topk]\n",
        "    # print(results)\n",
        "\n",
        "    # Analysis on missed pages\n",
        "    '''\n",
        "    if series_data[\"label\"] != \"NOT ENOUGH INFO\":\n",
        "        for page in gt_pages:\n",
        "            if page in mapping:\n",
        "                if page not in results:\n",
        "                    print(f\"Missed: ID={page}, score={mapping[page]}\")\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                if page not in search_list:\n",
        "                    print(f\"Missed: ID={page}, not in search_list\")\n",
        "                else:\n",
        "                    print(f\"Missed: ID={page}, score < {THRESHOLD_LOWEST}\")\n",
        "    '''\n",
        "    df = pd.DataFrame(df_res, columns=['Claim', 'Search_ID', 'Sim_ID', 'Sim_ID_Adjusted', 'Sim_Line', 'Sim_Score'])\n",
        "\n",
        "    with open(f\"data/{mode}_doc5_logging_{suffix}_{i}.jsonl\", \"a\", encoding=\"utf8\") as f:\n",
        "        f.write(df.to_json(orient='records', lines=True, force_ascii=False))\n",
        "    \n",
        "\n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_tfidf(\n",
        "    series_data: pd.Series, \n",
        "    tokenizing_method: callable,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
        "    wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        "    threshold: float\n",
        ") -> set:\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    results = []\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    claim_vector = vectorizer.transform([tokens])\n",
        "    sim_scores = cosine_similarity(tf_idf_matrix, claim_vector)\n",
        "    sim_scores = sim_scores[:, 0]  # flatten the array\n",
        "    sorted_indices = np.argsort(sim_scores)[::-1]\n",
        "    topk_sorted_indices = sorted_indices[:topk]\n",
        "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
        "\n",
        "    # for search_id in search_list:\n",
        "    #     search_tokens = wiki_pages.loc[wiki_pages['id'] == search_id]\n",
        "    #     if search_tokens.empty:\n",
        "    #         continue\n",
        "    #     search_processed_text = search_tokens[\"processed_text\"]\n",
        "    #     search_vector = vectorizer.transform(search_processed_text)\n",
        "    #     sim_scores = cosine_similarity(search_vector, claim_vector)\n",
        "    #     sim_scores = sim_scores[0][0]\n",
        "    #     if sim_scores > threshold:\n",
        "    #         mapping[search_id] = sim_scores\n",
        "            # print(sim_scores, search_id)\n",
        "\n",
        "    # print(mapping)\n",
        "    # results = sorted(mapping, key=mapping.get, reverse=True)[:topk]\n",
        "    # print(results)\n",
        "    return set(results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Get noun phrases from hanlp consituency parsing tree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initialize"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor = (hanlp.pipeline().append(\n",
        "    hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "    output_key=\"tok\",\n",
        ").append(\n",
        "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "    output_key=\"con\",\n",
        "    input_key=\"tok\",\n",
        "))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will skip this process which for creating parsing tree when demo on class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_file = f\"data/hanlp_con_results_0522.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA_1]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up document filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_path = f\"data/train_doc5.jsonl\"\n",
        "doc_path_sbert_0316 = f\"data/train_doc5_sbert_0316.jsonl\"\n",
        "doc_path_sbert_0522 = f\"data/train_doc5_sbert_0522.jsonl\"\n",
        "doc_path_search_0316 = f\"data/train_doc5_search_0316.jsonl\"\n",
        "doc_path_search_0522 = f\"data/train_doc5_search_0522.jsonl\"\n",
        "doc_path_tfidf_0316 = f\"data/train_doc5_tfidf_0316.jsonl\"\n",
        "doc_path_tfidf_0522 = f\"data/train_doc5_tfidf_0522.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len_public_0316 = 3969\n",
        "len_public_0522 = 7678\n",
        "len_test_private = 8049"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Do Search data on two public train dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dr_search(\n",
        "    data_name: str,\n",
        "    data_len: int,\n",
        "    hanlp_results: list,\n",
        "    suffix: str,\n",
        "    batch: int = 2000,\n",
        "    nb_workers: int = 32,\n",
        "    save_mode: str = \"train\",\n",
        "    start_round: int = 0,\n",
        "):\n",
        "    import math\n",
        "    rounds = math.ceil(data_len / batch)\n",
        "    pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=nb_workers)\n",
        "    DATA = load_json(data_name)\n",
        "\n",
        "    for i in range(start_round, rounds):\n",
        "        start = i*batch\n",
        "        df = pd.DataFrame(DATA[start:start+batch])\n",
        "        df.loc[:, \"hanlp_results\"] = hanlp_results[start:start+batch]\n",
        "        df_search = df.parallel_apply(\n",
        "            get_pred_pages_search, axis=1)\n",
        "        res_p = df_search[\"predicted_pages\"]\n",
        "        res_d = df_search[\"direct_match\"]\n",
        "        save_doc(DATA[start:start+batch], res_p, mode=save_mode, suffix=f\"_search_{suffix}_{i}p\", col_name=\"predicted_pages\")\n",
        "        DATA_SEARCH = load_json(f\"data/{save_mode}_doc5_search_{suffix}_{it}p.jsonl\")\n",
        "        save_doc(DATA_SEARCH, res_d, mode=save_mode, suffix=f\"_search_{suffix}_{it}d\", col_name=\"direct_match\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = 2000\n",
        "start = 0\n",
        "with open(f'data/train_doc5_search_{start}d.jsonl') as fp:\n",
        "    data = fp.read()\n",
        "with open(f'data/train_doc5_search_{start+batch}d.jsonl') as fp:\n",
        "    data2 = fp.read()\n",
        "    data += data2\n",
        "with open(f'data/train_doc5_search_{start+batch*2}d.jsonl') as fp:\n",
        "    data2 = fp.read()\n",
        "    data += data2\n",
        "with open(f'data/train_doc5_search_{start+batch*3}d.jsonl') as fp:\n",
        "    data2 = fp.read()\n",
        "    data += data2\n",
        "\n",
        "with open (f'data/train_doc5_search_0522.jsonl', 'w') as fp:\n",
        "    fp.write(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate Top 50 TFIDF Data for first phase document retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dr_tfidf(\n",
        "    data_len: int,                   # Total len for tfidf\n",
        "    data_name: str,             # Data File name\n",
        "    suffix: str,       # Determine save doc suffix, 0316/0522/public/private\n",
        "    save_mode: str = \"train\",   # Determine save doc mode, train/test\n",
        "    batch_size: int = 500,      # batch\n",
        "    start_round: int = 0,       # Start i\n",
        "):\n",
        "    import math\n",
        "\n",
        "    pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=2)\n",
        "    rounds = math.ceil(data_len / batch_size)\n",
        "    DATA = load_json(data_name)\n",
        "\n",
        "    for i in range(start_round, rounds):\n",
        "        start = i*batch_size\n",
        "        df_batch = pd.DataFrame(DATA[start:start+batch_size])\n",
        "        predicted_results = df_batch.parallel_apply(\n",
        "            partial(\n",
        "                get_pred_pages_tfidf,\n",
        "                tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "                vectorizer=vectorizer,\n",
        "                tf_idf_matrix=X,\n",
        "                wiki_pages=wiki_pages,\n",
        "                topk=50,\n",
        "                threshold=0.0\n",
        "            ), axis=1\n",
        "        )\n",
        "        save_doc(DATA[start:start+batch_size], predicted_results, mode=save_mode, suffix=f\"_tfidf_{suffix}_{start}\")\n",
        "        if save_mode == \"train\" and i <= 13:\n",
        "            print(f\"On TFIDF top 50 Data, batch = {i}:\")\n",
        "            precision = calculate_precision(DATA[start:start+batch_size], predicted_results)\n",
        "            recall = calculate_recall(DATA[start:start+batch_size], predicted_results)\n",
        "            f1 = calculate_f1(precision, recall)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_separate_data(\n",
        "    len: int,                   # Total len for tfidf\n",
        "    suffix: str,                # Determine save doc suffix\n",
        "    phase: str,                 # Determine phase, tfidf/search/sbert\n",
        "    batch_size: int = 500,      # batch\n",
        "    save_mode: str = \"train\",   # Determine save doc mode\n",
        "):\n",
        "    import math\n",
        "    rounds = math.ceil(len / batch_size)\n",
        "\n",
        "    start = 0\n",
        "    with open(f'data/{save_mode}_doc5_{phase}_{suffix}_{start}.jsonl') as fp:\n",
        "        data = fp.read()\n",
        "        fp.close()\n",
        "    for i in range(1, rounds):\n",
        "        with open(f'data/{save_mode}_doc5_{phase}_{suffix}_{start+batch_size}.jsonl') as fp:\n",
        "            data2 = fp.read()\n",
        "            data += data2\n",
        "            fp.close()\n",
        "        start += batch_size\n",
        "    \n",
        "    with open (f'data/{save_mode}_doc5_{phase}_{suffix}.jsonl', 'w') as fp:\n",
        "        fp.write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def union_result(series_data: pd.Series,) -> set:\n",
        "    tfidf = series_data[\"tfidf\"]\n",
        "    search = series_data[\"search\"]\n",
        "    return set(set(tfidf).union(set(search)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def union_tfidf_search(\n",
        "    data_name: str,\n",
        "    suffix: str,\n",
        "    save_mode: str = \"train\",\n",
        "):\n",
        "    DATA = load_json(data_name)\n",
        "\n",
        "    with open(f\"data/{save_mode}_doc5_search_{suffix}.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results_search = pd.Series([\n",
        "            set(json.loads(line)[\"direct_match\"])\n",
        "            for line in f\n",
        "        ], name=\"search\")\n",
        "    with open(f\"data/{save_mode}_doc5_tfidf_{suffix}.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results_tfidf = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ], name=\"tfidf\")\n",
        "\n",
        "    results_df = pd.merge(pd.Series([line for line in predicted_results_tfidf], name=\"tfidf\"), \n",
        "                        pd.Series([line for line in predicted_results_search], name=\"search\"), right_index=True, left_index=True)\n",
        "    predicted_results = results_df.apply(union_result, axis=1)\n",
        "    save_doc(DATA, predicted_results, mode=save_mode, suffix=f\"_tfidf_{suffix}_union\", col_name=\"predicted_pages\")\n",
        "\n",
        "    total = 0\n",
        "    for data in predicted_results:\n",
        "        total += len(data)\n",
        "    print(f\"Total Predicted pages: {total}\")\n",
        "\n",
        "    if save_mode == \"train\":\n",
        "        recall = calculate_recall(DATA, predicted_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_tfidf(\n",
        "    suffix: str,\n",
        "    save_mode: str = \"train\",\n",
        "):\n",
        "    DATA = load_json(f\"data/{save_mode}_doc5_tfidf_{suffix}_union.jsonl\")\n",
        "    with open(f\"data/{save_mode}_doc5_search_{suffix}.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "        direct_match = pd.Series([\n",
        "            set(json.loads(line)[\"direct_match\"])\n",
        "            for line in f\n",
        "        ], name=\"direct_match\")\n",
        "    save_doc(DATA, direct_match, mode=save_mode, suffix=f\"_tfidf_{suffix}_with_d\", col_name=\"direct_match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_name = file_train_0316\n",
        "suffix = \"0316\"\n",
        "\n",
        "dr_tfidf(data_len=len_public_0316, batch_size=500, data_name=data_name, suffix=suffix, start_round=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_separate_data(len=len_public_0316, suffix=suffix, phase=\"tfidf\")\n",
        "union_tfidf_search(data_name=file_train_0316, suffix=suffix)\n",
        "append_tfidf(suffix=suffix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def clean(series_data):\n",
        "#     def post_processing(page) -> str:\n",
        "#         page = page.replace(\" \", \"_\")\n",
        "#         page = page.replace(\"-\", \"\")\n",
        "#         return page\n",
        "    \n",
        "#     result = []\n",
        "#     for element in series_data:\n",
        "#         # print(series_data)\n",
        "#         element = post_processing(element)\n",
        "#         if \"Template:\" in element:\n",
        "#             continue\n",
        "#         result.append(element)\n",
        "\n",
        "#     return set(result)\n",
        "# doc_path_search = f\"data/train_doc5_search.jsonl\"\n",
        "# doc_path_search_backup = f\"data/train_doc5_search_backup.jsonl\"\n",
        "# TRAIN_DATA_SEARCH1 = load_json(doc_path_search_backup)\n",
        "\n",
        "# pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=12)\n",
        "# train_df = pd.DataFrame(TRAIN_DATA)\n",
        "# train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "# train_df_search1 = pd.DataFrame(TRAIN_DATA_SEARCH1)\n",
        "# predicted_results_search = train_df_search1.loc[:, \"predicted_pages\"]\n",
        "# direct_match = train_df_search1.loc[:, \"direct_match\"]\n",
        "# # print(predicted_results_search)\n",
        "# predicted_results_search = predicted_results_search.apply(clean)\n",
        "# direct_match = direct_match.apply(clean)\n",
        "\n",
        "# # predicted_results = train_df.progress_apply(get_pred_pages, axis=1)\n",
        "# save_doc(TRAIN_DATA, predicted_results_search, mode=\"train\", suffix=\"_search\", col_name=\"predicted_pages\")\n",
        "# TRAIN_DATA_SEARCH = load_json(doc_path_search)\n",
        "# # direct_match = train_df.parallel_apply(\n",
        "# #     get_pred_pages_search, axis=1)\n",
        "# save_doc(TRAIN_DATA_SEARCH, direct_match, mode=\"train\", suffix=\"_search\", col_name=\"direct_match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if Path(doc_path_search).exists():\n",
        "#     with open(doc_path_search, \"r\", encoding=\"utf8\") as f:\n",
        "#         predicted_results_search = pd.Series([\n",
        "#             set(json.loads(line)[\"predicted_pages\"])\n",
        "#             for line in f\n",
        "#         ], name=\"search\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Operate Second Document Retrieval using Sentence BERT on TFIDF top 50 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dr_sbert(\n",
        "    suffix: str,\n",
        "    data_len: int,\n",
        "    compare_data: str,          # Compare Data Filename\n",
        "    end_round: int, \n",
        "    num_of_samples: int = 500,\n",
        "    start_round: int = 0,\n",
        "    save_mode: str = \"train\"\n",
        "):\n",
        "    DATA = load_json(f\"data/{save_mode}_doc5_tfidf_{suffix}_with_d.jsonl\")\n",
        "    COMPARE = load_json(compare_data)\n",
        "\n",
        "    for i in range(start_round, end_round):\n",
        "        start = i*num_of_samples\n",
        "        df_tfidf = pd.DataFrame(DATA[start:start+num_of_samples])\n",
        "        results_sbert = df_tfidf.progress_apply(\n",
        "            partial(\n",
        "                get_pred_pages_sbert,\n",
        "                tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "                sbert_model = sbert_model,\n",
        "                topk=5,\n",
        "                threshold=0.375,\n",
        "                i = i,\n",
        "                suffix = suffix,\n",
        "                mode = save_mode,\n",
        "            ), axis=1)\n",
        "        save_doc(COMPARE[start:start+num_of_samples], results_sbert, mode=save_mode, suffix=f\"_sbert_{suffix}_{start}\")\n",
        "        try:\n",
        "            if i < 13 and save_mode == \"train\":\n",
        "                print(f\"On Sbert Data, batch = {i}:\")\n",
        "                precision = calculate_precision(COMPARE[start:start+num_of_samples], results_sbert)\n",
        "                recall = calculate_recall(COMPARE[start:start+num_of_samples], results_sbert)\n",
        "                f1 = calculate_f1(precision, recall)\n",
        "        except ZeroDivisionError:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "end_round = math.ceil(len_public_0316 / 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr_sbert(suffix=\"0316\", data_len=len_public_0316, compare_data=file_train_0316, start_round=1, end_round=end_round)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_separate_data(len=len_public_0522, suffix=\"0522\", phase=\"sbert\")\n",
        "# merge_separate_data(len=len_public_0316, suffix=\"0316\", phase=\"sbert\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Precision, Recall and F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"On Original Data, batch\")\n",
        "with open(f\"data/train_doc5_sbert_0522.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "    predicted_results_original = pd.Series([\n",
        "        set(json.loads(line)[\"predicted_pages\"])\n",
        "        for line in f\n",
        "    ], name=\"sbert\")\n",
        "old_precision = calculate_precision(TRAIN_DATA_2, predicted_results_original)\n",
        "old_recall = calculate_recall(TRAIN_DATA_2, predicted_results_original)\n",
        "old_f1 = calculate_f1(old_precision, old_recall)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Operation on Log File"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_log(\n",
        "    data: pd.DataFrame, \n",
        "    topk: int,\n",
        "    threshold: float,\n",
        "    progress_bar,\n",
        "    \n",
        "):\n",
        "    # Parameters:\n",
        "    THRESHOLD_LOWEST = 0.6\n",
        "    THRESHOLD_MID = 0.7\n",
        "    THRESHOLD_HIGHEST = 0.885\n",
        "    THRESHOLD_SIM_LINE = threshold\n",
        "    WEIGHT_SIM_ID = 0.2    # The lower it is, the higher sim_id is when it directly matches claim.\n",
        "    \n",
        "    def sim_score_eval(sim_line, sim_id, claim):\n",
        "        if len(claim) <= 15:\n",
        "            res = sim_id\n",
        "        else:\n",
        "            w_line = 1.1\n",
        "            w_id = 1.1\n",
        "            if sim_line > THRESHOLD_SIM_LINE:\n",
        "                res = 2*(w_line*sim_line*w_id*sim_id)/(w_line*sim_line+w_id*sim_id)\n",
        "            else:\n",
        "                res = 0\n",
        "        \n",
        "        return res\n",
        "    \n",
        "    def post_processing(page) -> str:\n",
        "        import opencc\n",
        "        CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "        CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "        simplified = CONVERTER_T2S.convert(page)\n",
        "        page = CONVERTER_S2T.convert(simplified)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        return page\n",
        "\n",
        "    results = []\n",
        "    doc_res = []\n",
        "    mapping = {}\n",
        "    claim_prev = \"\"\n",
        "    claim_count = 0\n",
        "    claim_comma = 0\n",
        "    direct_match = []\n",
        "    predicted_pages = []\n",
        "\n",
        "    for index, series_data in data.iterrows():\n",
        "        claim = series_data[\"Claim\"]\n",
        "        search_id = series_data[\"Search_ID\"]\n",
        "        sim_id = series_data[\"Sim_ID\"]\n",
        "        sim_id_new = series_data[\"Sim_ID_Adjusted\"]\n",
        "        sim_line = series_data[\"Sim_Line\"]\n",
        "\n",
        "        if index == 0:  \n",
        "            claim_prev = claim\n",
        "            claim_comma = claim.count(\"，\")\n",
        "        elif claim != claim_prev:\n",
        "            mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "            DIFF = 0.125\n",
        "            for k, v in mapping_sorted:\n",
        "                THRESHOLD_TOP = v\n",
        "                break\n",
        "            # print(mapping_sorted[:topk])\n",
        "            if len(mapping_sorted) >= topk:\n",
        "                doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "            else:\n",
        "                doc_res= [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "            if not doc_res:\n",
        "                doc_res = direct_match[:topk]\n",
        "            if not doc_res:\n",
        "                doc_res = predicted_pages[:topk]\n",
        "            \n",
        "            results.append(doc_res)\n",
        "            #print(claim_count, mapping)\n",
        "            doc_res = []\n",
        "            mapping = {}\n",
        "            claim_prev = claim\n",
        "            claim_comma = claim.count(\"，\")\n",
        "            claim_count = claim_count + 1\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        if sim_id != sim_id_new:\n",
        "            direct_match.append(search_id)\n",
        "            if sim_id > 0:\n",
        "                # print(f\"{search_id}: sim_id={sim_id}\")\n",
        "                sim_id_new = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "            else:\n",
        "                sim_id = 0\n",
        "                sim_id_new = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "        else:\n",
        "            sim_id_new = sim_id\n",
        "\n",
        "        predicted_pages.append(search_id)\n",
        "        sim_score = sim_score_eval(sim_line=sim_line, sim_id=sim_id_new, claim=claim)\n",
        "        if sim_score > 0:\n",
        "            sim_score = max(sim_score, sim_line)\n",
        "            # print(sim_score, search_id)\n",
        "            if sim_score > THRESHOLD_LOWEST:\n",
        "                search_id = post_processing(search_id)\n",
        "                if search_id in mapping:\n",
        "                    mapping[search_id] = max(sim_score, mapping[search_id])\n",
        "                else:\n",
        "                    mapping[search_id] = sim_score\n",
        "\n",
        "    mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "    if len(mapping_sorted) >= topk:\n",
        "        doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "    else:\n",
        "        doc_res= [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "    if not doc_res:\n",
        "        doc_res = direct_match[:topk]\n",
        "    if not doc_res:\n",
        "        doc_res = predicted_pages[:topk]\n",
        "\n",
        "    results.append(doc_res)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topk = 6\n",
        "num_of_samples = 500\n",
        "\n",
        "def merge(series_data: pd.Series) -> set:\n",
        "    result = []\n",
        "    for i in range(0, topk):\n",
        "        if series_data.iloc[i] != None:\n",
        "            result.append(series_data.iloc[i])\n",
        "    # print(set(result))\n",
        "    return set(result)\n",
        "\n",
        "for i in range(14, 16):\n",
        "    start = i*num_of_samples\n",
        "    doc_log = f\"data/train_doc5_logging_0522_{i}.jsonl\"\n",
        "    TRAIN_DATA_LOG = load_json(doc_log)\n",
        "    train_df_log = pd.DataFrame(TRAIN_DATA_LOG)\n",
        "\n",
        "    progress_bar = tqdm(range(500))\n",
        "    \n",
        "    predicted_results_log = get_pred_pages_log(\n",
        "        data=train_df_log, \n",
        "        topk=topk, \n",
        "        threshold=0.375, \n",
        "        progress_bar=progress_bar\n",
        "    )\n",
        "    predicted_results_log_df = pd.DataFrame(predicted_results_log)\n",
        "    predicted_results_log_df_b = predicted_results_log_df.apply(merge, axis=1)\n",
        "    save_doc(TRAIN_DATA[start:start+num_of_samples], predicted_results_log_df_b, mode=\"train\", suffix=f\"_log_0522_{i}\")\n",
        "\n",
        "    with open(f\"data/train_doc5_sbert_0522.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results_original = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ], name=\"sbert\")\n",
        "\n",
        "    if i < 13:\n",
        "        print(f\"On Original Data, batch = {i}\")\n",
        "        old_precision = calculate_precision(TRAIN_DATA[start:start+num_of_samples], predicted_results_original[start:start+num_of_samples])\n",
        "        old_recall = calculate_recall(TRAIN_DATA[start:start+num_of_samples], predicted_results_original[start:start+num_of_samples])\n",
        "        old_f1 = calculate_f1(precision, recall)\n",
        "\n",
        "        print(f\"\\nOn Log Data, batch = {i}\")\n",
        "        precision = calculate_precision(TRAIN_DATA[start:start+num_of_samples], predicted_results_log_df_b)\n",
        "        print(f\"(Diff: {precision-old_precision})\")\n",
        "        recall = calculate_recall(TRAIN_DATA[start:start+num_of_samples], predicted_results_log_df_b)\n",
        "        print(f\"(Diff: {recall-old_recall})\")\n",
        "        f1 = calculate_f1(precision, recall)\n",
        "        print(f\"F1-Score: {f1}\")\n",
        "        print(f\"(Diff: {f1-old_f1})\")\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = 500\n",
        "start = 0\n",
        "with open(f'data/train_doc5_log_0522_{start}.jsonl', \"r\", encoding=\"utf8\") as fp:\n",
        "    data = fp.read()\n",
        "    fp.close()\n",
        "for i in range(1, 16):\n",
        "    with open(f'data/train_doc5_log_0522_{i}.jsonl', \"r\", encoding=\"utf8\") as fp:\n",
        "        data2 = fp.read()\n",
        "        data += data2\n",
        "        fp.close()\n",
        "    start += batch\n",
        "\n",
        "with open(f'data/train_doc5_log_0522.jsonl', 'w', encoding=\"utf8\") as fp:\n",
        "    fp.write(data)\n",
        "    fp.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"On Original Data, batch\")\n",
        "with open(f\"data/train_doc5_sbert_0522.jsonl\", \"r\", encoding=\"utf8\") as f:\n",
        "    predicted_results_original = pd.Series([\n",
        "        set(json.loads(line)[\"predicted_pages\"])\n",
        "        for line in f\n",
        "    ], name=\"sbert\")\n",
        "old_precision = calculate_precision(TRAIN_DATA, predicted_results_original)\n",
        "old_recall = calculate_recall(TRAIN_DATA, predicted_results_original)\n",
        "old_f1 = calculate_f1(precision, recall)\n",
        "\n",
        "with open(f'data/train_doc5_log_0522.jsonl', 'r', encoding=\"utf8\") as fp:\n",
        "    predicted_results_log_df_b = pd.Series([\n",
        "        set(json.loads(line)[\"predicted_pages\"])\n",
        "        for line in fp\n",
        "    ], name=\"sbert\")\n",
        "print(f\"\\nOn Log Data, batch\")\n",
        "precision = calculate_precision(TRAIN_DATA, predicted_results_log_df_b)\n",
        "print(f\"(Diff: {precision-old_precision})\")\n",
        "recall = calculate_recall(TRAIN_DATA, predicted_results_log_df_b)\n",
        "print(f\"(Diff: {recall-old_recall})\")\n",
        "f1 = calculate_f1(precision, recall)\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"(Diff: {f1-old_f1})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Merge Two Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'data/train_doc5_sbert_0316.jsonl') as fp:\n",
        "    data = fp.read()\n",
        "    fp.close()\n",
        "with open(f'data/train_doc5_sbert_0522.jsonl') as fp:\n",
        "    data2 = fp.read()\n",
        "    data += data2\n",
        "    fp.close()\n",
        "\n",
        "with open (f'data/train_doc5_sbert.jsonl', 'w') as fp:\n",
        "    fp.write(data)\n",
        "    fp.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Repeat the same process on test set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create parsing tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
        "# if Path(hanlp_test_file).exists():\n",
        "#     with open(hanlp_test_file, \"rb\") as f:\n",
        "#         hanlp_test_results = pickle.load(f)\n",
        "# else:\n",
        "#     hanlp_test_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
        "#     with open(hanlp_test_file, \"wb\") as f:\n",
        "#         pickle.dump(hanlp_test_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_test_file = f\"data/hanlp_con_test_results_private.pkl\"\n",
        "if Path(hanlp_test_file).exists():\n",
        "    with open(hanlp_test_file, \"rb\") as f:\n",
        "        hanlp_test_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_test_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA_PRIVATE]\n",
        "    with open(hanlp_test_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_test_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_doc_path = f\"data/test_doc5.jsonl\"\n",
        "test_doc_path_aicup = f\"data/test_doc5_aicup.jsonl\"\n",
        "test_doc_path_search = f\"data/test_doc5_search.jsonl\"\n",
        "test_doc_path_tfidf = f\"data/test_doc5_tfidf.jsonl\"\n",
        "test_doc_path_sbert = f\"data/test_doc5_sbert_0522.jsonl\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On Search Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr_search(\n",
        "    data_name=file_test_private, \n",
        "    data_len=len_test_private, \n",
        "    hanlp_results= hanlp_test_results, \n",
        "    suffix=\"private\", \n",
        "    save_mode=\"test\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On TF-IDF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_name = file_test_private\n",
        "suffix = \"private\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr_tfidf(data_len=len_test_private, batch_size=500, data_name=data_name, suffix=suffix, start_round=11, save_mode=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merge_separate_data(len=len_test_private, suffix=suffix, phase=\"tfidf\", save_mode=\"test\")\n",
        "union_tfidf_search(data_name=data_name, suffix=suffix, save_mode=\"test\")\n",
        "append_tfidf(suffix=suffix, save_mode=\"test\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### On SBERT data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "end_round = math.ceil(len_test_private / 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.cuda as cuda\n",
        "with cuda.device('cuda:0'):\n",
        "    cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr_sbert(\n",
        "    suffix=\"private\", \n",
        "    data_len=len_test_private, \n",
        "    compare_data=file_test_private, \n",
        "    start_round=14, \n",
        "    save_mode=\"test\", \n",
        "    end_round=end_round,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4zFkSbjgXF"
      },
      "source": [
        "## PART 2. Sentence retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import some libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GlliDsgXjisj"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "# third-party libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset, Dataset\n",
        "\n",
        "# local libs\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J3BBLE3_hlPi"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
        "TEST_DATA_PUBLIC = load_json(\"data/public_test.jsonl\")\n",
        "TEST_DATA_PRIVATE = load_json(\"data/private_test_data.jsonl\")\n",
        "DOC_DATA = load_json(\"data/train_doc5_sbert.jsonl\")\n",
        "\n",
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
        "# GT means Ground Truth\n",
        "TRAIN_GT, DEV_GT = train_test_split(\n",
        "    DOC_DATA,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    shuffle=True,\n",
        "    stratify=_y,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and concatenating jsonl files in data/wiki-pages\n",
            "Generate parse mapping\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbbd35b4dd5c4245bcac4c0f968e6a14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to id to evidence_map mapping\n"
          ]
        }
      ],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate precision for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_precision(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate precision for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of precision)\n",
        "        [2]: retrieved (denominator of precision)\n",
        "    \"\"\"\n",
        "    this_precision = 0.0\n",
        "    this_precision_hits = 0.0\n",
        "\n",
        "    # Return 0, 0 if label is not enough info since not enough info does not\n",
        "    # contain any evidence.\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # e[2] is the page title, e[3] is the sentence index\n",
        "        all_evi = [[e[2], e[3]]\n",
        "                   for eg in instance[\"evidence\"]\n",
        "                   for e in eg\n",
        "                   if e[3] is not None]\n",
        "        claim = instance[\"claim\"]\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for prediction in predicted_evidence:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "        return (this_precision /\n",
        "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
        "\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate recall for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_recall(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate recall for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of recall)\n",
        "        [2]: relevant (denominator of recall)\n",
        "    \"\"\"\n",
        "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # If there's no evidence to predict, return 1\n",
        "        if len(instance[\"evidence\"]) == 0 or all(\n",
        "            [len(eg) == 0 for eg in instance]):\n",
        "            return 1.0, 1.0\n",
        "\n",
        "        claim = instance[\"claim\"]\n",
        "\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for evidence_group in instance[\"evidence\"]:\n",
        "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "            if all([item in predicted_evidence for item in evidence]):\n",
        "                # We only want to score complete groups of evidence. Incomplete\n",
        "                # groups are worthless.\n",
        "                return 1.0, 1.0\n",
        "        return 0.0, 1.0\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the scores of sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_retrieval(\n",
        "    probs: np.ndarray,\n",
        "    df_evidences: pd.DataFrame,\n",
        "    ground_truths: pd.DataFrame,\n",
        "    top_n: int = 5,\n",
        "    cal_scores: bool = True,\n",
        "    save_name: str = None,\n",
        "    threshold: int = 0.3,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Calculate the scores of sentence retrieval\n",
        "\n",
        "    Args:\n",
        "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
        "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
        "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
        "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: F1 score, precision, and recall\n",
        "    \"\"\"\n",
        "    df_evidences[\"prob\"] = probs\n",
        "    top_rows = (\n",
        "        df_evidences.where(df_evidences[\"prob\"].gt(threshold)).groupby(\"claim\", group_keys=True).apply(\n",
        "        lambda x: x.nlargest(top_n, \"prob\"))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    if top_rows.empty:\n",
        "        top_rows = (\n",
        "        df_evidences.groupby(\"claim\", group_keys=True).apply(\n",
        "        lambda x: x.nlargest(top_n, \"prob\"))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if cal_scores:\n",
        "        macro_precision = 0\n",
        "        macro_precision_hits = 0\n",
        "        macro_recall = 0\n",
        "        macro_recall_hits = 0\n",
        "\n",
        "        for i, instance in enumerate(ground_truths):\n",
        "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
        "            macro_precision += macro_prec[0]\n",
        "            macro_precision_hits += macro_prec[1]\n",
        "\n",
        "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
        "            macro_recall += macro_rec[0]\n",
        "            macro_recall_hits += macro_rec[1]\n",
        "\n",
        "        pr = (macro_precision /\n",
        "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
        "        rec = (macro_recall /\n",
        "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
        "        f1 = 2.0 * pr * rec / (pr + rec)\n",
        "\n",
        "    if save_name is not None:\n",
        "        # write doc7_sent5 file\n",
        "        with open(f\"data/{save_name}\", \"w\", encoding=\"utf8\") as f:\n",
        "            for instance in ground_truths:\n",
        "                claim = instance[\"claim\"]\n",
        "                predicted_evidence = top_rows[\n",
        "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
        "                instance[\"predicted_evidence\"] = predicted_evidence\n",
        "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    if cal_scores:\n",
        "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference script to get probabilites for the candidate evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predicted_probs(\n",
        "    model: nn.Module,\n",
        "    dataloader: Dataset,\n",
        "    device: torch.device,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
        "\n",
        "    Args:\n",
        "        model: the one from HuggingFace Transformers\n",
        "        dataloader: devset or testset in torch dataloader\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: probabilites of the candidate evidence sentences\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
        "\n",
        "    return np.array(probs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentRetrievalBERTDataset(BERTDataset):\n",
        "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        sentA = item[\"claim\"]\n",
        "        sentB = item[\"text\"]\n",
        "\n",
        "        # claim [SEP] text\n",
        "        concat = self.tokenizer(\n",
        "            sentA,\n",
        "            sentB,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gpvXpFwXszfv"
      },
      "outputs": [],
      "source": [
        "def pair_with_wiki_sentences(\n",
        "    mapping: Dict[str, Dict[int, str]],\n",
        "    df: pd.DataFrame,\n",
        "    negative_ratio: float,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating train sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # positive\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "        evidence_sets = df[\"evidence\"].iloc[i]\n",
        "        for evidence_set in evidence_sets:\n",
        "            sents = []\n",
        "            for evidence in evidence_set:\n",
        "                # evidence[2] is the page title\n",
        "                page = evidence[2].replace(\" \", \"_\")\n",
        "                # the only page with weird name\n",
        "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
        "                    continue\n",
        "                # evidence[3] is in form of int however, mapping requires str\n",
        "                sent_idx = str(evidence[3])\n",
        "                sents.append(mapping[page][sent_idx])\n",
        "\n",
        "            whole_evidence = \" \".join(sents)\n",
        "\n",
        "            claims.append(claim)\n",
        "            sentences.append(whole_evidence)\n",
        "            labels.append(1)\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        evidence_set = set([(evidence[2], evidence[3])\n",
        "                            for evidences in df[\"evidence\"][i]\n",
        "                            for evidence in evidences])\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [\n",
        "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
        "                ]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for pair in page_sent_id_pairs:\n",
        "                if pair in evidence_set:\n",
        "                    continue\n",
        "                text = mapping[page][pair[1]]\n",
        "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
        "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    labels.append(0)\n",
        "\n",
        "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
        "\n",
        "\n",
        "def pair_with_wiki_sentences_eval(\n",
        "    mapping: Dict[str, Dict[int, str]],\n",
        "    df: pd.DataFrame,\n",
        "    is_testset: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    evidence = []\n",
        "    predicted_evidence = []\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "        #     continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for page_name, sentence_id in page_sent_id_pairs:\n",
        "                text = mapping[page][sentence_id]\n",
        "                if text != \"\":\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    if not is_testset:\n",
        "                        evidence.append(df[\"evidence\"].iloc[i])\n",
        "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"claim\": claims,\n",
        "        \"text\": sentences,\n",
        "        \"evidence\": evidence if not is_testset else None,\n",
        "        \"predicted_evidence\": predicted_evidence,\n",
        "    })"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "MODEL_NAME = \"hfl/chinese-bert-wwm\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm-ext\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-macbert-base\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-lert-base\" #@param {type:\"string\"}\n",
        "\n",
        "MODEL_SHORT = \"hfl_bert\"\n",
        "NUM_EPOCHS = 1  #@param {type:\"integer\"}\n",
        "LR = 2e-5  #@param {type:\"number\"}\n",
        "TRAIN_BATCH_SIZE = 64  #@param {type:\"integer\"}\n",
        "TEST_BATCH_SIZE = 256  #@param {type:\"integer\"}\n",
        "NEGATIVE_RATIO = 0.1  #@param {type:\"number\"}\n",
        "WARMUP_RATIO = 0.1  #@param {type:\"number\"}\n",
        "VALIDATION_STEP = 50  #@param {type:\"integer\"}\n",
        "TOP_N = 5  #@param {type:\"integer\"}\n",
        "#@title  { display-mode: \"form\" }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ToLvE9oxIXQo"
      },
      "outputs": [],
      "source": [
        "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_warm{WARMUP_RATIO}_top{TOP_N}_{MODEL_SHORT}_new\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Combine claims and evidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4A5vWEzPiXGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
            "0    18268\n",
            "1     7079\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train_df = pair_with_wiki_sentences(\n",
        "    mapping,\n",
        "    pd.DataFrame(TRAIN_GT),\n",
        "    NEGATIVE_RATIO,\n",
        ")\n",
        "counts = train_df[\"label\"].value_counts()\n",
        "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
        "print(counts)\n",
        "\n",
        "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Start training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataloader things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l48WifjeIGui"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer)\n",
        "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save your memory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4rl_u0YbeQtY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "# print(torch.cuda.device_count())\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     # import os\n",
        "#     # os.environ['MASTER_ADDR'] = 'localhost'\n",
        "#     # os.environ['MASTER_PORT'] = '5678'\n",
        "#     # torch.distributed.init_process_group(backend=\"nccl\")\n",
        "#     model = nn.DataParallel(model)\n",
        "#     # model = model.cuda()\n",
        "#     # model = nn.parallel.DistributedDataParallel(model)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps, warmup_ratio=WARMUP_RATIO)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please make sure that you are using gpu when training (5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AHGaKh1eKmg"
      },
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        torch.cuda.empty_cache()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        # loss.sum().backward()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.sum().item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "            # print(probs)\n",
        "\n",
        "            val_results = evaluate_retrieval(\n",
        "                probs=probs,\n",
        "                df_evidences=dev_evidences,\n",
        "                ground_truths=DEV_GT,\n",
        "                top_n=TOP_N,\n",
        "            )\n",
        "            print(current_steps, val_results)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                writer.add_scalar(\n",
        "                    f\"dev_{metric_name}\",\n",
        "                    metric_value,\n",
        "                    current_steps,\n",
        "                )\n",
        "\n",
        "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation part (15 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QS1Ei5DAIO5p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start final evaluations and write prediction files.\n",
            "Start calculating training scores\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce21373c906f43e7a1607c7ddbdebaa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1021 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "ckpt_name = \"model.350.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "print(\"Start final evaluations and write prediction files.\")\n",
        "\n",
        "train_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping=mapping,\n",
        "    df=pd.DataFrame(TRAIN_GT),\n",
        ")\n",
        "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer)\n",
        "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start calculating training scores\")\n",
        "probs = get_predicted_probs(model, train_dataloader, device)\n",
        "train_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=train_evidences,\n",
        "    ground_truths=TRAIN_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"sent_retrieval/train_doc5sent{TOP_N}_neg{NEGATIVE_RATIO}_{LR}_e{NUM_EPOCHS}_{MODEL_SHORT}_new.jsonl\",\n",
        ")\n",
        "print(f\"Training scores => {train_results}\")\n",
        "\n",
        "print(\"Start validation\")\n",
        "probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "val_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=dev_evidences,\n",
        "    ground_truths=DEV_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"sent_retrieval/dev_doc5sent{TOP_N}_neg{NEGATIVE_RATIO}_{LR}_e{NUM_EPOCHS}_{MODEL_SHORT}.jsonl\",\n",
        ")\n",
        "\n",
        "print(f\"Validation scores => {val_results}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the model we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_name = \"model.350.pt\"\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Check on our test data\n",
        "(5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVFusJqjmex-"
      },
      "outputs": [],
      "source": [
        "test_data = load_json(\"data/test_doc5_sbert_public.jsonl\")\n",
        "\n",
        "test_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping,\n",
        "    pd.DataFrame(test_data),\n",
        "    is_testset=True,\n",
        ")\n",
        "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
        "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start predicting the test data\")\n",
        "probs = get_predicted_probs(model, test_dataloader, device)\n",
        "evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=test_evidences,\n",
        "    ground_truths=test_data,\n",
        "    top_n=TOP_N,\n",
        "    cal_scores=False,\n",
        "    save_name= f\"sent_retrieval/test_doc5sent{TOP_N}_neg{NEGATIVE_RATIO}_{LR}_e{NUM_EPOCHS}_{MODEL_SHORT}_new.jsonl\",\n",
        "    # save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGzl8a5JteT7"
      },
      "source": [
        "## PART 3. Claim verification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgA1vcUyzjlx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
        "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
        "\n",
        "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
        "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (same as part 2.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AICUP dataset with top-k evidence sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
        "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        claim = item[\"claim\"]\n",
        "        evidence = item[\"evidence_list\"]\n",
        "\n",
        "        # In case there are less than topk evidence sentences\n",
        "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
        "        evidence += pad\n",
        "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
        "\n",
        "        concat = self.tokenizer(\n",
        "            concat_claim_evidence,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(label)\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            y_true.extend(batch[\"labels\"].tolist())\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss += outputs.loss.sum().item()\n",
        "            logits = outputs.logits\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    for batch in tqdm(test_dl,\n",
        "                      total=len(test_dl),\n",
        "                      leave=False,\n",
        "                      desc=\"Predicting\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(**batch).logits\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        preds.extend(pred.tolist())\n",
        "    return preds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_with_topk_evidence(\n",
        "    df: pd.DataFrame,\n",
        "    mapping: dict,\n",
        "    mode: str = \"train\",\n",
        "    topk: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
        "\n",
        "    Note:\n",
        "        After extraction, the dataset will be like this:\n",
        "               id     label         claim                           evidence            evidence_list\n",
        "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
        "        ..    ...       ...            ...                                ...                     ...\n",
        "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
        "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
        "\n",
        "        [946 rows x 5 columns]\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset with evidence.\n",
        "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
        "        topk (int, optional): The topk evidence. Defaults to 5.\n",
        "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
        "            If cache is None, return the result directly.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataset with topk evidence_list.\n",
        "            The `evidence_list` column will be: List[str]\n",
        "    \"\"\"\n",
        "\n",
        "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "    if \"evidence\" in df.columns:\n",
        "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
        "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
        "            if not isinstance(x[0][0], list) else x)\n",
        "\n",
        "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "    # if mode == \"eval\":\n",
        "        # extract evidence\n",
        "    df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "        mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "        for evi_id, evi_idx in x  # for each evidence list\n",
        "    ][:topk] if isinstance(x, list) else [])\n",
        "    print(df[\"evidence_list\"][:topk])\n",
        "    # else:\n",
        "    #     # extract evidence\n",
        "    #     # if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "    #     #     df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "    #     #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "    #     #         for evi_id, evi_idx in x  # for each evidence list\n",
        "    #     #     ][:topk] if isinstance(x, list) else [])\n",
        "    #     # else:\n",
        "    #     df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
        "    #         \" \".join([  # join evidence\n",
        "    #             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "    #             for _, _, evi_id, evi_idx in evi_list\n",
        "    #         ]) if isinstance(evi_list, list) else \"\"\n",
        "    #         for evi_list in x  # for each evidence list\n",
        "    #     ][:1] if isinstance(x, list) else [])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def join_with_topk_evidence(\n",
        "#     df: pd.Series,\n",
        "#     mapping: dict,\n",
        "#     mode: str = \"train\",\n",
        "#     topk: int = 5,\n",
        "# ) -> pd.Series:\n",
        "#     # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "#     if \"evidence\" in df:\n",
        "#         df[\"evidence\"] = [[df[\"evidence\"]]] if not isinstance(df[\"evidence\"][0], list) else [df[\"evidence\"]] if not isinstance(df[\"evidence\"][0][0], list) else df[\"evidence\"]\n",
        "\n",
        "#     print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "#     if mode == \"eval\":\n",
        "#         df[\"evidence_list\"] = [\n",
        "#             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#             for evi_id, evi_idx in df[\"predicted_evidence\"]  # for each evidence list\n",
        "#         ][:1] if isinstance(df[\"predicted_evidence\"], list) else []\n",
        "#         print(df[\"evidence_list\"][:1])\n",
        "#     else:\n",
        "#         if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "#             df[\"evidence_list\"] = [\n",
        "#                 mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#                 for evi_id, evi_idx in df[\"predicted_evidence\"]  # for each evidence list\n",
        "#             ][:1] if isinstance(df[\"predicted_evidence\"], list) else []\n",
        "#             print(df[\"evidence_list\"][:1])\n",
        "#         else:\n",
        "#             df[\"evidence_list\"] = [\n",
        "#                 \" \".join([  # join evidence\n",
        "#                     mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#                     for _, _, evi_id, evi_idx in evi_list\n",
        "#                 ]) if isinstance(evi_list, list) else \"\"\n",
        "#                 for evi_list in df[\"evidence\"]  # for each evidence list\n",
        "#             ][:1] if isinstance(df[\"evidence\"], list) else []\n",
        "#     # else:\n",
        "#     #     # extract evidence\n",
        "#     #     # if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "#     #     #     df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "#     #     #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#     #     #         for evi_id, evi_idx in x  # for each evidence list\n",
        "#     #     #     ][:topk] if isinstance(x, list) else [])\n",
        "#     #     # else:\n",
        "#     #     df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
        "#     #         \" \".join([  # join evidence\n",
        "#     #             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#     #             for _, _, evi_id, evi_idx in evi_list\n",
        "#     #         ]) if isinstance(evi_list, list) else \"\"\n",
        "#     #         for evi_list in x  # for each evidence list\n",
        "#     #     ][:1] if isinstance(x, list) else [])\n",
        "\n",
        "#     return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "\n",
        "# MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"ckiplab/bert-base-chinese\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"ckiplab/albert-base-chinese\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm-ext\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-macbert-base\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\" #@param {type:\"string\"}\n",
        "MODEL_NAME = \"hfl/chinese-lert-base\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-lert-large\" #@param {type:\"string\"}\n",
        "\n",
        "MODEL_SHORT = \"hfl-lert-base-1\"\n",
        "EVAL_VERSION = 2\n",
        "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "SEED = 42  #@param {type:\"integer\"}\n",
        "LR = 5.625e-5  #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 20  #@param {type:\"integer\"}\n",
        "REAL_EPOCHS = 8\n",
        "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
        "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
        "VALIDATION_STEP = 25  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_FILENAME = \"submission.jsonl\"\n",
        "\n",
        "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}_{MODEL_SHORT}_eval{EVAL_VERSION}_maxlen{MAX_SEQ_LEN}\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Concat claim and evidences\n",
        "join topk evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRAIN_PKL_FILE.exists():\n",
        "    # train_df = pd.DataFrame(TRAIN_DATA)\n",
        "    # train_df = train_df.parallel_apply(partial(\n",
        "    #     join_with_topk_evidence,\n",
        "    #     mapping=mapping,\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    # ), axis=1)\n",
        "    train_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(TRAIN_DATA),\n",
        "        mapping,\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
        "        train_df = pickle.load(f)\n",
        "\n",
        "if not DEV_PKL_FILE.exists():\n",
        "    # dev_df = pd.DataFrame(DEV_DATA)\n",
        "    # dev_df = dev_df.parallel_apply(partial(\n",
        "    #     join_with_topk_evidence,\n",
        "    #     mapping=mapping,\n",
        "    #     mode=\"eval\",\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    # ), axis=1)\n",
        "    dev_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(DEV_DATA),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
        "        dev_df = pickle.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prevent CUDA out of memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0rVk3990DlD"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "val_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    dev_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    num_workers=0,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, num_workers=0,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzMgs-Zs3sTN"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABEL2ID),\n",
        ")\n",
        "# if torch.cuda.device_count() > 1:\n",
        "#     model = nn.DataParallel(model)\n",
        "# torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training (30 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqMjEek3wmu"
      },
      "outputs": [],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(REAL_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        torch.cuda.empty_cache()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        # loss.sum().backward()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.sum().item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(f\"Start validation: current_steps={current_steps}, epoch={epoch}\")\n",
        "            val_results = run_evaluation(model, eval_dataloader, device)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                print(f\"{metric_name}: {metric_value}\")\n",
        "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
        "\n",
        "            val_acc = val_results['val_acc']\n",
        "            if val_acc > 0.6:\n",
        "                save_checkpoint(\n",
        "                    model,\n",
        "                    CKPT_DIR,\n",
        "                    current_steps,\n",
        "                    mark=f\"val_acc={val_acc:.4f}\",\n",
        "                )\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Make your submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLkfuoAE49mz"
      },
      "outputs": [],
      "source": [
        "TEST_DATA = load_json(\"data/test_doc5sent5_public.jsonl\")\n",
        "TEST_PKL_FILE = Path(\"data/test_doc5sent5_public.pkl\")\n",
        "\n",
        "if not TEST_PKL_FILE.exists():\n",
        "    # test_df = pd.DataFrame(TEST_DATA)\n",
        "    # test_df = test_df.parallel_apply(partial(\n",
        "    #     join_with_topk_evidence,\n",
        "    #     mapping=mapping,\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    #     mode=\"eval\",\n",
        "    # ), axis=1)\n",
        "    test_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(TEST_DATA),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
        "        test_df = pickle.load(f)\n",
        "\n",
        "test_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqIjlht8yCMA"
      },
      "outputs": [],
      "source": [
        "ckpt_name = \"val_acc=0.6725_model.575.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "predicted_label = run_predict(model, test_dataloader, device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9I3ZWW4pHo"
      },
      "outputs": [],
      "source": [
        "predict_dataset = test_df.copy()\n",
        "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
        "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
        "    f\"submission/{ckpt_name[:14]}_{MODEL_SHORT}_{LR}_{EVAL_VERSION}_e{NUM_EPOCHS}_data3_maxlen{MAX_SEQ_LEN}_{OUTPUT_FILENAME}\",\n",
        "    orient=\"records\",\n",
        "    lines=True,\n",
        "    force_ascii=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
