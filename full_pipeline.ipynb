{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline\n",
        "python: 3.8.*\n",
        "\n",
        "use ```Ctrl + ]``` to collapse all section :)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download our starter pack (3~5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown 1Xq2Fv6UGA1pc25pF0qwEc_l7Fa5jPP6p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown --folder 1T6jpOtdf_i6XNYA6F_lqU4mRRh1xYPcl\n",
        "!mv baseline/* ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown --folder 1hnVYEgN-gYzFCeBZo8cbKjGLBP-YTnTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h7MSEcenjVrL"
      },
      "source": [
        "## PART 1. Document retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the environment and import all library we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "niqu9pLajYC_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "from functools import partial\n",
        "\n",
        "# 3rd party libs\n",
        "import hanlp\n",
        "import opencc\n",
        "import pandas as pd\n",
        "from hanlp.components.pipeline import Pipeline\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "# our own libs\n",
        "from utils import load_json\n",
        "from hw3_utils import jsonl_dir_to_df\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from TCSP import read_stopwords_list\n",
        "\n",
        "stopwords = read_stopwords_list()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
        "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
        "CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data class for type hinting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Claim:\n",
        "    data: str\n",
        "\n",
        "@dataclass\n",
        "class AnnotationID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class EvidenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class PageTitle:\n",
        "    title: str\n",
        "\n",
        "@dataclass\n",
        "class SentenceID:\n",
        "    id: int\n",
        "\n",
        "@dataclass\n",
        "class Evidence:\n",
        "    data: List[List[Tuple[AnnotationID, EvidenceID, PageTitle, SentenceID]]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of consistency, we convert traditional to simplified Chinese first before converting it back to traditional Chinese.  This is due to some errors occuring when converting traditional to traditional Chinese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A3NU01DnjKp-"
      },
      "outputs": [],
      "source": [
        "def do_st_corrections(text: str) -> str:\n",
        "    simplified = CONVERTER_T2S.convert(text)\n",
        "\n",
        "    return CONVERTER_S2T.convert(simplified)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use constituency parsing to separate part of speeches or so called constituent to extract noun phrases.  In the later stages, we will use the noun phrases as the query to search for relevant documents.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_nps_hanlp(\n",
        "    predictor: Pipeline,\n",
        "    d: Dict[str, Union[int, Claim, Evidence]],\n",
        ") -> List[str]:\n",
        "    claim = d[\"claim\"]\n",
        "    tree = predictor(claim)[\"con\"]\n",
        "    nps = [\n",
        "        do_st_corrections(\"\".join(subtree.leaves()))\n",
        "        for subtree in tree.subtrees(lambda t: t.label() == \"NP\")\n",
        "    ]\n",
        "\n",
        "    return nps"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Precision refers to how many related documents are retrieved.  Recall refers to how many relevant documents are retrieved.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_precision(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> float:\n",
        "    precision = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        # Extract all ground truth of titles of the wikipedia pages\n",
        "        # evidence[2] refers to the title of the wikipedia page\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        if len(predicted_pages) != 0:\n",
        "            precision += len(hits) / len(predicted_pages)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Macro precision\n",
        "    print(f\"Precision: {precision / count}\")\n",
        "    return precision / count\n",
        "\n",
        "\n",
        "def calculate_recall(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        ") -> float:\n",
        "    recall = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, d in enumerate(data):\n",
        "        if d[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in d[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "        predicted_pages = predictions.iloc[i]\n",
        "        hits = predicted_pages.intersection(gt_pages)\n",
        "        recall += len(hits) / len(gt_pages)\n",
        "        count += 1\n",
        "\n",
        "    print(f\"Recall: {recall / count}\")\n",
        "    return recall / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_f1(precision: float, recall: float) -> float:\n",
        "    return 2*(precision*recall)/(precision+recall)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default amount of documents retrieved is at most five documents.  This `num_pred_doc` can be adjusted based on your objective.  Save data in jsonl format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_doc(\n",
        "    data: List[Dict[str, Union[int, Claim, Evidence]]],\n",
        "    predictions: pd.Series,\n",
        "    mode: str = \"train\",\n",
        "    suffix: str = \"\",\n",
        "    num_pred_doc: int = 5,\n",
        "    col_name = \"predicted_pages\"\n",
        ") -> None:\n",
        "    with open(\n",
        "        f\"data/{mode}_doc{num_pred_doc}{suffix}.jsonl\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        for i, d in enumerate(data):\n",
        "            d[col_name] = list(predictions.iloc[i])\n",
        "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from /home/P78081057/Gaber_AICUP2023/data/jieba_dict/dict.txt.big ...\n",
            "Loading model from cache /tmp/jieba.ua00b00166cb119b323a586144d426557.cache\n",
            "Loading model cost 0.630 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ],
      "source": [
        "import jieba\n",
        "jieba.set_dictionary('data/jieba_dict/dict.txt.big')\n",
        "jieba.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text: str, stopwords: list) -> str:\n",
        "    tokens = jieba.cut(text)\n",
        "\n",
        "    return \" \".join([w for w in tokens if w not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_path = \"data/wiki-pages\"\n",
        "min_wiki_length = 25\n",
        "topk = 5\n",
        "min_df = 1\n",
        "max_df = 0.5\n",
        "use_idf = True\n",
        "sublinear_tf = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_cache = \"wiki\"\n",
        "target_column = \"text\"\n",
        "\n",
        "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
        "if wiki_cache_path.exists():\n",
        "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
        "else:\n",
        "    def text_split(line: str) -> list:\n",
        "        import re\n",
        "        line = re.sub(r\"[0-9]+\\t\", \"\", line)\n",
        "        lines = line.split(\"\\n\")\n",
        "        lines = list(filter(None, lines))\n",
        "        return lines\n",
        "    # You need to download `wiki-pages.zip` from the AICUP website\n",
        "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
        "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
        "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
        "\n",
        "    # tokenize the text and keep the result in a new column `processed_text`\n",
        "    wiki_pages[\"lines\"] = wiki_pages[\"lines\"].parallel_apply(text_split)\n",
        "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
        "        partial(tokenize, stopwords=stopwords)\n",
        "    )\n",
        "    # save the result to a pickle file\n",
        "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "wiki_pages = wiki_pages[\n",
        "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tfidf Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = wiki_pages[\"processed_text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     min_df=min_df,\n",
        "#     max_df=max_df,\n",
        "#     use_idf=use_idf,\n",
        "#     sublinear_tf=sublinear_tf,\n",
        "#     dtype=np.float64,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.models\n",
        "w2vmodel = gensim.models.Word2Vec.load(\"models/w2v.zh.300/word2vec.model\")\n",
        "w2v = dict(zip(w2vmodel.wv.index_to_key, w2vmodel.wv.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "class TfidfEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec, size=300):\n",
        "        self.word2vec = word2vec\n",
        "        self.word2weight = None\n",
        "        self.dim = size\n",
        "    \n",
        "    def fit(self, X):\n",
        "        tfidf = TfidfVectorizer(\n",
        "            min_df=min_df,\n",
        "            max_df=max_df,\n",
        "            use_idf=use_idf,\n",
        "            sublinear_tf=sublinear_tf,\n",
        "            dtype=np.float64,\n",
        "            analyzer=lambda x: x\n",
        "        )\n",
        "        tfidf.fit(X)\n",
        "        # if a word was never seen - it must be at least as infrequent\n",
        "        # as any of the known words - so the default idf is the max of \n",
        "        # known idf's\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfEmbeddingVectorizer(w2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = vectorizer.fit(corpus).transform(corpus)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentence BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /home/P78081057/.cache/torch/sentence_transformers/uer_sbert-base-chinese-nli. Creating a new one with MEAN pooling.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "sbert_model = SentenceTransformer('uer/sbert-base-chinese-nli', device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool = sbert_model.start_multi_process_pool()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for document retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages(\n",
        "        series_data: pd.Series, \n",
        "        ) -> Set[Dict[int, str]]:\n",
        "    import wikipedia\n",
        "    import re\n",
        "    import opencc\n",
        "    import pandas as pd\n",
        "\n",
        "    from TCSP import read_stopwords_list\n",
        "    stopwords = read_stopwords_list()\n",
        "\n",
        "    import numpy as np\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    wikipedia.set_lang(\"zh\")\n",
        "    CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "    CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "    def do_st_corrections(text: str) -> str:\n",
        "        simplified = CONVERTER_T2S.convert(text)\n",
        "        return CONVERTER_S2T.convert(simplified)\n",
        "\n",
        "    results = []\n",
        "    tmp_muji = []\n",
        "    # wiki_page: its index showned in claim\n",
        "    mapping = {}\n",
        "    claim = series_data[\"claim\"]\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "    first_wiki_term = []\n",
        "    repeated_mention = []\n",
        "    quote_search = []\n",
        "\n",
        "    def clean_claim(claim) -> str:     # Clean claim function because hanlp has error when conducting cons\n",
        "        def multiple_replacer(*kv):\n",
        "            replace_dict = dict(kv)\n",
        "            replace_func = lambda match: replace_dict[match.group(0)]\n",
        "            pattern = re.compile(\"|\".join([re.escape(k) for k, v in kv]), re.M)\n",
        "            return lambda string: pattern.sub(replace_func, string) \n",
        "        def multiple_replace(string, *kv):\n",
        "            return multiple_replacer(*replace_dict)(claim)\n",
        "\n",
        "        replace_dict = (\" \", \"\"), (\"牠\", \"它\"), (\"（\", \"(\"), (\"）\", \")\"), (\"，\", \",\"), (\"、\", \",\"), (\"群\", \"羣\"), (\"“\", \"\\\"\"), (\"”\", \"\\\"\"), (\"「\", \"“\"), (\"」\", \"”\")\n",
        "        claim = multiple_replace(claim, *replace_dict)\n",
        "        claim = claim.lower()\n",
        "        return claim\n",
        "\n",
        "    claim = clean_claim(claim)\n",
        "\n",
        "    def post_processing(np, page, loc):\n",
        "        page = do_st_corrections(page)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        search_pos = claim.find(np)\n",
        "        if search_pos != -1:\n",
        "            if page in results:\n",
        "                repeated_mention.append(page)\n",
        "                # results.insert(0, results.pop(results.index(page)))     # Fresh page to front if it was mention before\n",
        "            else:\n",
        "                results.append(page)\n",
        "            if loc == 0:\n",
        "                pass\n",
        "                # print(f\"Add: {page}, at page direct search, np={np}\")\n",
        "            elif loc == 1:\n",
        "                pass\n",
        "                # print(f\"Add: {page}, at match, new term={np}\")\n",
        "            mapping[page] = search_pos\n",
        "            tmp_muji.append(np)\n",
        "\n",
        "    def if_page_exists(page: str) -> bool:\n",
        "        import requests\n",
        "        url_base = \"https://zh.wikipedia.org/wiki/\"\n",
        "        new_url = [url_base + page, url_base + page.upper()]\n",
        "        for url in new_url:\n",
        "            r = requests.head(url)\n",
        "            if r.status_code == 200:\n",
        "                return True\n",
        "            else:\n",
        "                continue\n",
        "        return False\n",
        "    \n",
        "    def clean_time_format(np: str):\n",
        "        if (matched := re.search(r\"\\d+年\", np)) != None:\n",
        "            return True\n",
        "        if (matched := re.search(r\"\\d+月\\d+日\", np)) != None:\n",
        "            return True\n",
        "        if (matched := re.search(r\"\\d+小時\", np)) != None:\n",
        "            return True\n",
        "        if (matched := re.search(r\"\\d+天\", np)) != None:\n",
        "            return True\n",
        "        if (matched := re.search(r\"\\d+世紀\", np)) != None:\n",
        "            return True\n",
        "        if (matched := re.search(r\"\\d+年代\", np)) != None:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    def tokenize(text: str, stopwords: list) -> str:\n",
        "        import jieba\n",
        "        \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
        "\n",
        "        Args:\n",
        "            text (str): claim or wikipedia article\n",
        "            stopwords (list): common words that contribute little to the meaning of a sentence\n",
        "\n",
        "        Returns:\n",
        "            str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
        "        \"\"\"\n",
        "\n",
        "        tokens = jieba.cut(text)\n",
        "\n",
        "        return \" \".join([w for w in tokens if w not in stopwords])\n",
        "\n",
        "    for i, np in enumerate(nps):\n",
        "        # print(f\"searching {np}\")\n",
        "        quote_dup = False\n",
        "        if np in stopwords:         # 如果包含停用詞\n",
        "            continue\n",
        "        if clean_time_format(np):   # 如果包含時間\n",
        "            continue\n",
        "        \n",
        "        # Ignore parsing among quotation mark, for example, if《仲夏夜之夢》exists, ignore「仲夏夜」and「夢」\n",
        "        for search in quote_search:\n",
        "            if search.find(np) != -1:\n",
        "                quote_dup = True\n",
        "        if quote_dup == True:\n",
        "            continue\n",
        "\n",
        "        # Delete Bookname Mark, Quote Mark\n",
        "        np_no_quote = re.sub(r\"《|》|〈|〉|【|】|「|」|『|』|（|）\", \"\", np)\n",
        "        if np != np_no_quote:\n",
        "            quote_search.append(np_no_quote)\n",
        "            np = np_no_quote\n",
        "\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np)\n",
        "        ]\n",
        "\n",
        "        # Directly Search by Redirection\n",
        "            # Check if a page exists\n",
        "        if (if_page_exists(np)):\n",
        "            try:\n",
        "                page = do_st_corrections(wikipedia.page(title=np).title)\n",
        "                if page == np:\n",
        "                    # print(f\"Found, np={np}, page={page}\")\n",
        "                    post_processing(np=np, page=page, loc=0)\n",
        "                else:\n",
        "                    # print(f\"Redirect, np={np}, page={page}\")\n",
        "                    post_processing(np=np, page=page, loc=0)\n",
        "            except wikipedia.DisambiguationError as diserr:\n",
        "                page = do_st_corrections(wikipedia.search(np)[0])\n",
        "                if page == np:\n",
        "                    # print(f\"Disambig, np={np}, page={page}\")\n",
        "                    post_processing(np=np, page=page, loc=0)\n",
        "            except wikipedia.PageError as pageerr:\n",
        "                pass\n",
        "\n",
        "        # Remove the wiki page's description in brackets\n",
        "        wiki_set = [re.sub(r\"\\s\\(\\S+\\)\", \"\", w) for w in wiki_search_results]\n",
        "        wiki_df = pd.DataFrame({\n",
        "            \"wiki_set\": wiki_set,\n",
        "            \"wiki_results\": wiki_search_results\n",
        "        })\n",
        "\n",
        "        # Elements in wiki_set --> index\n",
        "        # Extracting only the first element is one way to avoid extracting\n",
        "        # too many of the similar wiki pages\n",
        "        grouped_df = wiki_df.groupby(\"wiki_set\", sort=False).first()\n",
        "        candidates = grouped_df[\"wiki_results\"].tolist()\n",
        "        # muji refers to wiki_set\n",
        "        muji = grouped_df.index.tolist()\n",
        "\n",
        "        for prefix, term in zip(muji, candidates):\n",
        "            if prefix not in tmp_muji:  #忽略掉括號，如果括號有重複的話。假設如果有\" 1 (數字)\", 則\"1 (符號)\" 會被忽略\n",
        "                matched = False\n",
        "\n",
        "                # Take at least one term from the first noun phrase\n",
        "                if i == 0:\n",
        "                    first_wiki_term.append(term)\n",
        "\n",
        "                # try:\n",
        "                #     print(term)\n",
        "                #     term_idx = wiki_pages.index[wiki_pages['id'] == do_st_corrections(term.replace(\" \", \"_\").replace(\"-\", \"\"))].tolist()[0]\n",
        "                #     processed_tokens = wiki_pages['processed_text'][term_idx]\n",
        "                #     processed_text_vector = vectorizer.transform([processed_tokens])\n",
        "                #     sim_score = cosine_similarity(processed_text_vector, claim_vector)[0][0]\n",
        "                #     if sim_score > 0.25: # 0.25 is hyperparam\n",
        "                #         score_mapping[term] = sim_score\n",
        "                #         print(sim_score, term)\n",
        "                # except IndexError:\n",
        "                #     pass\n",
        "                # except wikipedia.DisambiguationError:\n",
        "                #     pass\n",
        "                # except wikipedia.PageError:\n",
        "                #     pass\n",
        "\n",
        "                # Walrus operator :=\n",
        "                # https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions\n",
        "                # Through these filters, we are trying to figure out if the term\n",
        "                # is within the claim\n",
        "                if (((new_term := term) in claim) or\n",
        "                    ((new_term := term) in claim.replace(\" \", \"\")) or\n",
        "                    ((new_term := term.replace(\"·\", \"\")) in claim) or                                   # 過濾人名\n",
        "                    ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term)) in claim) or                          # 過濾空格 / 消歧義\n",
        "                    ((new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[0]) in claim and       # 消歧義與括號內皆有在裡面\n",
        "                     (new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[1]) in claim) or\n",
        "                    ((new_term := term.replace(\"-\", \" \")) in claim) or                                  # 過濾槓號\n",
        "                    ((new_term := term.lower()) in claim) or                                            # 過濾大小寫\n",
        "                    ((new_term := term.lower().replace(\"-\", \"\")) in claim) or                           # 過濾大小寫及槓號\n",
        "                    ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term.lower().replace(\"-\", \"\"))) in claim)    # 過濾大小寫、槓號及消歧義\n",
        "                    ):\n",
        "                    matched = True\n",
        "                    # print(new_term, term)\n",
        "\n",
        "                # 人名匹配\n",
        "                elif \"·\" in term:\n",
        "                    splitted = term.split(\"·\")\n",
        "                    if \"·\" not in claim:        # 要求claim顯示的不為全名，不然都需要全名\n",
        "                        for split in splitted:\n",
        "                            if (new_term := split) in claim:\n",
        "                                matched = True\n",
        "                                break\n",
        "\n",
        "                if matched:\n",
        "                    post_processing(np=new_term, page=term, loc=1)\n",
        "\n",
        "    # score_results = sorted(score_mapping, key=score_mapping.get)[:-5]\n",
        "\n",
        "    # 8 is a hyperparameter\n",
        "    if len(results) > 8:\n",
        "        assert -1 not in mapping.values()\n",
        "        # print(\"長度大於8\", results)\n",
        "\n",
        "        results = repeated_mention + sorted(mapping, key=mapping.get)[:8]\n",
        "        results = list(set(results))            # remove duplicates\n",
        "        # print(\"排序後\", results)\n",
        "    if len(results) < 1:\n",
        "        results = first_wiki_term\n",
        "        # print(\"第一搜尋結果\", results)\n",
        "    \n",
        "    print(results)\n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_search(\n",
        "        series_data: pd.Series, \n",
        "        ):\n",
        "    import wikipedia\n",
        "    import re\n",
        "    import opencc\n",
        "    import pandas as pd\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    wikipedia.set_lang(\"zh\")\n",
        "    CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "    CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "    def do_st_corrections(text: str) -> str:\n",
        "        simplified = CONVERTER_T2S.convert(text)\n",
        "        return CONVERTER_S2T.convert(simplified)\n",
        "    \n",
        "    def if_page_exists(page: str) -> bool:\n",
        "        import requests\n",
        "        url_base = \"https://zh.wikipedia.org/wiki/\"\n",
        "        new_url = [url_base + page, url_base + page.upper()]\n",
        "        for url in new_url:\n",
        "            r = requests.head(url)\n",
        "            if r.status_code == 200:\n",
        "                return True\n",
        "            else:\n",
        "                continue\n",
        "        return False\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    results = []\n",
        "    direct_results = []\n",
        "    nps = series_data[\"hanlp_results\"]\n",
        "\n",
        "    def post_processing(page):\n",
        "        page = do_st_corrections(page)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "\n",
        "    for i, np in enumerate(nps):\n",
        "        # print(f\"searching {np}\")\n",
        "\n",
        "        if (if_page_exists(np)):\n",
        "            try:\n",
        "                page = do_st_corrections(wikipedia.page(title=np).title)\n",
        "                if page == np:\n",
        "                    # print(f\"Found, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "                else:\n",
        "                    # print(f\"Redirect, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "            except wikipedia.DisambiguationError as diserr:\n",
        "                for option in diserr.options:\n",
        "                    option = do_st_corrections(option)\n",
        "                    if new_option := re.sub(r\"\\s\\(\\S+\\)\", \"\", option) in claim:\n",
        "                        # print(f\"Disambig, np={np}, page={option}, claim={claim}\")\n",
        "                        post_processing(option)\n",
        "                        direct_results.append(option)\n",
        "                    post_processing(option)\n",
        "                    results.append(option)\n",
        "                page = do_st_corrections(wikipedia.search(np)[0])\n",
        "                if page == np:\n",
        "                    # print(f\"Disambig, np={np}, page={page}, claim={claim}\")\n",
        "                    post_processing(page)\n",
        "                    direct_results.append(page)\n",
        "            except wikipedia.PageError as pageerr:\n",
        "                pass\n",
        "\n",
        "        # Simplified Traditional Chinese Correction\n",
        "        wiki_search_results = [\n",
        "            do_st_corrections(w) for w in wikipedia.search(np)\n",
        "        ]\n",
        "\n",
        "        for term in wiki_search_results:\n",
        "            if (((new_term := term) in claim) or\n",
        "                ((new_term := term) in claim.replace(\" \", \"\")) or\n",
        "                ((new_term := term.replace(\"·\", \"\")) in claim) or                                   # 過濾人名\n",
        "                ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term)) in claim) or                          # 過濾空格 / 消歧義\n",
        "                ((new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[0]) in claim and       # 消歧義與括號內皆有在裡面\n",
        "                    (new_term := term.replace(\"(\", \"\").replace(\")\", \"\").split()[1]) in claim) or\n",
        "                ((new_term := term.replace(\"-\", \" \")) in claim) or                                  # 過濾槓號\n",
        "                ((new_term := term.lower()) in claim) or                                            # 過濾大小寫\n",
        "                ((new_term := term.lower().replace(\"-\", \"\")) in claim) or                           # 過濾大小寫及槓號\n",
        "                ((new_term := re.sub(r\"\\s\\(\\S+\\)\", \"\", term.lower().replace(\"-\", \"\"))) in claim)    # 過濾大小寫、槓號及消歧義\n",
        "                ):\n",
        "                post_processing(term)\n",
        "                direct_results.append(term)\n",
        "            # if prefix not in tmp_muji:  #忽略掉括號，如果括號有重複的話。假設如果有\" 1 (數字)\", 則\"1 (符號)\" 會被忽略\n",
        "            post_processing(term)\n",
        "            results.append(term)\n",
        "\n",
        "    direct_results = list(set(direct_results))\n",
        "    results = list(set(results))            # remove duplicates\n",
        "    # print(results)\n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_sbert(\n",
        "    series_data: pd.Series, \n",
        "    tokenizing_method: callable,\n",
        "    # model: SentenceTransformer,\n",
        "    # wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        "    threshold: float\n",
        ") -> set:\n",
        "    # Disable huggingface tokenizor parallelism warning\n",
        "    import os\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
        "\n",
        "    import torch.cuda as cuda\n",
        "    cuda.empty_cache()\n",
        "    \n",
        "    # Parameters:\n",
        "    THRESHOLD_LOWEST = 0.6\n",
        "    THRESHOLD_SIM_LINE = threshold\n",
        "    WEIGHT_SIM_ID = 0.05    # The lower it is, the higher sim_id is when it directly matches claim.\n",
        "    \n",
        "    def sim_score_eval(sim_line, sim_id):\n",
        "        if len(claim) > 15:\n",
        "            if sim_line > THRESHOLD_SIM_LINE:\n",
        "                res = 2*(1.1*sim_line*1.1*sim_id)/(1.1*sim_line+1.1*sim_id)\n",
        "            else:\n",
        "                res = 0\n",
        "        else:\n",
        "            res = sim_id\n",
        "        \n",
        "        return res\n",
        "    \n",
        "    \n",
        "    def post_processing(page) -> str:\n",
        "        import opencc\n",
        "        CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "        CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "        simplified = CONVERTER_T2S.convert(page)\n",
        "        page = CONVERTER_S2T.convert(simplified)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        return page\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    search_list = series_data[\"predicted_pages\"]\n",
        "    direct_search = series_data[\"direct_match\"]\n",
        "    results = []\n",
        "    mapping = {}\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    emb_claim_tok = sbert_model.encode(tokens)\n",
        "    emb_claim = sbert_model.encode(claim)\n",
        "\n",
        "    search_list = [post_processing(id) for id in search_list]\n",
        "    '''\n",
        "    if series_data[\"label\"] != \"NOT ENOUGH INFO\":\n",
        "        gt_pages = set([\n",
        "            evidence[2]\n",
        "            for evidence_set in series_data[\"evidence\"]\n",
        "            for evidence in evidence_set\n",
        "        ])\n",
        "    else:\n",
        "        gt_pages = set([])\n",
        "    '''\n",
        "\n",
        "    for search_id in search_list:\n",
        "        # print(search_id)\n",
        "        search_series = wiki_pages.loc[wiki_pages['id'] == search_id]\n",
        "        if search_series.empty:\n",
        "            continue\n",
        "        try:\n",
        "            for temp in search_series[\"lines\"]:\n",
        "                search_lines = temp\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if len(search_lines) == 0:\n",
        "             continue\n",
        "        search_id_tok = tokenizing_method(search_id)\n",
        "        emb_id = sbert_model.encode(search_id_tok)\n",
        "        sim_id = util.pytorch_cos_sim(emb_id, emb_claim).numpy()\n",
        "        sim_id = sim_id[0][0]\n",
        "        new_sim_id = 0\n",
        "        if search_id in direct_search:\n",
        "            if sim_id > 0:\n",
        "                new_sim_id = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "            else:\n",
        "                sim_id = 0\n",
        "                new_sim_id = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "        else:\n",
        "            new_sim_id = sim_id\n",
        "\n",
        "        sim_score = 0\n",
        "        sim_line = 0\n",
        "        sim_line_b = 0\n",
        "\n",
        "        embs = sbert_model.encode_multi_process(search_lines, pool=pool)\n",
        "        for emb in embs:\n",
        "            sim = util.pytorch_cos_sim(emb, emb_claim).numpy()\n",
        "            sim = sim[0][0]\n",
        "            sim_line = max(sim, sim_line)\n",
        "\n",
        "        search_lines_tok = [tokenizing_method(line) for line in search_lines]\n",
        "        embs = sbert_model.encode_multi_process(search_lines_tok, pool=pool)\n",
        "        for emb in embs:\n",
        "            sim = util.pytorch_cos_sim(emb, emb_claim_tok).numpy()\n",
        "            sim = sim[0][0]\n",
        "            sim_line = max(sim, sim_line)\n",
        "\n",
        "        if sim_line > THRESHOLD_SIM_LINE:\n",
        "            sim_line = max(sim_line, sim_line_b)\n",
        "            sim_line_b = sim_line\n",
        "            sim_score = sim_score_eval(sim_line, new_sim_id)\n",
        "            sim_score = max(sim_score, sim_line_b)\n",
        "            # print(sim_score, sim_line, search_id)\n",
        "            if sim_score > THRESHOLD_LOWEST:\n",
        "                search_id = post_processing(search_id)\n",
        "                if search_id in mapping:\n",
        "                    mapping[search_id] = max(sim_score, mapping[search_id])\n",
        "                else:\n",
        "                    mapping[search_id] = sim_score\n",
        "\n",
        "    mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "    # print(mapping_sorted[:topk])\n",
        "    DIFF = 0.125\n",
        "    for k, v in mapping_sorted:\n",
        "        THRESHOLD_TOP = v\n",
        "        break\n",
        "    if len(mapping_sorted) >= topk:\n",
        "        results = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "    else:\n",
        "        results = [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "    if not results:\n",
        "        results = [k for k, v in mapping_sorted][:topk]\n",
        "    if not results:\n",
        "        results = series_data[\"direct_match\"]\n",
        "    if not results:\n",
        "        results = series_data[\"predicted_pages\"][:topk]\n",
        "    # print(results)\n",
        "\n",
        "    # Analysis on missed pages\n",
        "    '''\n",
        "    if series_data[\"label\"] != \"NOT ENOUGH INFO\":\n",
        "        for page in gt_pages:\n",
        "            if page in mapping:\n",
        "                if page not in results:\n",
        "                    print(f\"Missed: ID={page}, score={mapping[page]}\")\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                if page not in search_list:\n",
        "                    print(f\"Missed: ID={page}, not in search_list\")\n",
        "                else:\n",
        "                    print(f\"Missed: ID={page}, score < {THRESHOLD_LOWEST}\")\n",
        "\n",
        "    df = pd.DataFrame(df_res, columns=['Claim', 'Search_ID', 'Sim_ID', 'Sim_ID_Adjusted', 'Sim_Line', 'Sim_Score'])\n",
        "\n",
        "    with open(\"data/train_doc5_sbert_logging.jsonl\", \"a\", encoding=\"utf8\") as f:\n",
        "        f.write(df.to_json(orient='records', lines=True, force_ascii=False))\n",
        "    '''\n",
        "\n",
        "    return set(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claim = \"天衛三軌道在天王星內部的磁層，以《 仲夏夜之夢 》作者緹坦妮雅命名。\"\n",
        "proof = \"1787年由威廉·赫雪爾發現，並以威廉·莎士比亞的《仲夏夜之夢》中的妖精王后緹坦妮雅命名。\"\n",
        "claim_tok = tokenize(claim, stopwords=stopwords)\n",
        "proof_tok = tokenize(proof, stopwords=stopwords)\n",
        "print(claim_tok)\n",
        "print(proof_tok)\n",
        "\n",
        "emb_claim = sbert_model.encode(claim_tok)\n",
        "emb_proof = sbert_model.encode(proof_tok)\n",
        "print(util.pytorch_cos_sim(emb_proof, emb_claim).numpy()[0][0])\n",
        "\n",
        "emb_claim = sbert_model.encode(claim)\n",
        "emb_proof = sbert_model.encode(proof)\n",
        "print(util.pytorch_cos_sim(emb_proof, emb_claim).numpy()[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_tfidf(\n",
        "    series_data: pd.Series, \n",
        "    tokenizing_method: callable,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
        "    wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        "    threshold: float\n",
        ") -> set:\n",
        "    import numpy as np\n",
        "    import scipy\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    claim = series_data[\"claim\"]\n",
        "    search_list = series_data[\"predicted_pages\"]\n",
        "    results = []\n",
        "    mapping = {}\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    claim_vector = vectorizer.transform([tokens])\n",
        "    for search_id in search_list:\n",
        "        search_tokens = wiki_pages.loc[wiki_pages['id'] == search_id]\n",
        "        if search_tokens.empty:\n",
        "            continue\n",
        "        search_processed_text = search_tokens[\"processed_text\"]\n",
        "        search_vector = vectorizer.transform(search_processed_text)\n",
        "        sim_scores = cosine_similarity(search_vector, claim_vector)\n",
        "        sim_scores = sim_scores[0][0]\n",
        "        if sim_scores > threshold:\n",
        "            mapping[search_id] = sim_scores\n",
        "            # print(sim_scores, search_id)\n",
        "\n",
        "    # print(mapping)\n",
        "    results = sorted(mapping, key=mapping.get, reverse=True)[:topk]\n",
        "    # print(results)\n",
        "    return set(results)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Get noun phrases from hanlp consituency parsing tree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup [HanLP](https://github.com/hankcs/HanLP) predictor (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                             \r"
          ]
        }
      ],
      "source": [
        "predictor = (hanlp.pipeline().append(\n",
        "    hanlp.load(\"FINE_ELECTRA_SMALL_ZH\"),\n",
        "    output_key=\"tok\",\n",
        ").append(\n",
        "    hanlp.load(\"CTB9_CON_ELECTRA_SMALL\"),\n",
        "    output_key=\"con\",\n",
        "    input_key=\"tok\",\n",
        "))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will skip this process which for creating parsing tree when demo on class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_file = f\"data/hanlp_con_results.pkl\"\n",
        "if Path(hanlp_file).exists():\n",
        "    with open(hanlp_file, \"rb\") as f:\n",
        "        hanlp_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_results = [get_nps_hanlp(predictor, d) for d in TRAIN_DATA]\n",
        "    with open(hanlp_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_path = f\"data/train_doc5.jsonl\"\n",
        "doc_path_aicup = f\"data/train_doc5_aicup.jsonl\"\n",
        "doc_path_sbert = f\"data/train_doc5_sbert.jsonl\"\n",
        "doc_path_search = f\"data/train_doc5_search.jsonl\"\n",
        "doc_path_tfidf = f\"data/train_doc5_tfidf.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Path(doc_path).exists():\n",
        "    with open(doc_path_aicup, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ])\n",
        "else:\n",
        "    if Path(doc_path_search).exists():\n",
        "        with open(doc_path_search, \"r\", encoding=\"utf8\") as f:\n",
        "            predicted_results_search = pd.Series([\n",
        "                set(json.loads(line)[\"predicted_pages\"])\n",
        "                for line in f\n",
        "            ], name=\"search\")\n",
        "    else:\n",
        "        pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "        train_df = pd.DataFrame(TRAIN_DATA)\n",
        "        train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "        # predicted_results = train_df.progress_apply(get_pred_pages, axis=1)\n",
        "        predicted_results_search = train_df.parallel_apply(\n",
        "            get_pred_pages_search, axis=1)\n",
        "        save_doc(TRAIN_DATA, predicted_results_search, mode=\"train\", suffix=\"_search\")\n",
        "\n",
        "    if Path(doc_path_aicup).exists():\n",
        "        with open(doc_path_aicup, \"r\", encoding=\"utf8\") as f:\n",
        "            predicted_results_aicup = pd.Series([\n",
        "                set(json.loads(line)[\"predicted_pages\"])\n",
        "                for line in f\n",
        "            ], name=\"aicup\")\n",
        "    else:\n",
        "        pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "        train_df = pd.DataFrame(TRAIN_DATA)\n",
        "        train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "        # predicted_results = train_df.progress_apply(get_pred_pages, axis=1)\n",
        "        predicted_results_aicup = train_df.parallel_apply(\n",
        "            get_pred_pages, axis=1)\n",
        "        save_doc(TRAIN_DATA, predicted_results_aicup, mode=\"train\", suffix=\"_aicup\")\n",
        "\n",
        "    # if Path(doc_path_tfidf).exists():\n",
        "    #     with open(doc_path_tfidf, \"r\", encoding=\"utf8\") as f:\n",
        "    #         predicted_results_tfidf = pd.Series([\n",
        "    #             set(json.loads(line)[\"predicted_pages\"])\n",
        "    #             for line in f\n",
        "    #         ], name=\"tfidf\")\n",
        "    # else:\n",
        "    #     pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=3)\n",
        "    #     TRAIN_DATA_SEARCH = load_json(doc_path_search)\n",
        "    #     train_df_search = pd.DataFrame(TRAIN_DATA_SEARCH)\n",
        "    #     predicted_results_tfidf = train_df_search.parallel_apply(\n",
        "    #         partial(\n",
        "    #             get_pred_pages_tfidf,\n",
        "    #             tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "    #             vectorizer=vectorizer,\n",
        "    #             tf_idf_matrix=X,\n",
        "    #             wiki_pages=wiki_pages,\n",
        "    #             topk=topk,\n",
        "    #             threshold=0.65\n",
        "    #         ), axis=1)\n",
        "    #     save_doc(TRAIN_DATA, predicted_results_tfidf, mode=\"train\", suffix=\"_tfidf\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On Search Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/train_doc5_search_backup.jsonl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1030281/4245773708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdoc_path_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"data/train_doc5_search.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdoc_path_search_backup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"data/train_doc5_search_backup.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mTRAIN_DATA_SEARCH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_path_search_backup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpandarallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Gaber_AICUP2023/utils.py\u001b[0m in \u001b[0;36mload_json\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m946\u001b[0m \u001b[0mrows\u001b[0m \u001b[0mx\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mjson_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_doc5_search_backup.jsonl'"
          ]
        }
      ],
      "source": [
        "def clean(series_data):\n",
        "    def post_processing(page) -> str:\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        return page\n",
        "    \n",
        "    result = []\n",
        "    for element in series_data:\n",
        "        # print(series_data)\n",
        "        element = post_processing(element)\n",
        "        if \"Template:\" in element:\n",
        "            continue\n",
        "        result.append(element)\n",
        "\n",
        "    return set(result)\n",
        "doc_path_search = f\"data/train_doc5_search.jsonl\"\n",
        "doc_path_search_backup = f\"data/train_doc5_search_backup.jsonl\"\n",
        "TRAIN_DATA_SEARCH1 = load_json(doc_path_search_backup)\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=12)\n",
        "train_df = pd.DataFrame(TRAIN_DATA)\n",
        "train_df.loc[:, \"hanlp_results\"] = hanlp_results\n",
        "train_df_search1 = pd.DataFrame(TRAIN_DATA_SEARCH1)\n",
        "predicted_results_search = train_df_search1.loc[:, \"predicted_pages\"]\n",
        "direct_match = train_df_search1.loc[:, \"direct_match\"]\n",
        "# print(predicted_results_search)\n",
        "predicted_results_search = predicted_results_search.apply(clean)\n",
        "direct_match = direct_match.apply(clean)\n",
        "\n",
        "# predicted_results = train_df.progress_apply(get_pred_pages, axis=1)\n",
        "save_doc(TRAIN_DATA, predicted_results_search, mode=\"train\", suffix=\"_search\", col_name=\"predicted_pages\")\n",
        "TRAIN_DATA_SEARCH = load_json(doc_path_search)\n",
        "# direct_match = train_df.parallel_apply(\n",
        "#     get_pred_pages_search, axis=1)\n",
        "save_doc(TRAIN_DATA_SEARCH, direct_match, mode=\"train\", suffix=\"_search\", col_name=\"direct_match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# num_of_samples = 3969\n",
        "num_of_samples = 500\n",
        "start = 0\n",
        "TRAIN_DATA_SEARCH = load_json(doc_path_search)\n",
        "train_df_search = pd.DataFrame(TRAIN_DATA_SEARCH[start:start+num_of_samples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Path(doc_path_sbert).exists():\n",
        "    with open(doc_path_sbert, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results_sbert = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ], name=\"sbert\")\n",
        "else:\n",
        "    pandarallel.initialize(progress_bar=False, verbose=0, nb_workers=10)\n",
        "    predicted_results_sbert = train_df_search.progress_apply(\n",
        "        partial(\n",
        "            get_pred_pages_sbert,\n",
        "            tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "            # model=sbert_model,\n",
        "            # wiki_pages=wiki_pages,\n",
        "            topk=5,\n",
        "            threshold=0.375\n",
        "        ), axis=1)\n",
        "    save_doc(TRAIN_DATA[start:start+num_of_samples], predicted_results_sbert, mode=\"train\", suffix=\"_sbert1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_precision = 0.4831244778613204\n",
        "old_recall = 0.8928989139515455\n",
        "old_f1 = 0.6269970759980318"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On SBERT Data:\n",
            "Precision: 0.48475355054302455\n",
            "(Diff: 0.0016290726817041468)\n",
            "Recall: 0.8845446950710107\n",
            "(Diff: -0.00835421888053478)\n",
            "F1-Score: 0.6262860307067561\n",
            "(Diff: -0.000711045291275636)\n"
          ]
        }
      ],
      "source": [
        "print(\"On SBERT Data:\")\n",
        "precision = calculate_precision(TRAIN_DATA[:num_of_samples], predicted_results_sbert)\n",
        "print(f\"(Diff: {precision-old_precision})\")\n",
        "recall = calculate_recall(TRAIN_DATA[:num_of_samples], predicted_results_sbert)\n",
        "print(f\"(Diff: {recall-old_recall})\")\n",
        "f1 = calculate_f1(precision, recall)\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"(Diff: {f1-old_f1})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sbert_model.stop_multi_process_pool(pool=pool)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Operation on Log File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_log = \"data/train_doc5_sbert_logging.jsonl\"\n",
        "TRAIN_DATA_LOG = load_json(doc_log)\n",
        "\n",
        "train_df_log = pd.DataFrame(TRAIN_DATA_LOG)\n",
        "\n",
        "import jieba.analyse\n",
        "sentence = \"荷馬爲虛構人物，兩部史詩整理成集者爲其他人。\"\n",
        "print(tokenize(sentence, stopwords=stopwords))\n",
        "print(jieba.analyse.extract_tags(sentence, topK=5))\n",
        "print(jieba.analyse.textrank(sentence, topK=5, withWeight=False, allowPOS=('ns', 'n', 'nr', 'nz', 'nt', 'nw')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pred_pages_log(\n",
        "    data: pd.DataFrame, \n",
        "    topk: int,\n",
        "    threshold: float\n",
        "):\n",
        "    # Parameters:\n",
        "    THRESHOLD_LOWEST = 0.6\n",
        "    THRESHOLD_MID = 0.7\n",
        "    THRESHOLD_HIGHEST = 0.885\n",
        "    THRESHOLD_SIM_LINE = threshold\n",
        "    WEIGHT_SIM_ID = 0.05    # The lower it is, the higher sim_id is when it directly matches claim.\n",
        "    \n",
        "    def sim_score_eval(sim_line, sim_id, claim):\n",
        "        # res = (weight_id + weight_line)*(s1*s2)/(weight_line*s1+weight_id*s2)\n",
        "        # if sim_line > 0.5:\n",
        "        #     res = sim_line + (1-sim_line)*sim_id\n",
        "        # elif sim_line < 0.5 and sim_id > 0.5:\n",
        "        #     res = sim_line + (1-sim_line)*sim_id\n",
        "        # else:\n",
        "        #     res = 0\n",
        "        if len(claim) > 15:\n",
        "            w_line = 1.1\n",
        "            w_id = 1.1\n",
        "            if sim_line > THRESHOLD_SIM_LINE:\n",
        "                res = 2*(w_line*sim_line*w_id*sim_id)/(w_line*sim_line+w_id*sim_id)\n",
        "            else:\n",
        "                res = 0\n",
        "        else:\n",
        "            res = sim_id\n",
        "        \n",
        "        return res\n",
        "    \n",
        "    def post_processing(page) -> str:\n",
        "        import opencc\n",
        "        CONVERTER_T2S = opencc.OpenCC(\"t2s.json\")\n",
        "        CONVERTER_S2T = opencc.OpenCC(\"s2t.json\")\n",
        "    \n",
        "        simplified = CONVERTER_T2S.convert(page)\n",
        "        page = CONVERTER_S2T.convert(simplified)\n",
        "        page = page.replace(\" \", \"_\")\n",
        "        page = page.replace(\"-\", \"\")\n",
        "        return page\n",
        "\n",
        "    results = []\n",
        "    doc_res = []\n",
        "    mapping = {}\n",
        "    claim_prev = \"\"\n",
        "    claim_count = 0\n",
        "\n",
        "    for index, series_data in data.iterrows():\n",
        "        claim = series_data[\"Claim\"]\n",
        "        search_id = series_data[\"Search_ID\"]\n",
        "        sim_id = series_data[\"Sim_ID\"]\n",
        "        sim_id_new = series_data[\"Sim_ID_Adjusted\"]\n",
        "        sim_line = series_data[\"Sim_Line\"]\n",
        "\n",
        "        if index == 0:  \n",
        "            claim_prev = claim\n",
        "        elif claim != claim_prev:\n",
        "            mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "            DIFF = 0.125\n",
        "            for k, v in mapping_sorted:\n",
        "                THRESHOLD_TOP = v\n",
        "                break\n",
        "            print(mapping_sorted[:topk])\n",
        "            if len(mapping_sorted) >= topk:\n",
        "                doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "            else:\n",
        "                doc_res= [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "            \n",
        "            print(doc_res)\n",
        "            results.append(doc_res)\n",
        "            #print(claim_count, mapping)\n",
        "            doc_res = []\n",
        "            mapping = {}\n",
        "            claim_prev = claim\n",
        "            claim_count = claim_count + 1\n",
        "            if claim_count % 100 == 0:\n",
        "                print(f\"已處理{claim_count}筆資料\")\n",
        "\n",
        "        # if sim_id != sim_id_new:\n",
        "        #     if sim_id > 0:\n",
        "        #         # print(f\"{search_id}: sim_id={sim_id}\")\n",
        "        #         sim_id_new = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "        #     else:\n",
        "        #         sim_id = 0\n",
        "        #         sim_id_new = 1-((1-sim_id)*WEIGHT_SIM_ID)\n",
        "        # else:\n",
        "        #     sim_id_new = sim_id\n",
        "\n",
        "        sim_score = sim_score_eval(sim_line=sim_line, sim_id=sim_id_new, claim=claim)\n",
        "        if sim_score > 0:\n",
        "            sim_score = max(sim_score, sim_line)\n",
        "            # print(sim_score, search_id)\n",
        "            if sim_score > THRESHOLD_LOWEST:\n",
        "                search_id = post_processing(search_id)\n",
        "                if search_id in mapping:\n",
        "                    mapping[search_id] = max(sim_score, mapping[search_id])\n",
        "                else:\n",
        "                    mapping[search_id] = sim_score\n",
        "\n",
        "    mapping_sorted = sorted(mapping.items(), key=lambda x:x[1], reverse=True)\n",
        "    # print(mapping_sorted[:topk])\n",
        "    # if len(mapping_sorted) >= topk:\n",
        "    #     doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_HIGHEST][:topk]\n",
        "    # else:\n",
        "    #     doc_res= [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "\n",
        "    # if not doc_res:\n",
        "    #     doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_MID][:topk]\n",
        "    if len(mapping_sorted) >= topk:\n",
        "        doc_res = [k for k, v in mapping_sorted if v > THRESHOLD_TOP-DIFF][:topk]\n",
        "    else:\n",
        "        doc_res= [k for k, v in mapping_sorted if v > THRESHOLD_LOWEST][:topk]\n",
        "    # print(doc_res)\n",
        "    results.append(doc_res)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_results_log = get_pred_pages_log(data=train_df_log, topk=5, threshold=0.375)\n",
        "predicted_results_log_df = pd.DataFrame(predicted_results_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge(series_data: pd.Series) -> set:\n",
        "    result = []\n",
        "    for i in range(0, 5):\n",
        "        if series_data.iloc[i] != None:\n",
        "            result.append(series_data.iloc[i])\n",
        "    print(set(result))\n",
        "    return set(result)\n",
        "predicted_results_log_df_b = predicted_results_log_df.apply(merge, axis=1)\n",
        "save_doc(TRAIN_DATA[:num_of_samples], predicted_results_log_df_b, mode=\"train\", suffix=\"_log\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "old_precision = 0.4831244778613204\n",
        "old_recall = 0.8928989139515455\n",
        "old_f1 = 0.6269970759980318"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"On Log Data:\")\n",
        "precision = calculate_precision(TRAIN_DATA[:num_of_samples], predicted_results_log_df_b)\n",
        "print(f\"(Diff: {precision-old_precision})\")\n",
        "recall = calculate_recall(TRAIN_DATA[:num_of_samples], predicted_results_log_df_b)\n",
        "print(f\"(Diff: {recall-old_recall})\")\n",
        "f1 = calculate_f1(precision, recall)\n",
        "print(f\"F1-Score: {f1}\")\n",
        "print(f\"(Diff: {f1-old_f1})\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge Two Pandas Series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def union_result(series_data: pd.Series,) -> set:\n",
        "    aicup = series_data[\"aicup\"]\n",
        "    sbert = series_data[\"sbert\"]\n",
        "    print(aicup, sbert)\n",
        "    return set(aicup).union(set(sbert))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.merge(pd.Series([line for line in predicted_results_aicup[:500]], name=\"aicup\"), \n",
        "                      pd.Series([line for line in predicted_results_sbert], name=\"sbert\"), right_index=True, left_index=True)\n",
        "predicted_results = results_df.apply(union_result, axis=1)\n",
        "save_doc(TRAIN_DATA[:500], predicted_results, mode=\"train\", suffix=\"_temp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_path_temp = f\"data/train_doc5_temp.jsonl\"\n",
        "\n",
        "if Path(doc_path).exists():\n",
        "    with open(doc_path, \"r\", encoding=\"utf8\") as f:\n",
        "        predicted_results = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ])\n",
        "else:\n",
        "    pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=3)\n",
        "    TRAIN_DATA_TEMP = load_json(doc_path_temp)\n",
        "    train_df_temp = pd.DataFrame(TRAIN_DATA_TEMP)\n",
        "    predicted_results = train_df_temp.parallel_apply(\n",
        "        partial(\n",
        "            get_pred_pages_tfidf,\n",
        "            tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "            vectorizer=vectorizer,\n",
        "            tf_idf_matrix=X,\n",
        "            wiki_pages=wiki_pages,\n",
        "            topk=topk,\n",
        "            threshold=0,\n",
        "        ), axis=1)\n",
        "    save_doc(TRAIN_DATA, predicted_results, mode=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"On Data:\")\n",
        "precision = calculate_precision(TRAIN_DATA[:500], predicted_results)\n",
        "recall = calculate_recall(TRAIN_DATA[:500], predicted_results)\n",
        "print(calculate_f1(precision, recall))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Calculate our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision = 0.143234\n",
        "recall = 0.635255\n",
        "print(calculate_f1(precision, recall))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "precision = calculate_precision(TRAIN_DATA, predicted_results)\n",
        "recall = calculate_recall(TRAIN_DATA, predicted_results)\n",
        "f1_score = 2*(precision*recall)/(precision+recall)\n",
        "print(f\"F1 Score: {f1_score}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Repeat the same process on test set\n",
        "Create parsing tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "hanlp_test_file = f\"data/hanlp_con_test_results.pkl\"\n",
        "if Path(hanlp_test_file).exists():\n",
        "    with open(hanlp_test_file, \"rb\") as f:\n",
        "        hanlp_test_results = pickle.load(f)\n",
        "else:\n",
        "    hanlp_test_results = [get_nps_hanlp(predictor, d) for d in TEST_DATA]\n",
        "    with open(hanlp_test_file, \"wb\") as f:\n",
        "        pickle.dump(hanlp_test_results, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get pages via wiki online api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_doc_path = f\"data/test_doc5.jsonl\"\n",
        "test_doc_path_aicup = f\"data/test_doc5_aicup.jsonl\"\n",
        "test_doc_path_search = f\"data/test_doc5_search.jsonl\"\n",
        "test_doc_path_tfidf = f\"data/test_doc5_tfidf.jsonl\"\n",
        "test_doc_path_sbert = f\"data/test_doc5_sbert.jsonl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Path(test_doc_path_search).exists():\n",
        "    with open(test_doc_path_search, \"r\", encoding=\"utf8\") as f:\n",
        "        test_results_search = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ], name=\"search\")\n",
        "else:\n",
        "    pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)\n",
        "    test_df = pd.DataFrame(TEST_DATA)\n",
        "    test_df.loc[:, \"hanlp_results\"] = hanlp_test_results\n",
        "    # predicted_results = test_df.progress_apply(get_pred_pages, axis=1)\n",
        "    test_results_search = test_df.parallel_apply(\n",
        "        get_pred_pages_search, axis=1)\n",
        "    save_doc(TEST_DATA, test_results_search, mode=\"test\", suffix=\"_search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3774324 0.3774324 滷水蛋\n",
            "0.376664 0.376664 高志斌\n",
            "0.37533295 0.37533295 童子蛋\n",
            "0.4099716 0.4099716 高志斌_(臺灣)\n",
            "0.38409573 0.38409573 麥茶\n",
            "0.38726097 0.38726097 東方美人茶\n",
            "[]\n",
            "['茶葉蛋', '茶']\n"
          ]
        }
      ],
      "source": [
        "TEST_DATA_SEARCH = load_json(test_doc_path_search)\n",
        "test_df_search = pd.DataFrame(TEST_DATA_SEARCH)\n",
        "res = get_pred_pages_sbert(\n",
        "    series_data=test_df_search.loc[930],\n",
        "    tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "    # model=sbert_model,\n",
        "    # wiki_pages=wiki_pages,\n",
        "    topk=5,\n",
        "    threshold=0.375\n",
        ")\n",
        "print(list(res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef238e4d4ae4d56a31e6ce4e131ecc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/989 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if Path(test_doc_path_sbert).exists():\n",
        "    with open(test_doc_path_sbert, \"r\", encoding=\"utf8\") as f:\n",
        "        test_results_sbert = pd.Series([\n",
        "            set(json.loads(line)[\"predicted_pages\"])\n",
        "            for line in f\n",
        "        ], name=\"sbert\")\n",
        "else:\n",
        "    pandarallel.initialize(progress_bar=False, verbose=0, nb_workers=10)\n",
        "    TEST_DATA_SEARCH = load_json(test_doc_path_search)\n",
        "    test_df_search = pd.DataFrame(TEST_DATA_SEARCH)\n",
        "    test_results_sbert = test_df_search.progress_apply(\n",
        "        partial(\n",
        "            get_pred_pages_sbert,\n",
        "            tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "            # model=sbert_model,\n",
        "            # wiki_pages=wiki_pages,\n",
        "            topk=5,\n",
        "            threshold=0.375\n",
        "        ), axis=1)\n",
        "    save_doc(TEST_DATA, test_results_sbert, mode=\"test\", suffix=\"_sbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df = pd.merge(pd.Series([line for line in test_results_aicup], name=\"aicup\"), \n",
        "                      pd.Series([line for line in test_results_tfidf], name=\"tfidf\"), right_index=True, left_index=True)\n",
        "test_results = results_df.apply(union_result, axis=1)\n",
        "save_doc(TEST_DATA, test_results, mode=\"test\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol4zFkSbjgXF"
      },
      "source": [
        "notebook2\n",
        "## PART 2. Sentence retrieval"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import some libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "GlliDsgXjisj"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple, Union\n",
        "\n",
        "# third-party libs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset, Dataset\n",
        "\n",
        "# local libs\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "J3BBLE3_hlPi"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/public_train.jsonl\")\n",
        "TEST_DATA = load_json(\"data/public_test.jsonl\")\n",
        "DOC_DATA = load_json(\"data/train_doc5.jsonl\")\n",
        "\n",
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "_y = [LABEL2ID[data[\"label\"]] for data in TRAIN_DATA]\n",
        "# GT means Ground Truth\n",
        "TRAIN_GT, DEV_GT = train_test_split(\n",
        "    DOC_DATA,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    shuffle=True,\n",
        "    stratify=_y,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (1 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and concatenating jsonl files in data/wiki-pages\n",
            "Generate parse mapping\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcae48380b4c464bbad9a8851cac58e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=118776), Label(value='0 / 118776')…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to id to evidence_map mapping\n"
          ]
        }
      ],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate precision for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_precision(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate precision for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of precision)\n",
        "        [2]: retrieved (denominator of precision)\n",
        "    \"\"\"\n",
        "    this_precision = 0.0\n",
        "    this_precision_hits = 0.0\n",
        "\n",
        "    # Return 0, 0 if label is not enough info since not enough info does not\n",
        "    # contain any evidence.\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # e[2] is the page title, e[3] is the sentence index\n",
        "        all_evi = [[e[2], e[3]]\n",
        "                   for eg in instance[\"evidence\"]\n",
        "                   for e in eg\n",
        "                   if e[3] is not None]\n",
        "        claim = instance[\"claim\"]\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for prediction in predicted_evidence:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "        return (this_precision /\n",
        "                this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
        "\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate recall for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evidence_macro_recall(\n",
        "    instance: Dict,\n",
        "    top_rows: pd.DataFrame,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Calculate recall for sentence retrieval\n",
        "    This function is modified from fever-scorer.\n",
        "    https://github.com/sheffieldnlp/fever-scorer/blob/master/src/fever/scorer.py\n",
        "\n",
        "    Args:\n",
        "        instance (dict): a row of the dev set (dev.jsonl) of test set (test.jsonl)\n",
        "        top_rows (pd.DataFrame): our predictions with the top probabilities\n",
        "\n",
        "        IMPORTANT!!!\n",
        "        instance (dict) should have the key of `evidence`.\n",
        "        top_rows (pd.DataFrame) should have a column `predicted_evidence`.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]:\n",
        "        [1]: relevant and retrieved (numerator of recall)\n",
        "        [2]: relevant (denominator of recall)\n",
        "    \"\"\"\n",
        "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # If there's no evidence to predict, return 1\n",
        "        if len(instance[\"evidence\"]) == 0 or all(\n",
        "            [len(eg) == 0 for eg in instance]):\n",
        "            return 1.0, 1.0\n",
        "\n",
        "        claim = instance[\"claim\"]\n",
        "\n",
        "        predicted_evidence = top_rows[top_rows[\"claim\"] ==\n",
        "                                      claim][\"predicted_evidence\"].tolist()\n",
        "\n",
        "        for evidence_group in instance[\"evidence\"]:\n",
        "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "            if all([item in predicted_evidence for item in evidence]):\n",
        "                # We only want to score complete groups of evidence. Incomplete\n",
        "                # groups are worthless.\n",
        "                return 1.0, 1.0\n",
        "        return 0.0, 1.0\n",
        "    return 0.0, 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the scores of sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_retrieval(\n",
        "    probs: np.ndarray,\n",
        "    df_evidences: pd.DataFrame,\n",
        "    ground_truths: pd.DataFrame,\n",
        "    top_n: int = 5,\n",
        "    cal_scores: bool = True,\n",
        "    save_name: str = None,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Calculate the scores of sentence retrieval\n",
        "\n",
        "    Args:\n",
        "        probs (np.ndarray): probabilities of the candidate retrieved sentences\n",
        "        df_evidences (pd.DataFrame): the candiate evidence sentences paired with claims\n",
        "        ground_truths (pd.DataFrame): the loaded data of dev.jsonl or test.jsonl\n",
        "        top_n (int, optional): the number of the retrieved sentences. Defaults to 2.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: F1 score, precision, and recall\n",
        "    \"\"\"\n",
        "    df_evidences[\"prob\"] = probs\n",
        "    top_rows = (\n",
        "        df_evidences.groupby(\"claim\").apply(\n",
        "        lambda x: x.nlargest(top_n, \"prob\"))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if cal_scores:\n",
        "        macro_precision = 0\n",
        "        macro_precision_hits = 0\n",
        "        macro_recall = 0\n",
        "        macro_recall_hits = 0\n",
        "\n",
        "        for i, instance in enumerate(ground_truths):\n",
        "            macro_prec = evidence_macro_precision(instance, top_rows)\n",
        "            macro_precision += macro_prec[0]\n",
        "            macro_precision_hits += macro_prec[1]\n",
        "\n",
        "            macro_rec = evidence_macro_recall(instance, top_rows)\n",
        "            macro_recall += macro_rec[0]\n",
        "            macro_recall_hits += macro_rec[1]\n",
        "\n",
        "        pr = (macro_precision /\n",
        "              macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
        "        rec = (macro_recall /\n",
        "               macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
        "        f1 = 2.0 * pr * rec / (pr + rec)\n",
        "\n",
        "    if save_name is not None:\n",
        "        # write doc7_sent5 file\n",
        "        with open(f\"data/{save_name}\", \"w\", encoding=\"utf8\") as f:\n",
        "            for instance in ground_truths:\n",
        "                claim = instance[\"claim\"]\n",
        "                predicted_evidence = top_rows[\n",
        "                    top_rows[\"claim\"] == claim][\"predicted_evidence\"].tolist()\n",
        "                instance[\"predicted_evidence\"] = predicted_evidence\n",
        "                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    if cal_scores:\n",
        "        return {\"F1 score\": f1, \"Precision\": pr, \"Recall\": rec}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference script to get probabilites for the candidate evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_predicted_probs(\n",
        "    model: nn.Module,\n",
        "    dataloader: Dataset,\n",
        "    device: torch.device,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Inference script to get probabilites for the candidate evidence sentences\n",
        "\n",
        "    Args:\n",
        "        model: the one from HuggingFace Transformers\n",
        "        dataloader: devset or testset in torch dataloader\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: probabilites of the candidate evidence sentences\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            probs.extend(torch.softmax(logits, dim=1)[:, 1].tolist())\n",
        "\n",
        "    return np.array(probs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentRetrievalBERTDataset(BERTDataset):\n",
        "    \"\"\"AicupTopkEvidenceBERTDataset class for AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        sentA = item[\"claim\"]\n",
        "        sentB = item[\"text\"]\n",
        "\n",
        "        # claim [SEP] text\n",
        "        concat = self.tokenizer(\n",
        "            sentA,\n",
        "            sentB,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(item[\"label\"])\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function for sentence retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "gpvXpFwXszfv"
      },
      "outputs": [],
      "source": [
        "def pair_with_wiki_sentences(\n",
        "    mapping: Dict[str, Dict[int, str]],\n",
        "    df: pd.DataFrame,\n",
        "    negative_ratio: float,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating train sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    # positive\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "        evidence_sets = df[\"evidence\"].iloc[i]\n",
        "        for evidence_set in evidence_sets:\n",
        "            sents = []\n",
        "            for evidence in evidence_set:\n",
        "                # evidence[2] is the page title\n",
        "                page = evidence[2].replace(\" \", \"_\")\n",
        "                # the only page with weird name\n",
        "                if page == \"臺灣海峽危機#第二次臺灣海峽危機（1958）\":\n",
        "                    continue\n",
        "                # evidence[3] is in form of int however, mapping requires str\n",
        "                sent_idx = str(evidence[3])\n",
        "                sents.append(mapping[page][sent_idx])\n",
        "\n",
        "            whole_evidence = \" \".join(sents)\n",
        "\n",
        "            claims.append(claim)\n",
        "            sentences.append(whole_evidence)\n",
        "            labels.append(1)\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "            continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        evidence_set = set([(evidence[2], evidence[3])\n",
        "                            for evidences in df[\"evidence\"][i]\n",
        "                            for evidence in evidences])\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [\n",
        "                    (page, sent_idx) for sent_idx in mapping[page].keys()\n",
        "                ]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for pair in page_sent_id_pairs:\n",
        "                if pair in evidence_set:\n",
        "                    continue\n",
        "                text = mapping[page][pair[1]]\n",
        "                # `np.random.rand(1) <= 0.05`: Control not to add too many negative samples\n",
        "                if text != \"\" and np.random.rand(1) <= negative_ratio:\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    labels.append(0)\n",
        "\n",
        "    return pd.DataFrame({\"claim\": claims, \"text\": sentences, \"label\": labels})\n",
        "\n",
        "\n",
        "def pair_with_wiki_sentences_eval(\n",
        "    mapping: Dict[str, Dict[int, str]],\n",
        "    df: pd.DataFrame,\n",
        "    is_testset: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Only for creating dev and test sentences.\"\"\"\n",
        "    claims = []\n",
        "    sentences = []\n",
        "    evidence = []\n",
        "    predicted_evidence = []\n",
        "\n",
        "    # negative\n",
        "    for i in range(len(df)):\n",
        "        # if df[\"label\"].iloc[i] == \"NOT ENOUGH INFO\":\n",
        "        #     continue\n",
        "        claim = df[\"claim\"].iloc[i]\n",
        "\n",
        "        predicted_pages = df[\"predicted_pages\"][i]\n",
        "        for page in predicted_pages:\n",
        "            page = page.replace(\" \", \"_\")\n",
        "            try:\n",
        "                page_sent_id_pairs = [(page, k) for k in mapping[page]]\n",
        "            except KeyError:\n",
        "                # print(f\"{page} is not in our Wiki db.\")\n",
        "                continue\n",
        "\n",
        "            for page_name, sentence_id in page_sent_id_pairs:\n",
        "                text = mapping[page][sentence_id]\n",
        "                if text != \"\":\n",
        "                    claims.append(claim)\n",
        "                    sentences.append(text)\n",
        "                    if not is_testset:\n",
        "                        evidence.append(df[\"evidence\"].iloc[i])\n",
        "                    predicted_evidence.append([page_name, int(sentence_id)])\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"claim\": claims,\n",
        "        \"text\": sentences,\n",
        "        \"evidence\": evidence if not is_testset else None,\n",
        "        \"predicted_evidence\": predicted_evidence,\n",
        "    })"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "MODEL_NAME = \"hfl/chinese-bert-wwm\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm-ext\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-macbert-base\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"sentence-transformers/distiluse-base-multilingual-cased-v2\" #@param {type:\"string\"} 太不穩定\n",
        "#MODEL_NAME = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\" #@param {type:\"string\"} CUDA OUT OF MEM\n",
        "MODEL_SHORT = \"hfl_bert\"\n",
        "NUM_EPOCHS = 1  #@param {type:\"integer\"}\n",
        "LR = 3.5e-5  #@param {type:\"number\"}\n",
        "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "TEST_BATCH_SIZE = 256  #@param {type:\"integer\"}\n",
        "NEGATIVE_RATIO = 0.075  #@param {type:\"number\"}\n",
        "VALIDATION_STEP = 50  #@param {type:\"integer\"}\n",
        "TOP_N = 5  #@param {type:\"integer\"}\n",
        "#@title  { display-mode: \"form\" }"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ToLvE9oxIXQo"
      },
      "outputs": [],
      "source": [
        "EXP_DIR = f\"sent_retrieval/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_neg{NEGATIVE_RATIO}_top{TOP_N}_{MODEL_SHORT}\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Combine claims and evidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "4A5vWEzPiXGF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using the following train data with 0 (Negative) and 1 (Positive)\n",
            "0    5498\n",
            "1    2774\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "train_df = pair_with_wiki_sentences(\n",
        "    mapping,\n",
        "    pd.DataFrame(TRAIN_GT),\n",
        "    NEGATIVE_RATIO,\n",
        ")\n",
        "counts = train_df[\"label\"].value_counts()\n",
        "print(\"Now using the following train data with 0 (Negative) and 1 (Positive)\")\n",
        "print(counts)\n",
        "\n",
        "dev_evidences = pair_with_wiki_sentences_eval(mapping, pd.DataFrame(DEV_GT))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Start training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataloader things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "l48WifjeIGui"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = SentRetrievalBERTDataset(train_df, tokenizer=tokenizer)\n",
        "val_dataset = SentRetrievalBERTDataset(dev_evidences, tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save your memory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "4rl_u0YbeQtY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at hfl/chinese-bert-wwm were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    # import os\n",
        "    # os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    # os.environ['MASTER_PORT'] = '5678'\n",
        "    # torch.distributed.init_process_group(backend=\"nccl\")\n",
        "    model = nn.DataParallel(model)\n",
        "    # model = model.cuda()\n",
        "    # model = nn.parallel.DistributedDataParallel(model)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please make sure that you are using gpu when training (5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1AHGaKh1eKmg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4009c4c1a854b549b46a5a6e70282dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/256 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c191cef34ccb4c2399835d213840e516",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50 {'F1 score': 0.38050722216510735, 'Precision': 0.2538541666666666, 'Recall': 0.759375}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5224255bab54e5983e4b5ef1a964a56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 {'F1 score': 0.392653274990083, 'Precision': 0.2613541666666664, 'Recall': 0.7890625}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1181158ac074d87b44a76daadd03bfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150 {'F1 score': 0.3951555097710222, 'Precision': 0.2632291666666664, 'Recall': 0.7921875}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8f0319ff42f43f1a38739bdc5ede8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 {'F1 score': 0.39644198664440705, 'Precision': 0.26385416666666645, 'Recall': 0.796875}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc6551fed4344078aa9589928f3a9544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250 {'F1 score': 0.3973408099764795, 'Precision': 0.26447916666666643, 'Recall': 0.7984375}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training!\n"
          ]
        }
      ],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        torch.cuda.empty_cache()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.sum().backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.sum().item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(\"Start validation\")\n",
        "            probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "\n",
        "            val_results = evaluate_retrieval(\n",
        "                probs=probs,\n",
        "                df_evidences=dev_evidences,\n",
        "                ground_truths=DEV_GT,\n",
        "                top_n=TOP_N,\n",
        "            )\n",
        "            print(current_steps, val_results)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                writer.add_scalar(\n",
        "                    f\"dev_{metric_name}\",\n",
        "                    metric_value,\n",
        "                    current_steps,\n",
        "                )\n",
        "\n",
        "            save_checkpoint(model, CKPT_DIR, current_steps)\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1035698), started 2:11:00 ago. (Use '!kill 1035698' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-7905f8eb51aec407\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-7905f8eb51aec407\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation part (15 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QS1Ei5DAIO5p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start final evaluations and write prediction files.\n",
            "Start calculating training scores\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3222913627904664b2a3dbf7c6760b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/349 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training scores => {'F1 score': 0.4208745891351096, 'Precision': 0.28615234375000265, 'Recall': 0.7953125}\n",
            "Start validation\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c9f6d43dbbd4853a651313de4e59c54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/92 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation scores => {'F1 score': 0.4004236670875008, 'Precision': 0.26635416666666634, 'Recall': 0.80625}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "ckpt_name = \"model.250.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "print(\"Start final evaluations and write prediction files.\")\n",
        "\n",
        "train_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping=mapping,\n",
        "    df=pd.DataFrame(TRAIN_GT),\n",
        ")\n",
        "train_set = SentRetrievalBERTDataset(train_evidences, tokenizer)\n",
        "train_dataloader = DataLoader(train_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start calculating training scores\")\n",
        "probs = get_predicted_probs(model, train_dataloader, device)\n",
        "train_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=train_evidences,\n",
        "    ground_truths=TRAIN_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"claim_verification/train_doc5sent{TOP_N}_neg{NEGATIVE_RATIO}_{LR}_e{NUM_EPOCHS}_{MODEL_SHORT}.jsonl\",\n",
        ")\n",
        "print(f\"Training scores => {train_results}\")\n",
        "\n",
        "print(\"Start validation\")\n",
        "probs = get_predicted_probs(model, eval_dataloader, device)\n",
        "val_results = evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=dev_evidences,\n",
        "    ground_truths=DEV_GT,\n",
        "    top_n=TOP_N,\n",
        "    save_name=f\"claim_verification/dev_doc5sent{TOP_N}_neg{NEGATIVE_RATIO}_{LR}_e{NUM_EPOCHS}_{MODEL_SHORT}.jsonl\",\n",
        ")\n",
        "\n",
        "print(f\"Validation scores => {val_results}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the model we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt_name = \"model.250.pt\"\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Check on our test data\n",
        "(5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "lVFusJqjmex-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start predicting the test data\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19cf4253725341659d0d461ccdb804ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/112 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_data = load_json(\"data/test_doc5.jsonl\")\n",
        "\n",
        "test_evidences = pair_with_wiki_sentences_eval(\n",
        "    mapping,\n",
        "    pd.DataFrame(test_data),\n",
        "    is_testset=True,\n",
        ")\n",
        "test_set = SentRetrievalBERTDataset(test_evidences, tokenizer)\n",
        "test_dataloader = DataLoader(test_set, batch_size=TEST_BATCH_SIZE)\n",
        "\n",
        "print(\"Start predicting the test data\")\n",
        "probs = get_predicted_probs(model, test_dataloader, device)\n",
        "evaluate_retrieval(\n",
        "    probs=probs,\n",
        "    df_evidences=test_evidences,\n",
        "    ground_truths=test_data,\n",
        "    top_n=TOP_N,\n",
        "    cal_scores=False,\n",
        "    save_name=f\"test_doc5sent{TOP_N}.jsonl\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGzl8a5JteT7"
      },
      "source": [
        "notebook3\n",
        "## PART 3. Claim verification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tgA1vcUyzjlx"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandarallel import pandarallel\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from dataset import BERTDataset\n",
        "from utils import (\n",
        "    generate_evidence_to_wiki_pages_mapping,\n",
        "    jsonl_dir_to_df,\n",
        "    load_json,\n",
        "    load_model,\n",
        "    save_checkpoint,\n",
        "    set_lr_scheduler,\n",
        ")\n",
        "\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL2ID: Dict[str, int] = {\n",
        "    \"supports\": 0,\n",
        "    \"refutes\": 1,\n",
        "    \"NOT ENOUGH INFO\": 2,\n",
        "}\n",
        "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
        "\n",
        "TRAIN_DATA = load_json(\"data/train_doc5sent5.jsonl\")\n",
        "DEV_DATA = load_json(\"data/dev_doc5sent5.jsonl\")\n",
        "\n",
        "TRAIN_PKL_FILE = Path(\"data/train_doc5sent5.pkl\")\n",
        "DEV_PKL_FILE = Path(\"data/dev_doc5sent5.pkl\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preload wiki database (same as part 2.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading and concatenating jsonl files in data/wiki-pages\n",
            "Generate parse mapping\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ac29477d0a4c19af8ecb7e4c3925d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=296938), Label(value='0 / 296938')…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transform to id to evidence_map mapping\n"
          ]
        }
      ],
      "source": [
        "wiki_pages = jsonl_dir_to_df(\"data/wiki-pages\")\n",
        "mapping = generate_evidence_to_wiki_pages_mapping(wiki_pages,)\n",
        "del wiki_pages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AICUP dataset with top-k evidence sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AicupTopkEvidenceBERTDataset(BERTDataset):\n",
        "    \"\"\"AICUP dataset with top-k evidence sentences.\"\"\"\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[Dict[str, torch.Tensor], int]:\n",
        "        item = self.data.iloc[idx]\n",
        "        claim = item[\"claim\"]\n",
        "        evidence = item[\"evidence_list\"]\n",
        "\n",
        "        # In case there are less than topk evidence sentences\n",
        "        pad = [\"[PAD]\"] * (self.topk - len(evidence))\n",
        "        evidence += pad\n",
        "        concat_claim_evidence = \" [SEP] \".join([*claim, *evidence])\n",
        "\n",
        "        concat = self.tokenizer(\n",
        "            concat_claim_evidence,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        label = LABEL2ID[item[\"label\"]] if \"label\" in item else -1\n",
        "        concat_ten = {k: torch.tensor(v) for k, v in concat.items()}\n",
        "\n",
        "        if \"label\" in item:\n",
        "            concat_ten[\"labels\"] = torch.tensor(label)\n",
        "\n",
        "        return concat_ten"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_evaluation(model: torch.nn.Module, dataloader: DataLoader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            y_true.extend(batch[\"labels\"].tolist())\n",
        "\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss += outputs.loss.sum().item()\n",
        "            logits = outputs.logits\n",
        "            y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {\"val_loss\": loss / len(dataloader), \"val_acc\": acc}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_predict(model: torch.nn.Module, test_dl: DataLoader, device) -> list:\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    for batch in tqdm(test_dl,\n",
        "                      total=len(test_dl),\n",
        "                      leave=False,\n",
        "                      desc=\"Predicting\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        pred = model(**batch).logits\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        preds.extend(pred.tolist())\n",
        "    return preds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_with_topk_evidence(\n",
        "    df: pd.DataFrame,\n",
        "    mapping: dict,\n",
        "    mode: str = \"train\",\n",
        "    topk: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"join_with_topk_evidence join the dataset with topk evidence.\n",
        "\n",
        "    Note:\n",
        "        After extraction, the dataset will be like this:\n",
        "               id     label         claim                           evidence            evidence_list\n",
        "        0    4604  supports       高行健...     [[[3393, 3552, 高行健, 0], [...  [高行健 （ ）江西赣州出...\n",
        "        ..    ...       ...            ...                                ...                     ...\n",
        "        945  2095  supports       美國總...  [[[1879, 2032, 吉米·卡特, 16], [...  [卸任后 ， 卡特積極參與...\n",
        "        停各种战争及人質危機的斡旋工作 ， 反对美国小布什政府攻打伊拉克...\n",
        "\n",
        "        [946 rows x 5 columns]\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The dataset with evidence.\n",
        "        wiki_pages (pd.DataFrame): The wiki pages dataframe\n",
        "        topk (int, optional): The topk evidence. Defaults to 5.\n",
        "        cache(Union[Path, str], optional): The cache file path. Defaults to None.\n",
        "            If cache is None, return the result directly.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The dataset with topk evidence_list.\n",
        "            The `evidence_list` column will be: List[str]\n",
        "    \"\"\"\n",
        "\n",
        "    # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "    if \"evidence\" in df.columns:\n",
        "        df[\"evidence\"] = df[\"evidence\"].parallel_map(\n",
        "            lambda x: [[x]] if not isinstance(x[0], list) else [x]\n",
        "            if not isinstance(x[0][0], list) else x)\n",
        "\n",
        "    print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "    # if mode == \"eval\":\n",
        "        # extract evidence\n",
        "    df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "        mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "        for evi_id, evi_idx in x  # for each evidence list\n",
        "    ][:topk] if isinstance(x, list) else [])\n",
        "    print(df[\"evidence_list\"][:topk])\n",
        "    # else:\n",
        "    #     # extract evidence\n",
        "    #     # if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "    #     #     df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "    #     #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "    #     #         for evi_id, evi_idx in x  # for each evidence list\n",
        "    #     #     ][:topk] if isinstance(x, list) else [])\n",
        "    #     # else:\n",
        "    #     df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
        "    #         \" \".join([  # join evidence\n",
        "    #             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "    #             for _, _, evi_id, evi_idx in evi_list\n",
        "    #         ]) if isinstance(evi_list, list) else \"\"\n",
        "    #         for evi_list in x  # for each evidence list\n",
        "    #     ][:1] if isinstance(x, list) else [])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def join_with_topk_evidence(\n",
        "#     df: pd.Series,\n",
        "#     mapping: dict,\n",
        "#     mode: str = \"train\",\n",
        "#     topk: int = 5,\n",
        "# ) -> pd.Series:\n",
        "#     # format evidence column to List[List[Tuple[str, str, str, str]]]\n",
        "#     if \"evidence\" in df:\n",
        "#         df[\"evidence\"] = [[df[\"evidence\"]]] if not isinstance(df[\"evidence\"][0], list) else [df[\"evidence\"]] if not isinstance(df[\"evidence\"][0][0], list) else df[\"evidence\"]\n",
        "\n",
        "#     print(f\"Extracting evidence_list for the {mode} mode ...\")\n",
        "#     if mode == \"eval\":\n",
        "#         df[\"evidence_list\"] = [\n",
        "#             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#             for evi_id, evi_idx in df[\"predicted_evidence\"]  # for each evidence list\n",
        "#         ][:1] if isinstance(df[\"predicted_evidence\"], list) else []\n",
        "#         print(df[\"evidence_list\"][:1])\n",
        "#     else:\n",
        "#         if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "#             df[\"evidence_list\"] = [\n",
        "#                 mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#                 for evi_id, evi_idx in df[\"predicted_evidence\"]  # for each evidence list\n",
        "#             ][:1] if isinstance(df[\"predicted_evidence\"], list) else []\n",
        "#             print(df[\"evidence_list\"][:1])\n",
        "#         else:\n",
        "#             df[\"evidence_list\"] = [\n",
        "#                 \" \".join([  # join evidence\n",
        "#                     mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#                     for _, _, evi_id, evi_idx in evi_list\n",
        "#                 ]) if isinstance(evi_list, list) else \"\"\n",
        "#                 for evi_list in df[\"evidence\"]  # for each evidence list\n",
        "#             ][:1] if isinstance(df[\"evidence\"], list) else []\n",
        "#     # else:\n",
        "#     #     # extract evidence\n",
        "#     #     # if df[\"label\"] == \"NOT ENOUGH INFO\":\n",
        "#     #     #     df[\"evidence_list\"] = df[\"predicted_evidence\"].parallel_map(lambda x: [\n",
        "#     #     #         mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#     #     #         for evi_id, evi_idx in x  # for each evidence list\n",
        "#     #     #     ][:topk] if isinstance(x, list) else [])\n",
        "#     #     # else:\n",
        "#     #     df[\"evidence_list\"] = df[\"evidence\"].parallel_map(lambda x: [\n",
        "#     #         \" \".join([  # join evidence\n",
        "#     #             mapping.get(evi_id, {}).get(str(evi_idx), \"\")\n",
        "#     #             for _, _, evi_id, evi_idx in evi_list\n",
        "#     #         ]) if isinstance(evi_list, list) else \"\"\n",
        "#     #         for evi_list in x  # for each evidence list\n",
        "#     #     ][:1] if isinstance(x, list) else [])\n",
        "\n",
        "#     return df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1. Setup training environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title  { display-mode: \"form\" }\n",
        "\n",
        "# MODEL_NAME = \"bert-base-chinese\"  #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"ckiplab/bert-base-chinese\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"ckiplab/albert-base-chinese\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-bert-wwm-ext\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-macbert-base\" #@param {type:\"string\"}\n",
        "# MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\" #@param {type:\"string\"}\n",
        "MODEL_NAME = \"hfl/chinese-lert-base\" #@param {type:\"string\"}\n",
        "\n",
        "MODEL_SHORT = \"hfl_lert\"\n",
        "EVAL_VERSION = 2\n",
        "TRAIN_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "TEST_BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
        "SEED = 42  #@param {type:\"integer\"}\n",
        "LR = 6.6e-5  #@param {type:\"number\"}\n",
        "NUM_EPOCHS = 20  #@param {type:\"integer\"}\n",
        "REAL_EPOCHS = 10\n",
        "MAX_SEQ_LEN = 256  #@param {type:\"integer\"}\n",
        "EVIDENCE_TOPK = 5  #@param {type:\"integer\"}\n",
        "VALIDATION_STEP = 25  #@param {type:\"integer\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiment Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_FILENAME = \"submission.jsonl\"\n",
        "\n",
        "EXP_DIR = f\"claim_verification/e{NUM_EPOCHS}_bs{TRAIN_BATCH_SIZE}_\" + f\"{LR}_top{EVIDENCE_TOPK}_{MODEL_SHORT}_{EVAL_VERSION}\"\n",
        "LOG_DIR = \"logs/\" + EXP_DIR\n",
        "CKPT_DIR = \"checkpoints/\" + EXP_DIR\n",
        "\n",
        "if not Path(LOG_DIR).exists():\n",
        "    Path(LOG_DIR).mkdir(parents=True)\n",
        "\n",
        "if not Path(CKPT_DIR).exists():\n",
        "    Path(CKPT_DIR).mkdir(parents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2. Concat claim and evidences\n",
        "join topk evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRAIN_PKL_FILE.exists():\n",
        "    # train_df = pd.DataFrame(TRAIN_DATA)\n",
        "    # train_df = train_df.parallel_apply(partial(\n",
        "    #     join_with_topk_evidence,\n",
        "    #     mapping=mapping,\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    # ), axis=1)\n",
        "    train_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(TRAIN_DATA),\n",
        "        mapping,\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    train_df.to_pickle(TRAIN_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TRAIN_PKL_FILE, \"rb\") as f:\n",
        "        train_df = pickle.load(f)\n",
        "\n",
        "if not DEV_PKL_FILE.exists():\n",
        "    # dev_df = pd.DataFrame(DEV_DATA)\n",
        "    # dev_df = dev_df.parallel_apply(partial(\n",
        "    #     join_with_topk_evidence,\n",
        "    #     mapping=mapping,\n",
        "    #     mode=\"eval\",\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    # ), axis=1)\n",
        "    dev_df = join_with_topk_evidence(\n",
        "        pd.DataFrame(DEV_DATA),\n",
        "        mapping,\n",
        "        mode=\"eval\",\n",
        "        topk=EVIDENCE_TOPK,\n",
        "    )\n",
        "    dev_df.to_pickle(DEV_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(DEV_PKL_FILE, \"rb\") as f:\n",
        "        dev_df = pickle.load(f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3. Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prevent CUDA out of memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O0rVk3990DlD"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    train_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "val_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    dev_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    num_workers=0,\n",
        ")\n",
        "eval_dataloader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, num_workers=0,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CzMgs-Zs3sTN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at hfl/chinese-lert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-lert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(LABEL2ID),\n",
        ")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = set_lr_scheduler(optimizer, num_training_steps)\n",
        "\n",
        "writer = SummaryWriter(LOG_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training (30 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_aqMjEek3wmu"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d187396583c4fdb88ad726ff11836fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=25, epoch=0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b59b601f4594180a6bb08802881edc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.1095000219345095\n",
            "val_acc: 0.42191435768261965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=50, epoch=0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8c927313eeb45d9bcdf84d6542f8d1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.1077928066253664\n",
            "val_acc: 0.4093198992443325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=75, epoch=0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13a9ed39d55a49f080f811e2171885ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0850154685974123\n",
            "val_acc: 0.4357682619647355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=100, epoch=0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43b9b08859c743ae838b613871f378cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0847368001937867\n",
            "val_acc: 0.43828715365239296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=125, epoch=1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f3e03edc8c74cd6a9ffcc4889aec49b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.1196215391159057\n",
            "val_acc: 0.4269521410579345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=150, epoch=1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9121aeae305149e6bdf76168b6966bdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0801671743392944\n",
            "val_acc: 0.40428211586901763\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=175, epoch=1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13db01fa7d534366a83bf9adf2ac97d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.113657536506653\n",
            "val_acc: 0.4282115869017632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=200, epoch=1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3af1a53335746e696bb5cbb4924ddca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.1633358860015868\n",
            "val_acc: 0.40554156171284633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=225, epoch=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84d3b75982994f7da8aac24334bf25cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.046397304534912\n",
            "val_acc: 0.4534005037783375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=250, epoch=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e506ed84ba1847e1a6465598c230bfa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.076839261054993\n",
            "val_acc: 0.4332493702770781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=275, epoch=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ba1a7c4219439983dd70b56ad57453",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0745333433151245\n",
            "val_acc: 0.4433249370277078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=300, epoch=2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e30533a6d5f4425987bee3346abc6021",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.057052035331726\n",
            "val_acc: 0.44584382871536526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=325, epoch=3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75ff715676ae4988b03c925cd51761b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.133377366065979\n",
            "val_acc: 0.4596977329974811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=350, epoch=3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "527ebe576abe4ac68ba40625dd3f6407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.066209135055542\n",
            "val_acc: 0.44962216624685136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=375, epoch=3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b472d71319434cebae42780b690f9e6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0490316009521483\n",
            "val_acc: 0.48488664987405544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=400, epoch=3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f12c242f1d834bcfba0c0d3338bd758c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.0382484102249148\n",
            "val_acc: 0.482367758186398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=425, epoch=4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4af020be02f4875a8cf68caa9c0dc06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.508084440231323\n",
            "val_acc: 0.44962216624685136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=450, epoch=4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4abde8e9ba0b4429ae11a9a17583458a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.345254383087158\n",
            "val_acc: 0.4596977329974811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=475, epoch=4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4488b90f25644e3b92c2edd4550273f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.1852737045288086\n",
            "val_acc: 0.4760705289672544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=500, epoch=4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e409142a6b2b4008a1011f582c94e4f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.533659896850586\n",
            "val_acc: 0.4760705289672544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=525, epoch=5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "182cccfe178d45549918624d72f3fc96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.6867178535461425\n",
            "val_acc: 0.5025188916876574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=550, epoch=5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac484adfb08044a3a07d0e5a3c22aeae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.4360566473007204\n",
            "val_acc: 0.5113350125944585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=575, epoch=5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25a97f6780af457fbe6f329a203804ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.405405583381653\n",
            "val_acc: 0.5037783375314862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=600, epoch=5\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6c7bead9389453f8d1e2adef82a07fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.4246809577941892\n",
            "val_acc: 0.4924433249370277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=625, epoch=6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1186353d8884cf390ac51e1b92bb21f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.2995810604095457\n",
            "val_acc: 0.49370277078085645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=650, epoch=6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5796067d0ba4458b09eec0a0b8fe2ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.7114721870422365\n",
            "val_acc: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=675, epoch=6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "490fb8ccb4e442f983ea47e7ca4bbeac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.819591588973999\n",
            "val_acc: 0.517632241813602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=700, epoch=6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d0714ba55354e1f8bd5bff4853ae965",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.5679688167572023\n",
            "val_acc: 0.5138539042821159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=725, epoch=7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b7b888ca2842aa817d2cb6f4a378a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.1381780529022216\n",
            "val_acc: 0.5239294710327456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=750, epoch=7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80ad528fb484f1593b91b8ef48cb61f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.125720329284668\n",
            "val_acc: 0.491183879093199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=775, epoch=7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c33e58bfbba40eb921d2677c5a92f46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 2.989467434883118\n",
            "val_acc: 0.5201511335012594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=800, epoch=7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13c3d8b44b9e4f6aa65f46fcdf5c24a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.1500867080688475\n",
            "val_acc: 0.517632241813602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=825, epoch=8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c96df9cdb5e4796bfe6b035a5933230",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.6120473098754884\n",
            "val_acc: 0.5340050377833753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=850, epoch=8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c887e38d5c564dfeb24d7b14e80312b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.820312957763672\n",
            "val_acc: 0.5037783375314862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=875, epoch=8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8f9b27523c143e2bd3291e8b80bf016",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.79770806312561\n",
            "val_acc: 0.5012594458438288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=900, epoch=8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c46393bbb5420c82b5ce281c382a54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.530899152755737\n",
            "val_acc: 0.5327455919395466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=925, epoch=9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ed19f8566184f8aa38bab56a3b67fdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 4.147527170181275\n",
            "val_acc: 0.5088161209068011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=950, epoch=9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46407928b7c4a848bb3e4cb57d62f34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 4.333025178909302\n",
            "val_acc: 0.5050377833753149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=975, epoch=9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e051a46ba0d46109d5e6a04d820565f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 4.068388786315918\n",
            "val_acc: 0.5088161209068011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/P78081057/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start validation: current_steps=1000, epoch=9\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "457abdf6f9ab458597a9cdde01002610",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_loss: 3.8231439208984375\n",
            "val_acc: 0.5100755667506297\n",
            "Finished training!\n"
          ]
        }
      ],
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "current_steps = 0\n",
        "\n",
        "for epoch in range(REAL_EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        torch.cuda.empty_cache()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.sum().backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "        writer.add_scalar(\"training_loss\", loss.sum().item(), current_steps)\n",
        "\n",
        "        y_pred = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "        y_true = batch[\"labels\"].tolist()\n",
        "\n",
        "        current_steps += 1\n",
        "\n",
        "        if current_steps % VALIDATION_STEP == 0 and current_steps > 0:\n",
        "            print(f\"Start validation: current_steps={current_steps}, epoch={epoch}\")\n",
        "            val_results = run_evaluation(model, eval_dataloader, device)\n",
        "\n",
        "            # log each metric separately to TensorBoard\n",
        "            for metric_name, metric_value in val_results.items():\n",
        "                print(f\"{metric_name}: {metric_value}\")\n",
        "                writer.add_scalar(f\"{metric_name}\", metric_value, current_steps)\n",
        "\n",
        "            val_acc = val_results['val_acc']\n",
        "            if val_acc > 0.41:\n",
        "                save_checkpoint(\n",
        "                    model,\n",
        "                    CKPT_DIR,\n",
        "                    current_steps,\n",
        "                    mark=f\"val_acc={val_acc:.4f}\",\n",
        "                )\n",
        "\n",
        "print(\"Finished training!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4. Make your submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "zLkfuoAE49mz"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "456ae3d7323348678c198eb540c5303a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=248), Label(value='0 / 248'))), HB…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['許多昆蟲被認爲是對生態有益的捕食者 ， 少數昆蟲提供直接的經濟利益 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['綠山城縣  ， 是波蘭的縣份 ， 位於該國西部 ， 由盧布斯卡省負責管轄 ， 首府設於綠山城 ， 面積 1,571 平方公里 ， 2006年人口 89,543 ， 人口密度每平方公里57人 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影公司於1940年出品 ； 導演是茂文 · 李洛埃 （ Mervyn LeRoy ） ， 女主角是主演 《 亂世佳人 》 的費雯 · 麗 （ Vivien Leigh ） 而男主角是羅伯特 · 泰勒 （ Robert Taylor ） 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2015年以 《 刺客聶隱娘 》 獲得第68屆坎城影展最佳導演獎及第52屆金馬獎最佳導演獎與金馬獎最佳劇情片獎 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['但受王立軍事件影響 ， 2012年3月15日 ， 薄熙來被解除中共重慶市委書記職務 ， 同年4月10日被停止中共中央委員和政治局委員職務 ， 接受中共中央紀委調查 ， 並於同年9月28日被開除黨籍 、 公職並以其涉嫌犯罪問題及犯罪問題線索被移送司法機關處理 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['水星凌日發生在五月初或十一月初 ， 平均每百年出現十三次水星凌日的現象 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Extracting evidence_list for the eval mode ...['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['莎士比亞在埃文河畔斯特拉特福出生長大 ， 18歲時與安妮 · 哈瑟維結婚 ， 兩人共生育了三個孩子 ： 蘇珊娜 、 雙胞胎哈姆內特和朱迪思 。']\n",
            "Extracting evidence_list for the eval mode ...['該教的宇宙觀將人類歷史分成青陽 、 紅陽 、 白陽三期 ， 含有末劫救贖思想 ， 相信無極老母派遣彌勒佛 （ 轉世爲祖師路中一 ） 拯救凡間 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['回族 （ 小兒經 ： ； 漢語拼音 ： ） ， 中國民族之一 ， 族名來自唐朝回鶻 ， 古代又稱回回或畏兀兒 ， 世居於黃河流域西北地帶 ， 由內蒙古至山西 、 陝西 、 甘肅 ， 以至於新疆和中亞一帶 ， 先祖主要爲突厥人 ， 但混有蒙古人 、 波斯人 、 大月氏 、 塞種人 、 粟特人 、 阿拉伯人 ， 以至於羌族與漢族等不同血統 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['碳化鈣加水會形成乙炔和氫氧化鈣 ： CaC2 +2 H2O → C2H2 ↑ + Ca ( OH ) 2']\n",
            "['天衛四的表面呈暗紅色 ， 其主要地形是小行星和彗星撞擊後所形成的 ， 並有許多直徑達到210公里的撞擊坑存在 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['京畿道環繞着韓國首都首爾和仁川廣域市 ， 總面積 10,175 平方公里 ， 人口超過 1,300 萬 ， 是韓國人口最多的地方自治團體 ， 首府是水原市 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1939年任美國陸軍參謀長 ， 在第二次世界大戰中 ， 他幫助富蘭克林 · 德拉諾 · 羅斯福出謀劃策 ， 二戰期間堅持先進攻德國再攻打日本的戰略方針 ， 1945年退役 。']['她身上有著威爾士 、 荷蘭和四分之一的日本血統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['已知人類在賽普勒斯的活動足跡最早可以追溯至西元前 10,000 年 ， 此一時期的遺址有喬伊魯科蒂亞 ， 爲新石器時代保存至今依然完好的建築群 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['法輪功 ， 又名法輪大法 、 法輪佛法 ， 是由李洪志於1992年5月在中華人民共和國吉林省首次公開傳出的氣功修煉法 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['首先是女演員費伊 · 貝恩特因 《 白色橫幅 》 與 《 紅衫淚痕 》 分獲第11屆奧斯卡金像獎最佳女主角和女配角提名 。']['林鈺婷 Rita ， 臺灣新竹竹東客家人 ， 山狗大後生樂團團長及主唱 ， 也是電視廣播主持人及配音員等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['艦名出自18世紀英國皇家海軍上將子爵 ， 乃歷史上第4艘以 “ 胡德 ” （ Hood ） 命名的軍艦 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['乃王母頤養生息之天庭別府 ， 名爲 — — 別有洞天 ， 此亦是瑤池之所在 。']['在地質學方面 ， 研究地震波時 ， 反射是十分重要的部分 ， 通過對次聲波的檢測以研究地震與海嘯 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['占星術可至少上溯至公元前2000年 ， 植根於系統預測季節性變化和將天體週期解釋爲神聖傳意跡象的傳統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['是源自美國的跨國綜合企業 ， 經營產業包括電子工業 、 能源 、 運輸工業 、 航空航天 、 醫療與金融服務 ； 總部位於波士頓 、 公司則在紐約州註冊 ， 業務遍及世界100多個國家 ， 擁有員工約28萬7千人 。']['羅馬學者馬庫斯 · 特倫提烏斯 · 瓦羅認爲彌涅耳瓦是思想與宇宙命運的化身 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['《 隋書 》 記載 ， 九姓的祖先是月氏人 ， 自稱其祖先原居祁連山昭武城 （ 今甘肅張掖市臨澤縣 ） ， 爲匈奴所破 ， 遷居蔥嶺 ， 分爲多個小國 ， 其王均屬於昭武氏族 ， 而又分爲九姓 。']\n",
            "Extracting evidence_list for the eval mode ...['樂山大佛開鑿於唐代開元元年 （ 713年 ） ， 完成於貞元十九年 （ 803年 ） ， 先後歷經3位負責人 ， 歷時約九十年 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['木衛六 （ 英文 ： Himalia ） ， 木星的一顆自然衛星 ， 也是因她命名 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['韓劇體現着韓國東西合璧混合文化的特點 ， 通過挖掘人性之美 ， 褒揚真 、 善 、 美 ， 塑造執著 、 堅忍 、 充滿朝氣的人物形象 ， 倡導夫妻恩愛 、 孝敬父母 、 誠實守信 ， 珍惜親情等傳統儒家思想 。']['一旦彈劾議案通過 ， 總統的職能會被暫時凍結 ， 並由總理代總統職務 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['抄襲 （ Plagiarism ） ， 亦稱作剽竊 ， 根據教育部國語辭典定義 ， 爲抄錄他人作品以爲己作 ， 對於原著未經或基本未經修改 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['德川家康 是日本戰國時代的大名 、 元和年間朝廷的太政大臣 、 江戶幕府第一代徵夷大將軍 ， 爲日本於1598年至1616年的實際政治領袖 ， 與其同時代的織田信長 、 豐臣秀吉並稱 「 戰國三傑 」 。']\n",
            "['可擴展商業報告語言 （ eXtensible Business Reporting Language ， XBRL ） 是一種基於XML的標記語言 ， 用於商業和財務訊息的定義和交換 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['休斯敦 （ Houston ， -LSB- ˈhjuːstən -RSB- ） 是美國德克薩斯州的第一大城 ， 是德克薩斯州人口最多的城市 、 美國人口第四大城市 、 美國南部人口最多的城市 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2005年6月 ， 行政院原住民委員會爲了保護達悟族的飛魚節文化 ， 規定每年3 － 6月蘭嶼外海6海里內的海域禁止10噸以上漁船捕魚 ， 也不準使用流刺網 、 追逐網 、 毒魚 、 炸魚等破壞性捕魚手段 。']['在1980年代初 ， 賈伯斯是最早看到全錄帕洛奧圖中心 （ Xerox PARC ） 的滑鼠驅動圖形用戶介面的商業潛力的人 ， 並將其應用於Apple Lisa及一年後的麥金塔電腦 ， 蘋果引入這項技術後大大加強了電腦的易用性和普及 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['中國政法大學 （ 英語 ： China University of Political Science and Law ， 縮寫 ： CUPL ） ， 官方簡稱爲法大 ， 是位於北京的中華人民共和國教育部直屬 ， 教育部與北京市人民政府共建的政法類高等院校 ， 是 “ 211工程 ” 中唯一的政法類高校 ， “ 985工程優勢學科創新平臺 ” ， “ 2011計劃 ” 和 “ 111計劃 ” （ 高校學科創新引智計劃 ） 重點建設高校 ， 國家 “ 雙一流 ” 建設高校 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['廣義的珠江三角洲範圍則包括珠江干流及其所有支流 ， 西起 （ 西江 ） 肇慶羚羊峽東口及 （ 潭江 ） 開平司前 ， 北起 （ 北江 ） 三水黃塘及寶月 、 （ 流溪河 ） 廣州及東莞石碣 、 （ 綏江 ） 肇慶黃崗 ， 東起 （ 東江 ） 惠州園洲 、 （ 增江 ） 廣州增城沙塘 ， 面積爲 8,601 平方公里 。']\n",
            "\n",
            "['全市總面積 11,287 平方公里 ， 人口 878.23 萬 ， 市人民政府駐豐澤區東海街道 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['他統合了維爾納 · 海森堡的矩陣力學和埃爾溫 · 薛定諤的波動力學 ， 發展出了量子力學的基本數學架構 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['亞伯拉罕 （ אַבְרָהָם ， 意爲 “ 多國之父 ” ； Abraham ； إبراهيم ） ， 原名作亞伯蘭 （ 希伯來語 ： אַבְרָם ） 或亞巴郎 （ Abram ， 意爲 “ 崇高之父 ” ） ， 是亞伯拉罕諸教 （ 猶太教 、 基督教和伊斯蘭教等宗教 ） 的先知 ， 是從地上衆生中所揀選並給予祝福的人 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['身爲一位浸信會牧師 ， 金在他職業生涯早期就已開始投入民權運動 ， 曾領導1955年聯合抵制蒙哥馬利公車運動 ， 並在1957年協助建立 （ SCLC ） 。']\n",
            "\n",
            "['金獅獎  是威尼斯影展的最高榮譽 ， 從1949年開始頒發 ， 被認爲是電影界最高榮譽之一 ， 由該屆評審團自正式競賽單元中選出 。']Extracting evidence_list for the eval mode ...\n",
            "['蘇維埃俄國建國之後 ， 他擔任蘇俄外交人民委員 、 俄國立憲會議議員 ， 後任蘇俄軍事和海軍事務人民委員 、 革命軍事委員會主席 ， 建立了蘇聯紅軍 ， 並出任總司令 ， 被譽爲 “ 紅軍之父 ” 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['非營利組織 （ nonprofit organization ， 簡寫爲NPO ） 是指不以營利爲目的組織或團體 ， 其核心目標通常是支持或處理個人關心或者公衆關注的議題或事件 ， 因此其所涉及的領域非常廣 ， 從藝術 、 慈善 、 教育 、 政治 、 公共政策 、 宗教 、 學術 、 環保等 ， 分別擔任起彌補社會需求與政府供給間的落差 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['這類問題主要集中在慰安婦賠償案 ， 二戰期間毒氣問題的賠償 ， 二戰遺留的毒氣 、 炸彈等各種武器 、 彈藥在戰爭結束以後的幾十年裏引起的各種傷害的賠償等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺北101 （ Taipei 101 ） 是位於臺灣台北市信義區的超高層摩天大樓 ， 樓高 509.2 公尺 ， 地上101層 、 地下5層 ， 自落成以來即成爲臺北重要地標與觀光景點之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['許多有關質數的問題依然未解 ， 如哥德巴赫猜想 （ 每個大於2的偶數可表示成兩個素數之和 ） 及孿生質數猜想 （ 存在無窮多對相差2的質數 ） 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['主要由最高車速 、 加速時間 、 最大坡度三方面指標來評價 。']\n",
            "\n",
            "['由河源至太原市上蘭村爲上游 ， 穿行山地 ， 水土流失嚴重 ， 是洪水 、 泥沙的主要來源 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1999年8月底在澳洲等地區的支持下通過公投決定獨立 ， 2002年5月20日零時獨立 ， 2002年9月27日正式加入聯合國 ， 成爲第191個聯合國會員國 。']['她是藝人何基佑的太太 ， 兩人在2011年結婚 ， 也是基督徒 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李祖德 （ Tsu - Der Lee ) 生於臺北市 ， 畢業於臺北醫學大學牙醫學系 ， 由專業醫師轉爲經營者 ， 後又投身於創投業 、 當選北醫董事長 、 引進瑞士醫材產業等']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['煎餅餜子 ， 常作 “ 煎餅果子 ” ， 又稱天津煎餅 ， 是一種起源於天津的風味小喫 。']['在1999年12月31日即將踏入2000年的數小時前 ， 葉利欽突然宣佈辭去總統職務 ， 時任總理的普京受命成爲代理總統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['LiPACE現時主要提供職業專才教育 ， 包括全日制專上課程 、 兼讀制專業進修課程以及企業培訓課程等 。']['阿曼蘇丹國 （ سلطنة عُمان ） ， 簡稱阿曼  ， 是位於西南亞 ， 阿拉伯半島東南沿海的國家 ， 北部與阿拉伯聯合酋長國接壤 ， 西面毗鄰沙地阿拉伯 ， 西南靠近也門 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾爲 “ 雷神 ” 。']['周星馳 （ Stephen Chow ； ） ， 暱稱星爺 ， 是出身香港九龍的喜劇演員 、 導演 、 監製 、 編劇及廣東省政協委員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['核心家庭 ， 也稱爲核家庭 （ Nuclear family ） ， 是地球上人類最廣泛的家庭模式 ， 指的是以異性婚姻或同性婚姻爲基礎 ， 其父母與未婚子女共同生活的家庭 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['行政法與政府行政部門的決策活動密切相關 。']['曾在 《 紅白勝利 》 中扮演的經典角色 「 董月花 」 曾紅極一時 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在佛教史中 ， 根本分裂是指上座部與大衆部的教派分裂 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['水 （ 化學式 ： H2O ） 是由氫 、 氧兩種元素經過化學反應後組成的無機物 ， 在常溫常壓下爲無色無味的透明液體 。']['英國廣播公司 （ British Broadcasting Corporation ） ， 原文縮寫及通稱爲BBC ， 是英國主要的公共媒體機構 ， 世界第一家由國家成立的廣播機構 ， 亦是全球最大的 （ 按照僱員人數 ） 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['《 戰爭與和平 》 （ 改革前俄語 ： Война́ и миръ ， 改革後 -LSB- Война и мир , translit = Voyná i mir -RSB- ， -LSB- vɐjˈna i ˈmʲir -RSB- ） 是俄國作家列夫 · 托爾斯泰的一部長篇小說 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['燕山大學 ， 簡稱燕大 ， 是一所位於中國河北省秦皇島市的大學 。']['疾病是完整機體的反應 ， 但不同的疾病又在一定部位 （ 器官或系統 ） 有它特殊的變化 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['周星馳 （ Stephen Chow ； ） ， 暱稱星爺 ， 是出身香港九龍的喜劇演員 、 導演 、 監製 、 編劇及廣東省政協委員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['實藤惠秀 ， 是一名日本漢學家和中日關係研究者 ， 早年於早稻田大學以研究中國留學生史獲得文學博士學位 ， 之後多年來一直在其母校任職教授 ， 並且從事中日文化交流的探討 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['奧斯陸 （ Oslo ， -LSB- ˈʊ̂ʂlʊ -RSB- ） ， 1624年 — 1925年間被稱爲克里斯蒂安尼亞 （ Kristiania ） ， 是挪威首都和最大城市 ， 全國政治 、 經濟 、 文化中心 ， 也是挪威的貿易 、 銀行業 、 工業和航運樞紐 ， 位於挪威東南部的奧斯陸峽灣內側 ， 人口666 , 759人 （ 截至2017年1月1日 ） 。']\n",
            "['扣帽子 ， 又名貼標籤 、 咒罵法 、 鬥臭法 、 井裏下毒 （ Poisoning the Well ） ， 是一種非常常見的政治宣傳與修辭手法 ， 也是一種典型的人身攻擊 ， 讓閱聽人在還無法深入瞭解 、 思考之前 ， 就率先被某人身上負面的標籤所影響 ， 使閱聽人對其事有種 「 先入爲主 」 的負面認知與不良的刻板印象 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['懷化市 ， 別稱鶴城 、 五溪 ， 是中華人民共和國湖南省下轄的地級市 ， 位於湖南省西部 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['螺旋動物 （ 學名 ： ） ， 又稱螺旋卵裂動物 ， 是原口動物的一大分支 ， 包括多種型態的動物門類 ， 如軟體動物門 、 環節動物門 、 扁形動物門等 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['他的病情曾令當時醫界大惑不解 ， 現今學者一般相信他是罹患紫質症 （ Porphyria ） ， 這是一種血液病 ， 可被三氧化二砷 （ 砒霜 ） 引發 ， 近世研究的確發現喬治三世留存的頭髮樣本中有高含量砒霜 。']Extracting evidence_list for the eval mode ...\n",
            "['六和敬 ， 又稱六慰勞法 、 六可憘法 、 六和 、 六和精神 ， 佛教術語 ， 爲追求菩薩道的修行者在團體生活中要遵循的六種生活態度 ， 也是佛教僧團共住時需遵循的六種生活方法 ：']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['卡夫卡曾計劃印刷他的短篇故事合集 《 飢餓藝術家 》 （ Hungerkünstler ） ， 但卻在他死後纔出版 。']\n",
            "['天衛三覆蓋著無數直徑達326公里 （ 203 英里 ） 的撞擊坑 ， 但隕石坑的數量並不如天衛四多 ， 並表示天王星的五個衛星它可能經歷了早期的內源性表面重生事件 ， 抹去較舊的 、 嚴重坑坑窪窪的表面 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['於是 ， 以 “ 大金 ” 爲國號 ， 望其永遠不變不壞也 。']Extracting evidence_list for the eval mode ...\n",
            "['銀是柔軟且帶有白色光澤的過渡金屬 ， 在所有金屬中 ， 擁有最高的導電率 、 導熱率和反射率 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2006年 ， 原國防科工委和河北省共建燕山大學 。']['iPod touch可以比喻成iPhone的精簡版 — — 不含電話 、 GPS和touch ID等功能  ， 造型亦較輕薄 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1865年宣教師馬雅各醫師於臺南設立傳教本部並開始推行白話字 ， 用於聖經 、 聖詩 、 報紙 、 雜誌之書寫 ， 巴克禮牧師於1885年更發行 《 臺灣府城教會報 》  作爲教會的機關刊物 ， 白話字至此確實在臺灣落地生根 ， 也陸續產生了不少的白話字文學作品 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['- { zh - hans : 演示文稿 ; zh - hant : 簡報 } - ， 一種向聽衆傳達內容 、 消息等的過程及工具 ， 也是一種文件格式 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在一定程度上 ， 體現了當時人類文明的發展程度和價值取向及設計者個人的審美觀念 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['至新朝末年以來的中原戰亂不斷 ， 漢室後人劉秀通過武力統一天下 ， 光復漢朝 ， 結束了中原至西漢末年以來的政治動盪 ， 並進行多項政治改革讓漢朝再次進入平穩發展的局面史稱 “ 光武中興 ” 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['兒玉利國 ， 日本鹿兒島縣人 ， 明治時代官員 ， 日軍海軍少將 。']\n",
            "['日本神話由於經過天武天皇下令以編纂史書的態度將民間傳說系統化 ， 所以跟片段性的中國神話有所不同 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['功夫 ， 不僅是搏擊術和單純的拳腳運動 ， 現在 ， “ 功夫 ” 有了更深刻的內涵 ： 它講究的不再僅僅是武藝高強 ， 而是剛柔並濟 、 內外兼修 ， 即習武之人被要求擁有高尚的品德或情操 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['在此之前 ， 他曾在斯坦福大學教授物理學 。']\n",
            "\n",
            "['最短路徑問題是圖論研究中的一個經典算法問題 ， 旨在尋找圖 （ 由結點和路徑組成的 ） 中兩結點之間的最短路徑 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['威廉 · 華茲渥斯 （ William Wordsworth ) ， 英國浪漫主義詩人 ， 與雪萊 、 拜倫齊名 ， 也是湖畔詩人的代表 ， 曾當上桂冠詩人 。']\n",
            "['而公共服務獎的每位獲得者則將得到金牌一面 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['在人類的文化中 ， 家牛一般佔有很高的地位 ， 早期臺灣的農家子弟禁喫牛肉 ， 1949年的 《 印度憲法 》 與 《 尼泊爾憲法 》 甚至禁止宰牛 。']\n",
            "['沈鈞儒  ， 字秉甫 ， 號衡山 ， 男 ， 江蘇蘇州人 ， 祖籍嘉興 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['氣象衛星 ， 是人造衛星的一種 ， 其主要作用是觀察和監視地球的氣象和氣候 。']\n",
            "\n",
            "['於1990年被英國女王伊麗莎白二世授予爵士稱號 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['可見光 （ Visible light ） 是人類可看見的電磁波 ， 其波長範圍一般是落在360 - 400 nm ~ 760 - 830nm ， 這個電磁波譜又稱爲可見光譜 （ Visible spectrum ） ，']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['普路託 （ 拉丁語 ： Pluto ） ， 羅馬神話中的冥王 ， 陰間的主宰 。']\n",
            "\n",
            "['在國際組織 （ 特別是聯合國 ） 和印度 、 澳洲等比中東位置更東的國家 ， 較偏好使用 「 西亞 」 這一名詞 ， 因爲 「 中東 」 這個傳統的名詞會讓人有歐洲中心主義的感覺 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['爲了與第一代玻璃纖維增強樹脂複合材料相區別 ， 這種複合材料被稱爲先進複合材料 （ 新材料 ， Advanced Composites Material ， 簡稱ACM ） 。']\n",
            "\n",
            "['瓦西里 · 瓦西裏耶維奇 · 康丁斯基 ，  ， 出生於俄羅斯的畫家和美術理論家 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['D大調第九號交響曲由古斯塔夫 · 馬勒於1909年至1910年間所作 ， 此曲連同 《 大地之歌 》 ， 成爲第十套由馬勒創作之交響曲 。']\n",
            "['維克多 · 馬裏 · 雨果 （ Victor Marie Hugo ， -LSB- viktɔʁ maʁi yɡo -RSB- ) ， 法國浪漫主義文學的代表人物和19世紀前期積極浪漫主義文學運動的領袖 ， 法國文學史上卓越的作家 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['竹內瑪莉亞  ， 島根縣簸川郡大社町 （ 即今日的出雲市 ） 出生 ， 是日本的著名流行音樂創作歌手 ， 出道當時則是以偶像歌手身份定位 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['他最突出的貢獻是提出新興古典經濟學與超邊際分析方法和理論 。']\n",
            "['1968年4月4日 ， 馬丁 · 路德 · 金遭人暗殺 ， 地點是田納西州孟斐斯一家汽車旅館 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['已知人類在賽普勒斯的活動足跡最早可以追溯至西元前 10,000 年 ， 此一時期的遺址有喬伊魯科蒂亞 ， 爲新石器時代保存至今依然完好的建築群 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['回族 （ 小兒經 ： ； 漢語拼音 ： ） ， 中國民族之一 ， 族名來自唐朝回鶻 ， 古代又稱回回或畏兀兒 ， 世居於黃河流域西北地帶 ， 由內蒙古至山西 、 陝西 、 甘肅 ， 以至於新疆和中亞一帶 ， 先祖主要爲突厥人 ， 但混有蒙古人 、 波斯人 、 大月氏 、 塞種人 、 粟特人 、 阿拉伯人 ， 以至於羌族與漢族等不同血統 。']\n",
            "['他領導的二月革命推翻了沙皇的統治 ， 在新政府中他任政府總理 ， 1917年11月 ， 他的政府被布爾什維克發起的十月革命推翻 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['全羅北道  是位於朝鮮半島西南部的一個韓國行政道 ， 南與全羅南道接壤 ， 北以錦江爲界與忠清南道相鄰 ， 東以小白山脈爲界與慶尚南道相連 ， 東北一小部分與慶尚北道鄰接 ， 西與中華人民共和國隔黃海相望 ， 面積 8,066 平方公里 ， 人口約180萬 ， 有6市 、 8郡 ， 首府位於全州市 。']['後者專指十九世紀的舞劇 ， 如 《 天鵝湖 》 。']\n",
            "['新華網  ， 由新華社主辦 ， 是新華社的官方網站 ， 由北京總網和分佈於中國各地的30多個地方頻道及新華社的十多家子網站聯合組成 ， 屬世界範圍內最重要的中文新聞網站之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['阿馬蒂亞 · 庫馬爾 · 森 ， CH （ अमर्त्य कुमार सेन ， Amartya Sen ， 又譯爲沈恩 ) 以對福利經濟學的貢獻 ， 獲得諾貝爾經濟學獎 （ 1998年 ） ， 後獲得印度政府頒發 （ 1999年 ） 。']['在一定程度上 ， 體現了當時人類文明的發展程度和價值取向及設計者個人的審美觀念 。']\n",
            "\n",
            "['他經常演一些不安的 、 煩躁的以及帶有神經質的角色 ， 並且堅持將這些角色與現實中的自己完全區分開 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['煉武臺站  是位於韓國忠清南道論山市煉武邑的一個車站 ， 啓用日期爲1958年5月15日 ， 屬於江景線 。']['2004年 ， 喜劇中心將艾倫評爲百大滑稽演員 （ stand - up comics ） 第四位 ， 一項英國的票選則將艾倫位列最偉大喜劇演員第三位 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['通過即時聚合酶鏈式反應或染色質免疫沉澱 - 測序可得到轉錄組及蛋白質組的信息 。']\n",
            "\n",
            "['1940年考入國立中央大學工學院電機工程系 ， 1944年畢業 ， 1945年任電機系助教 ， 1949年任國立南京大學 （ 該年國立中央大學改名國立南京大學 ） 電機系講師 、 南京大學校務委員會常委 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['計算機科學 （ Computer science ， 有時縮寫爲CS ） 是系統性研究信息與計算的理論基礎以及它們在計算機系統中如何實現與應用的實用技術的學科 。']\n",
            "\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']['釀造醋是以穀類等天然原料爲主 ， 再加上食鹽 、 穀皮等發酵而成的食醋 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "\n",
            "['這裏主要居民是卡拉卡爾帕克人 ， 屬突厥語族欽察語支 ， 曾屬於阿拉什自治國 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雄性雉雞的羽毛非常多樣 ， 從幾乎白色到幾乎黑色都有 ， 這是由於馴化及與綠雉雜交 ， 加上放生不同的亞種雜交的結果 ； 雌鳥則維持棕色 、 灰色的樣子 ， 在各個大陸的外貌差別不大 。']\n",
            "['在日本 ， 這種身份的奴隸稱爲譜代下人或譜代奉公人 ， 各地區又有家抱 （ けほう ） 、 門屋 （ もんや ） 、 庭子 （ にわこ ） 、 內百姓等稱呼 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['大司馬是中國歷史上的一個官職名 。']['該劇1940年5月首演於上海黃金戲院 ， 當年的演員陣容甚爲硬整 ： 由四大名旦之一的程硯秋領銜主演 ， 另有吳富琴 、 芙蓉草 （ 飾胡婆 ） 、 孫甫亭 、 劉斌昆 、 李四廣 、 慈少泉等配演 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['由河源至太原市上蘭村爲上游 ， 穿行山地 ， 水土流失嚴重 ， 是洪水 、 泥沙的主要來源 。']['曾任中華民國教育部部長 、 中國國民黨祕書長 、 中華民國總統府資政等要職 ， 亦爲中國文化大學 、 臺北市私立華岡藝術學校之創辦人 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['自原子序數爲1的元素 （ 氫 ） 至原子序數爲118的元素 （ 鿫 ， Oganesson ） 均已被發現或成功合成 ， 並填滿週期表的前七個週期 。']['在攝氏400度以上 ， 乙炔會聚合生成乙烯基乙炔  和苯  。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['在許多數學家努力下 ， 希爾伯特問題中的大多數在20世紀中得到了解決 。']\n",
            "['2006年國際足協世界盃 （ 2006 FIFA World Cup ） 爲國際足協第十八屆舉行的世界盃足球賽 ， 於2006年6月9日至7月9日於德國十二個城市舉行 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['加拉哈德  是亞瑟王傳說中的一名騎士 ， 他在亞瑟王朝中的地位是獨一無二的 ， 因爲只有他才能最終尋得聖盃的下落 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['前451年 ， 頒佈了十二銅表法 ， 解除了平民不受法律保護的局面 ， 在各方面限制騎士階級和元老院的司法專橫 ， 保障平民的生命財產 ， 這也標誌著羅馬法的誕生 。']['氫氣是氫元素標準狀況下以氣態形式存在的物質 ， 化學式爲 ， 由兩個氫原子構成 ， 又稱分子氫 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2003年起因商業糾紛被美國通緝而潛逃中國大陸 （ 因香港和美國有引渡條例 ） ， 之後長居住在深圳和廣州 。']['在佛教史中 ， 根本分裂是指上座部與大衆部的教派分裂 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2015年11月1日 ， 徐翔因涉嫌非法手段獲取股市內幕信息 ， 從事內幕交易 、 操縱股票交易價格等違法犯罪 ， 被公安機關依法採取刑事強制措施 。']\n",
            "['在人類演化的脈絡下 ， 「 人類 」 這個專有名詞指的是 「 人屬 」 ， 但人類演化的研究往往包括其他人科動物 ， 如南方古猿 ， 人屬是在大約230萬至240萬年前的非洲 ， 從南方猿人屬分支出來 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['因此 ， 研究宗教哲學的人並不需要有任何宗教信仰 。']['民間受 《 三國演義 》 等傳統作品影響 ， 普遍認爲關羽與劉備 、 張飛義結金蘭 ， 關羽排行第二 ， 故又俗稱其爲關二爺 、 關二哥 。']\n",
            "['許多有關質數的問題依然未解 ， 如哥德巴赫猜想 （ 每個大於2的偶數可表示成兩個素數之和 ） 及孿生質數猜想 （ 存在無窮多對相差2的質數 ） 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['產於歐洲到亞洲以及非洲的沼澤地和淺湖區 。']['樂山大佛開鑿於唐代開元元年 （ 713年 ） ， 完成於貞元十九年 （ 803年 ） ， 先後歷經3位負責人 ， 歷時約九十年 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['向量空間是現代數學中的一個基本概念 ， 是線性代數研究的基本對象 ， 是指一組向量及相關的運算即向量加法 ， 純量乘法 ， 以及對運算的一些限制如封閉性 ， 結合律 。']['雖然半古典理論對於量子力學的初始發展做出重大貢獻 ， 從於1923年觀測到的電子對於單獨光子的康普頓散射開始 ， 更多的實驗證據使愛因斯坦光量子假說得到充分證實 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['梁朝偉亦曾是一名歌星 ， 於1990年代曾錄製多張唱片 ， 其中作品 《 一天一點愛戀 》 、 《 你是如此的難以忘記 》 成爲華語流行音樂名曲 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['舊桑比爾區和圖爾卡區以及原不屬於本區的桑比爾市併入桑比爾區 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['忒堤斯 （ 古希臘語 ： Τηθύς ） 希臘神話中的提坦神之一 ， 海神俄刻阿諾斯的姐姐與妻子 。']\n",
            "\n",
            "['這意味著鐒在元素週期表中的位置可能比預期的更具波動性 。']['土衛十一又稱爲 「 艾比米修斯 」 （ Epimetheus ） ， 是土星的一顆內側衛星 ， 它的專屬名稱艾比米修斯源自神話 ， 是普羅米修斯的兄弟 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['最初爲臺灣日治時期的總督官邸 ， 現爲中華民國的國家迎賓館 ， 由中華民國外交部管理使用 ， 專門接待國賓或舉辦慶祝活動 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['普路託 （ 拉丁語 ： Pluto ） ， 羅馬神話中的冥王 ， 陰間的主宰 。']\n",
            "['墨家邏輯是中國古代第一個邏輯學體系 ， 全球三大古典邏輯體系之一 ， 主要以三物論爲代表 ， 三物分別爲故 、 理 、 類 。']['大約80 ％ 的流產發生在懷孕的前12周 （ 前三個月 ， 又稱第一期 ） 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['高安公寓 ， 原名阿麥侖公寓  ， 位於上海市徐彙區高安路14號 ， 高安路 、 康平路路口的西北側 ， 與康平路1號住宅隔路相望 。']\n",
            "['棘皮動物  是一類海洋無脊椎動物 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['是指胚胎或胎兒發育到之前的自然死亡 。']['植物藉由果實來傳播種子 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['曲樞 ， 又作曲出 ， 哈剌魯氏 ， 元朝大臣 。']\n",
            "['國民政府軍事委員會爲中國國民黨主導之中華民國國民政府最高軍事機關 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['白鵜鶘 （ 學名 ： Pelecanus onocrotalus ） 也叫東方白鵜鶘或大白鵜鶘 ， 是一種大型鵜鶘 。']['國民政府軍事委員會爲中國國民黨主導之中華民國國民政府最高軍事機關 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['鍵長是兩個成鍵原子A和B的平衡核間距離 。']['藤島康介 （ 藤島 康介 ) ， 日本男性漫畫家 、 插畫家 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['2008年5月7日普京卸任總統後 ， 總統梅德維傑夫提名普京第二度出任總理 ， 繼續掌握國家的實權 。']['國立臺灣民主紀念館 （ 簡稱民主紀念館 ） ， 是中華民國行政院在陳水扁政府執政期間 ， 根據2007年4月13日覈定之 《 國立臺灣民主紀念館組織規程 》 所成立的文教設施機構 ， 正式掛牌成立於2007年5月19日 ， 改制於原本設置同址的中正紀念堂 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['莎士比亞在埃文河畔斯特拉特福出生長大 ， 18歲時與安妮 · 哈瑟維結婚 ， 兩人共生育了三個孩子 ： 蘇珊娜 、 雙胞胎哈姆內特和朱迪思 。']['內畫壺是清朝末年發展起來的一種中國工藝品 ， 最開始只是爲了裝飾鼻菸壺 ， 後來逐漸發展成爲一種獨特的工藝 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['崇義郡 ， 中國南北朝時設置的郡 。']\n",
            "['阿曼蘇丹國 （ سلطنة عُمان ） ， 簡稱阿曼  ， 是位於西南亞 ， 阿拉伯半島東南沿海的國家 ， 北部與阿拉伯聯合酋長國接壤 ， 西面毗鄰沙地阿拉伯 ， 西南靠近也門 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['愛爾蘭 （ Ireland ； Éire ） ， 常稱呼爲愛爾蘭共和國 （ Republic of Ireland ； Poblacht na hÉireann ） ， 是位於西歐的島嶼國家 ， 由愛爾蘭島32個縣中的26個縣組成 ， 面積約7萬平方公里 ， 也是唯一與英國的北愛爾蘭共享陸地邊界的主權國家 。']Extracting evidence_list for the eval mode ...\n",
            "['不同文化 、 不同宗教 、 不同民族 、 社會不同階層 ， 同處一城 ； 城市的東西兩部分截然不同 ， 發展水平懸殊 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['可擴展商業報告語言 （ eXtensible Business Reporting Language ， XBRL ） 是一種基於XML的標記語言 ， 用於商業和財務訊息的定義和交換 。']['臺北市政府舊廈 ， 是位於臺灣台北市大同區的市定古蹟 ， 爲近藤十郎設計 ， 日治時期原爲建成尋常小學校校舍 ， 1945年到1994年作爲臺北市政府辦公廳 ， 今正廳作爲臺北當代藝術館 、 兩翼部分屬建成國中教室 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "\n",
            "['舉例而言 ， 一件質量爲一公斤的物體在地球表面重 9.8 牛頓 ， 而在月球上則重 9.8 牛頓的六分之一 。']['大麥 （ 學名 ： Hordeum vulgare ） ， 是一種禾本科植物 ， 主要的糧食和飼料作物 ， 也可以作爲啤酒或某些蒸餾酒的發酵原料 。']Extracting evidence_list for the eval mode ...['它是傑拉德 · 古柏在1948年2月16日在美國德州的麥克唐納天文臺發現的 ， 並以莎士比亞的歌劇 《 暴風雪 》 中主角的女兒命名 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['跨越北京市 、 河北省 、 山西省 、 河南省四省市 。']['屬 （ genus ， 複數genera ） 是生物分類法中的一級 ， 用於生物學中的生物和化石生物以及病毒的生物分類 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['例如 ， 女性在半透明織物 （ sheer ） 做的裙裝時裏面穿襯裙 ， 在領口較大或者透明的外衣內穿吊帶背心 ， 而在容易被吹起的裙裝或者較短的迷你裙裏面穿安全褲 。']['相傳由古希臘盲詩人荷馬創作的兩部長篇史詩 《 伊利亞特 》 和 《 奧德賽 》 的統稱 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['町名來自兒玉利國 ， 其於1895年擔任首位臺灣縣知事 。']['南京大學 ， 簡稱南大 ， 位於中國南京市 ， 該校歷史或可追溯至三國吳永安元年 （ 258年 ） ， 歷史上曾歷經多次變遷 ， 亦是中國第一所集教學和研究於一體的現代大學 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2016年的人口普查結果顯示 ， 以波士頓領銜的大波士頓擁有480萬人口 ， 乃全美第十大的大都會區 ； 以波士頓爲中心的聯合統計區則擁有820萬人口 ， 乃全美第六大 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2021年1月6日因早前參與立會初選涉嫌違犯中華人民共和國香港特別行政區維護國家安全法而被捕 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['女性這個名詞可以用來表示生物學上的性別劃分 ， 同時亦可指社會認定或自我認同的性別角色 ， 一般只適用於稱呼人類 ， 其他生物通常說是 「 雌性 」 或 「 母的 」 。']['是指胚胎或胎兒發育到之前的自然死亡 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2000年簽約英皇娛樂 ； 2002年與同公司的劉思惠及蔣雅文組成3T （ 英皇第二代三小花 ） 推出 《 少女蝶 》 合輯EP入行 ， 其後爲三人中最突出的一位 ， 於同年率先推出首張個人同名EP 《 Yumiko The Debut EP 》 ， 收錄其首本名曲 《 相對溼度 》 ； 翌年推出 《 舞吧!舞吧 ! 》']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['貨幣代碼採行ISO 4217標準編爲TWD ， 符號爲NT $ 或NTD ， 並使用NT $ 100 、 NTD100之類方法表示 （ 中間無空格 ） 。']\n",
            "Extracting evidence_list for the eval mode ...['在20世紀的後半葉科隆從一個以重工業爲主的城市演變爲一個以服務業爲主的工業 。']\n",
            "\n",
            "\n",
            "['洞裏薩湖 （ -LSB- ɓəŋ tɔnlei saːp -RSB- ； ， 漢字 ： 湖海 、 壺海 ） ， 別名金邊湖 ， 又譯洞 - { 裏 } - 湖 ， 中國史籍作淡洋 、 淡水洋 、 淡水湖 ， 華人稱之爲大魚湖 、 太湖 ， 是位於柬埔寨西北部的湖泊 ， 屬湄公河水系 ， 爲東南亞最大的淡水湖 ， 也是世界上最多樣化和最具生產力的生態系統之一 ， 具有極高的生物多樣性 ， 於1997年被聯合國教科文組織指定爲生物圈保護區 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在編纂 《 百科全書 》 之前 ， 狄德羅已經開始與一些志同道合的自由思想作家結成了一個學術圈子 ， 主要人員有孟德斯鳩 、 魁奈 、 杜爾哥 、 伏爾泰 、 盧梭 、 布豐 、 孔狄亞克 、 達朗貝爾 、 霍爾巴赫 、 愛爾維修等不少法國啓蒙運動時期之著名人物 。']Extracting evidence_list for the eval mode ...['勇度 · 烏東塔  ， 或簡稱勇度  是一名出現於漫威漫畫出版的美國漫畫書中的虛構人物 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['《 約瑟的神奇彩衣 》 （ Joseph and the Amazing Technicolor Dreamcoat ） 是安德魯 · 洛伊 · 韋伯和提姆 · 萊斯 （ Tim Rice ） 於1967年推出的第一部音樂劇 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['臺南市美術館 （ 簡稱南美館 ) 位於中華民國臺南市中西區 ， 是臺灣唯一有科學研究 、 修復畫作與行政法人美術館 ， 內部空間有多功能劇場 、 兒童藝術中心 、 藝術家專室和美術科學研究中心 。']['大橋由中鐵十六局三處負責施工 ， 1993年4月開工 ， 1994年6月線下工程竣工 ， 1996年隨京九鐵路全線開通而正式投入使用 。']['他於2020年5月1日升爲主教級樞機 ， 領波多 - 聖魯菲納羅馬城郊教區主教銜 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['現在希臘神話已經從很多藝術品上關於衆神和英雄故事的裝飾得到考古學上證明 。']\n",
            "\n",
            "['在此之前 ， 他曾在斯坦福大學教授物理學 。']['松前城 ， 亦稱爲爲福山城 ， 是位於北海道松前町的一座日式城堡 ， 爲江戶時代松前藩的主城 ， 也是北海道里唯一的日式城堡 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['其展期結束後 ， 部分展區改爲常設繼續開放參觀 ， 並更名爲花博公園 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['因與張學良發動 “ 西安事變 ” 被迫出洋 “ 考察 ” ， 隨後祕密回國 ， 遭到軍統捕囚十二年 。']\n",
            "['任大理寺丞 ， 一年中判決了大量的積壓案件 ， 涉及到一萬七千人 ， 無冤訴者 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['許多昆蟲被認爲是對生態有益的捕食者 ， 少數昆蟲提供直接的經濟利益 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['銀是柔軟且帶有白色光澤的過渡金屬 ， 在所有金屬中 ， 擁有最高的導電率 、 導熱率和反射率 。']['根據傳統的說法 ， 穆罕默德聖人的多位同伴充當書記 ， 負責把真主的啓示筆記下來 。']\n",
            "['啤酒 （ Birra ， Bier ， Beer ， Cerveza ， Bière ） ， 又叫麥酒 ， 雅稱爲液體面包 ， 利用澱粉水解 、 發酵產生糖分後製成的酒精飲料 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['張京澤  ， 男 ， 漢族 ， 山西芮城人 ， 中華人民共和國政治人物 ， 現任國家民族事務委員會專職委員 （ 副部長級 ） ， 中央民族大學黨委書記 ， 第十三屆全國政協委員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['他是大地之母蓋亞與烏拉諾斯的女兒忒彌斯與伊阿珀託斯的兒子 。']Extracting evidence_list for the eval mode ...['其中於1989年 ， 憑藉電影 《 旺角卡門 》 獲得香港電影金像獎的最佳男配角獎 ， 隨後在1990年 ， 憑藉電影 《 笑傲江湖 》 獲得臺灣電影金馬獎的最佳男配角獎 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['該書不僅對研究突厥語言學有重要價值 ， 而且對研究古代中亞地區諸突厥部落的歷史 、 文化 、 地理 、 文學 、 民俗 、 社會情況等 ， 也提供了大量極其珍貴的資料 ， 具有很高的學術價值 。']['歐幾里得 （ Ευκλείδης ， Εὐκλείδης ， 意思是 「 好的名譽 」 ) ， 有時被稱爲亞歷山大里亞的歐幾里得 ， 以便區別於墨伽拉的歐幾里得 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['超人首次登場於 《 動作漫畫 》 # 1 （ 1938年6月 ） ， 並接著發展出各種媒體 ， 例如廣播劇 、 報紙連環漫畫 、 電視劇 、 電影和電子遊戲 。']\n",
            "['面積  是用作表示一個曲面或平面圖形所佔範圍的量 ， 可看成是長度 （ 一維度量 ） 及體積 （ 三維度量 ） 的二維類比 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['木衛四又稱爲 「 卡利斯托 」 （ Callisto 、 -LSB- iconkəˈlɪstoʊ -RSB- 、 希臘文 ： ） ， 是圍繞木星運轉的一顆衛星 ， 由伽利略在1610年首次發現 。']\n",
            "['府城中和境鷲嶺北極殿大上帝廟 ， 又稱臺南北極殿 ， 位於臺灣台南市中西區 ， 昔日府城海拔最高之鷲嶺 ， 主祀北極玄天上帝 ， 是一間明鄭時期就已建立的古廟 。']Extracting evidence_list for the eval mode ...\n",
            "['良好的代碼風格會幫助程序員閱讀和理解符合該風格的源代碼 ， 並且避免錯誤 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2014年 ， 京杭大運河作爲大運河的一部分 ， 被列入世界遺產名錄 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['華南理工大學是中華人民共和國頂尖高校之一 ， 爲 “ 雙一流 ” 建設高校和廣東省高水平大學 。']\n",
            "\n",
            "['一般認爲木衛三是由伽利略 · 伽利萊在1610年首次觀測到的 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['海峽現由新加坡 、 泰國 、 馬來西亞和印度尼西亞4國共管 。']['豫劇以唱腔鏗鏘大氣 、 抑揚有度 、 行腔酣暢 、 吐字清晰 、 韻味醇美 、 生動活潑 、 善於表達人物內心情感著稱 ， 憑藉其高度的藝術性而廣受歡迎 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['休斯敦 （ Houston ， -LSB- ˈhjuːstən -RSB- ） 是美國德克薩斯州的第一大城 ， 是德克薩斯州人口最多的城市 、 美國人口第四大城市 、 美國南部人口最多的城市 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['這種行爲廣泛地用於二戰後期的戰場上 ， 皆因日本的兵力 、 武器裝備 、 補給物資均遜於盟軍 ， 日軍於是利用自殺式襲擊 ， 以最少資源獲取最高破壞力 。']['他也是軍事獨裁時期最大規模罷工的主要組織者之一 ， 這些罷工使政府陷入困境 ， 並加速了軍政府的垮臺 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['他在2004年3月前往伊拉克尋找通訊生意商機 ， 自2004年4月9日失蹤 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['臺灣最高峯玉山 、 最大天然湖泊日月潭 、 最長河流濁水溪的源頭與臺灣地理中心皆位於該縣 。']\n",
            "['中國海洋大學是中華人民共和國頂尖高校之一 ， 是 “ 雙一流A類 ” 和原 “ 985工程 ” 、 原 “ 211工程 ” 重點建設大學 ， 同時入選國家111計劃 ， 爲北極大學聯盟 、 IAMRI聯盟 、 海洋大學聯盟成員校 ， 是國務院學位委員會首批批准的具有博士學位授予權的單位 ， 國家首批 “ 卓越農林人才教育培養計劃 ” 、 “ 卓越工程師教育培養計劃 ” 改革試點高校 ， 以及青島海洋科學與技術國家實驗室牽頭高校 ， 現轄嶗山 、 魚山 、 浮山 、 西海岸4個校區 。']Extracting evidence_list for the eval mode ...['鑽石是碳元素組成的無色晶體 ， 爲目前已知硬度僅次於藍絲黛爾石的天然物質 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1970年 ， 改設國家計劃委員會地質局 。']\n",
            "['鐒是在1961年 ， 由阿伯特 · 吉奧索等人在美國加利福尼亞柏克萊的勞倫斯放射實驗室中 ， 利用硼轟擊鐦合成 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['長征到達陝北後 ， 因中共中央有意鬥爭張國燾的緣故 ， 而與王建安等十餘名高級幹部一度計劃脫離紅軍 。']\n",
            "['這個在1970年代處由美國食品工業部建立的 “ 統一商品碼理事會 ” 爲的是建立一個統一的條形碼機制 。']['文化大革命結束後 ， 在撥亂反正期間獲得平反 ， 1979年被任命爲北京大學名譽校長 ， 1981年2月被推舉爲新成立的中國人口學會名譽會長 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['馬克士威在19世紀60年代構想出這方程組的早期形式 。']Extracting evidence_list for the eval mode ...\n",
            "['他在2004年3月前往伊拉克尋找通訊生意商機 ， 自2004年4月9日失蹤 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['奧克蘭 （ Auckland ； Tāmaki Makaurau 或 Ākarana ） ， 老華僑譯作屋侖 ， 是新西蘭的一個都會區 ， 位於南太平洋南緯37度 ， 爲紐西蘭人囗最多的城市 ， 也是北島最大的城市 。']['在作爲 “ 黑馬 ” 贏得1844年的選舉後 ， 他是第一位沒有尋求連任而直接退休的總統 。']\n",
            "\n",
            "['康乃爾大學 （ Cornell University -LSB- kɔrˈnɛl -RSB- ） 是一所位於美國紐約州伊薩卡的私立研究型大學 ， 另有兩所分校位於紐約市曼哈頓和卡塔爾教育城 ， 是美洲大學協會的十二個創會成員之一 ， 及NCAA體育賽事聯盟常春藤盟校的成員 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['離散政治是離散學 （ diaspora studies ） 的一部份 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['京劇四大名旦 ， 一般而言是指民國十六年 （ 1927年 ） 由北京 《 順天時報 》 評選出的梅蘭芳 、 程硯秋 、 尚小云 、 荀慧生 、 徐碧雲五位著名的京劇旦角中的前四位 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['釀造醋是以穀類等天然原料爲主 ， 再加上食鹽 、 穀皮等發酵而成的食醋 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['曾榮獲三屆金馬獎最佳導演獎 ， 1989年的 《 悲情城市 》 獲得第46屆威尼斯影展金獅獎 ， 1993年憑藉 《 戲夢人生 》 獲得第46屆坎城影展評審團獎 。']\n",
            "['每升溶液中所含溶質的物質的量稱爲濃度 ； 溶質在穩定態下所能達到的最大濃度稱爲溶解度 ； 濃度低於溶解度的稱爲未飽和溶液 ， 濃度等於溶解度的稱爲飽和溶液 ， 濃度大於溶解度的稱爲過飽和溶液 。']['《 隋書 》 記載 ， 九姓的祖先是月氏人 ， 自稱其祖先原居祁連山昭武城 （ 今甘肅張掖市臨澤縣 ） ， 爲匈奴所破 ， 遷居蔥嶺 ， 分爲多個小國 ， 其王均屬於昭武氏族 ， 而又分爲九姓 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['1966年無產階級文化大革命時 ， 全國的戲曲社廣受批鬥 ， 秦腔大受打擊 ， 如今殘存於陝西 、 甘肅 、 青海 、 寧夏 、 新疆的老年人口 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['排球起源於北美洲 ， 經過多年發展 ， 現已普及至全球各地 。']['清治末期 ， 基隆因航運地理位置優越 、 加上週邊有豐富的煤礦蘊藏 ， 清廷於1875年正式設治 、 並將原名 「 雞籠 」 更改爲 「 基隆 」 ， 開啓了都市發展的歷史 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['變流技術 （ power conversion technique ） 或電能轉換 （ electric power conversion ） ， 是電機工程 、 電力工程以及中的名稱 ， 是一種電能變換的技術 ， 可能是直流電和交流電之間的轉換 ， 電壓及電流的調整 ， 或是兩者都有 。']\n",
            "\n",
            "['麗臺科技是全球知名的電腦及智慧醫療研發製造商 、 NVIDIA長期合作伙伴 ， 以 「 研究創新 、 品質至上 」 爲不變的信念 ， 推出產品涵蓋GeForce顯示卡 、 Quadro專業繪圖卡 、 AI工作站 / 伺服器 、 AI管理軟體 、 桌面虛擬化Zero Client / Thin Client方案 、 智慧醫療 / 健康照護及大數據解決方案等 。']['天文學家曾經利用金星凌日的觀測結果 ， 結合恆星視差原理 ， 獲得了比之前更爲精確的天文單位的數值 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺中市立啓明學校 （ Taichung Minicipal Taichung Special Education School for The Visually impaired ） 簡稱中明 ， 是臺灣台中市后里區一所專收視覺障礙生的公立特殊教育學校 ， 1968年獨立創校 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['2014年 ， 京杭大運河作爲大運河的一部分 ， 被列入世界遺產名錄 。']\n",
            "['何時成爲明憲宗的嬪御 ， 已無法考證 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['因爲與需要昂貴的專用設備和開發工具的競爭對手 （ 如索尼和任天堂 ） 相比 ， Java ME程序可以在PC機上開發和仿真運行 ， 然後很容易地部署到目標機上 ， 從而使其開發 、 測試和發佈的變得容易和廉價 。']\n",
            "['承接自臺北帝國大學時代標本室的收藏 ， 館內收藏爲數衆多的百年以上之老標本與珍貴標本 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['湖廣鄉試第七名 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['它是由原梅克倫堡約三分之二的區域以及西波美拉尼亞 ， 還有普利希尼茨的一小部分地區和北部的烏克馬克組成 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['根據傳統的說法 ， 穆罕默德聖人的多位同伴充當書記 ， 負責把真主的啓示筆記下來 。']\n",
            "['教室又稱課堂 、 課室 ， 香港也稱班房 ， 是學生上課的地點 ， 是個學校重要的組成部分 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['自1968年以來 ， 埃塔已經造成超過800人喪生 、 數千人受傷 。']\n",
            "['敏迷龍曾擁有恐龍中最短的屬名 ， 之後由2004年發現於中國的肉食性恐龍寐龍 （ Mei ） 、 以及在2009年發現於蒙古國的足龍 （ Kol ） 、 2015年發表的奇翼龍 ( Yi ) 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['淋巴結腫大 （ lymphadenectasis ） 是一種淋巴結病 （ lymphadenopathy ） ， 舊名腺病 （ adenopathy ） ， 臨牀表現爲淋巴結的大小或硬度異常 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['蔡卓妍 （ Charlene Choi ) ， 香港女歌手 、 女演員 ， 女子團體Twins成員 ， 暱稱 「 阿Sa 」 ， 出生於加拿大溫哥華 、 在香港長大 ， 中三開始爲雜誌及廣告擔任兼職模特兒 ， 1999年 ， 在電影 《 自從他來了 》 擔任特約演員 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['香港科技大學 （ The Hong Kong University of Science and Technology ， 縮寫 ： HKUST ） ， 簡稱科大 ， 是香港的一所公立研究型大學 ， 位於香港新界西貢區清水灣半島 。']\n",
            "['在將領辦公地點的前方 ， 常用馬車之轅木對立而成進出口 ， 以便於士兵駐守 ， 遂稱 「 轅門 」 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['前身爲臺北市兒童交通博物館 ， 於2008年9月17日閉館 ， 原址移交臺北市政府客家事務委員會 ， 經翻修後2011年10月15日正式開園 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['稱帝之後遣衆將攻伐四方 ， 往往能從前方上報的排兵佈陣形勢中發現問題 ， 有時因前方不能及時得到糾正 ， 便爲敵人所敗 。']\n",
            "['李安 （ Ang Lee ) ， 臺灣屏東縣潮州鎮人 ， 祖籍則是江西省九江市德安縣 ， 外省人第二代 ， 國立臺南第一高級中學校友會 ， 臺灣導演 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['軍事方面任用蒂雷納子爵 、 大孔代親王 、 維拉爾公爵等名將爲他在戰爭期間贏下多次勝利 ， 締造法蘭西輝煌時期的不敗神話 。']Extracting evidence_list for the eval mode ...\n",
            "['根據基督教的傳統 ， 星期五是耶穌受難的日子 （ 受難節 ） ， 耶穌是星期五午後15時被釘上十字 ， 所以部分基督徒認爲星期五是不祥的日子 ， 特別當天又13號 ， 因爲耶穌只有門徒12名 ， 13號是猶大出賣耶穌 ， 故大凶兇中兇 ， 一年約1 - 3天 ， 一般教會不會選擇在星期五進行禮拜 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['GCC也可以編譯Windows 、 Android 、 iOS 、 Solaris 、 HP - UX 、 IBM AIX和DOS系統的代碼 。']\n",
            "\n",
            "['根據這個定義 ， 若要一個物體沒有重量 ， 原則上只有無限遠離所有其他具有質量的物體纔可能發生 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['荷馬如果確有其人 ， 應該是將兩部史詩整理定型的作者 。']['該校現設有高中部18班 、 國中部30班 ， 以其嚴謹校風著稱於中臺灣地區 ， 並與臺中女中爲臺中現存唯二的女子中學 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['洞裏薩湖 （ -LSB- ɓəŋ tɔnlei saːp -RSB- ； ， 漢字 ： 湖海 、 壺海 ） ， 別名金邊湖 ， 又譯洞 - { 裏 } - 湖 ， 中國史籍作淡洋 、 淡水洋 、 淡水湖 ， 華人稱之爲大魚湖 、 太湖 ， 是位於柬埔寨西北部的湖泊 ， 屬湄公河水系 ， 爲東南亞最大的淡水湖 ， 也是世界上最多樣化和最具生產力的生態系統之一 ， 具有極高的生物多樣性 ， 於1997年被聯合國教科文組織指定爲生物圈保護區 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['北理工擁有19個學院 ， 近200個科研機構 ， 兩家校辦產業和一個產業園區北理工科技園 ， 更擁有5個國家級重點實驗室 ， 是歷史上曾爲首批副部級高校 ， 也是中國首批設立研究生院的高校之一 。']['東漢末年 ， 漢廷因黃巾之亂 、 北宮伯玉之亂 、 黑山軍起義 、 王芬謀廢漢靈帝 、 張舉張純叛亂 、 外戚宦官火拼等一系列事件而動盪不安 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['伊萬 · 謝爾蓋耶維奇 · 屠格涅夫 （ -LSB- Ива́н Серге́евич Турге́нев , p = ɪˈvan sʲɪrˈɡʲeɪvʲɪtɕ tʊrˈɡʲenʲɪf -RSB- ， 公曆 ， 合儒略曆1818年10月28日 － 1883年8月22日 ） 俄國現實主義小說家 、 詩人和劇作家 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['蛋白質組 （ 也稱 - { zh - cn : 蛋白質體 ; zh - tw : 蛋白質組 ; zh - hk : 蛋白質體 } - ， proteome ） ， 是在特定時間內 ， 是一個由基因組 、 細胞 、 組織 、 或生物體表達的或可以表達的整套蛋白質 。']Extracting evidence_list for the eval mode ...\n",
            "['它的畫風最爲特別 ， 人物形象打破了秀美 、 苗條的舊框框 ， 使作品別有一番豪放的味道 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李祖德 （ Tsu - Der Lee ) 生於臺北市 ， 畢業於臺北醫學大學牙醫學系 ， 由專業醫師轉爲經營者 ， 後又投身於創投業 、 當選北醫董事長 、 引進瑞士醫材產業等']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['因北方遊牧民族發動戰爭 ， 晉懷帝與晉愍帝先後被俘殺 ， 琅琊王司馬睿在群臣擁戴下在建康 （ 今南京 ） 即位 ， 即晉元帝 ， 史稱東晉 。']['進入微軟公司後 ， 先後主持了Visual J++、.Net ， C# 和 TypeScript 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1969年10月24日開始在 《 明報 》 連載 ， 到1972年9月23日刊完 ， 一共連載了2年11個月 。']['各島泥土不厚 、 風浪較大 ， 僅釣魚臺上有淡水溪流 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['費茲傑羅被認爲是1920年代 「 迷惘的一代 」 的人 ， 他最著名的小說爲 《 大亨小傳 》 ， 此書堪稱美國社會縮影的經典代表 ， 描述1920年代美國人在歌舞昇平中空虛 、 享樂 、 矛盾的精神與思想 。']\n",
            "Extracting evidence_list for the eval mode ...['網景通訊家 （ Netscape Communicator ） 是由網景公司1997年所開發的網路套件 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['但是 ， 島上多樣化的生態系統和獨特的野生動植物種類也日益受到迅速增長人口侵佔的威脅 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['蘋果酒 （ Cider ； Le cidre ） ， 音譯西打 ， 是一種用蘋果汁釀造的酒精飲料 ， 是產量僅次於葡萄酒的世界第二大水果酒 ， 在西歐的英國 、 愛爾蘭 、 法國 、 德國 ， 以及美洲的美國和阿根廷均廣受歡迎 。']\n",
            "\n",
            "['臺灣海峽 （ 俗稱烏水溝 ， 臺語 : Oo - tsúi - kau ； 歐洲早期稱福爾摩沙海峽 ） 指的是介於中國大陸與臺灣之間的海域和海峽 ， 北東 — 南西走向 ， 長約370公里 ， 北窄南寬 ， 北口寬約200公里 ， 南口寬約410公里 ， 平均寬度180公里 ， 最窄處在臺灣側新竹市南寮與福建側平潭海壇島之間 ， 約126公里 ， 以大陸架爲主 ， 其水深 （ 岩牀最大深度 ） 爲70米 ， 總面積約8萬平方公里 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['此後定居法國 ， 在1939年加入法國國籍 ， 1940年法國被納粹德國佔領 ， 康定斯基沒有選擇前往美國定居 ， 他在1944年逝於']\n",
            "\n",
            "['認識論 、 博弈論 、 邏輯和思想心理學研究思考必須遵循哪些規則 ， 以便以有意義的方式處理感知 ， 得出真正的信念 ， 或者正確解決問題或得出結論 。']Extracting evidence_list for the eval mode ...['非合併屬地  或未合併屬地 、 非建制屬地 、 未建制屬地 ， 在美國法律裏指一個屬地由美國聯邦政府管轄 ， 但美國國會未對該屬地通過合併法律 ， 還沒成爲正式的國土 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['入聲 （ checked tone 、 entering tone ） 是音韻學之概念 ， 包括入聲韻及入聲調 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2007年5月2日推出線上版 ， 並支援11種語言 ， 可直接透過Flash驅動於網頁上和好友傳訊 ， 雅虎宣佈將於2011年11月1日 ， 終止網上版服務 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['隋朝結束自魏晉南北朝以來的分裂局面 ， 奠定日後大唐盛世的基礎 ， 對中國歷史的意義重大 。']\n",
            "\n",
            "['總人口數約21萬7800餘人 ， 是臺灣原住民族中人數最多的族群 ， 也是臺灣第三大族群 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['物理常數的物理意義有很多表述形式 ， 普朗克長度表徵基本物理長度 ， 真空光速是宇宙中最大的速度 ， 精細結構常數則表徵了電子和光子之間的相互作用 ， 是一個無量綱量 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2017年 ， STARSHIP娛樂宣佈與King Kong娛樂正式合併 ， 演員經紀合約的部分隸屬King Kong by STARSHIP娛樂 ， 同時亦成爲STARSHIP娛樂的100 % 投資持股子公司 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['曾在 《 紅白勝利 》 中扮演的經典角色 「 董月花 」 曾紅極一時 。']['高錳酸鉀 （ Potassium permanganate ； 化學式 ： KMnO4 ） ， 強氧化劑 ， 紫黑色晶體 ， 可溶於水 ， 遇乙醇即被還原 。']\n",
            "['該校現設有高中部18班 、 國中部30班 ， 以其嚴謹校風著稱於中臺灣地區 ， 並與臺中女中爲臺中現存唯二的女子中學 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['荷馬史詩是古希臘文學中最早的一部史詩 ， 也是最受歡迎 、 最具影響力的文學作品 。']\n",
            "['作業系統層虛擬化  ， 亦稱容器化  ， 是一種虛擬化技術 ， 這種技術將作業系統內核虛擬化 ， 可以允許使用者空間軟體實體 （ instances ） 被分割成幾個獨立的單元 ， 在內核中運行 ， 而不是隻有一個單一實體運行 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['例如 ， 糖尿病和愛滋病曾一度屬於絕症 ， 但由於對糖尿病患者施用胰島素 ， 對愛滋病毒感染者提供每日藥物治療 ， 這些人能夠在症狀受到控制的情況下存活 ， 轉變成爲慢性疾病 。']['2009年 ， 工信部 、 國家國防科工局和河北省共建燕山大學 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['已知人類在賽普勒斯的活動足跡最早可以追溯至西元前 10,000 年 ， 此一時期的遺址有喬伊魯科蒂亞 ， 爲新石器時代保存至今依然完好的建築群 。']\n",
            "\n",
            "\n",
            "['它還以國情簡介 、 每月的國別報告 、 5年國家的經濟預測 、 國家風險報告和行業報告服務聞名 ， 並會每年發表世界最佳居住城市和最佳出生地指數報告 。']Extracting evidence_list for the eval mode ...['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "\n",
            "\n",
            "['隋煬帝於604年7月21日由楊素協助登基 ， 在位期間加強了中央集權 ， 擴大了統治的社會基礎 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['因北方遊牧民族發動戰爭 ， 晉懷帝與晉愍帝先後被俘殺 ， 琅琊王司馬睿在群臣擁戴下在建康 （ 今南京 ） 即位 ， 即晉元帝 ， 史稱東晉 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['西亞 （ West Asia或Western Asia ） ， 或稱西南亞 （ Southwest Asia或Southwestern Asia ， غرب آسيا ） ， 指亞洲的西南部 ， 和中東有很大部份的重合 。']['可以以每月 1.99 美元的價格來取得100GB的儲存空間 。']\n",
            "['諾貝爾文學獎 （ Nobelpriset i litteratur ） 是瑞典學院頒發的諾貝爾獎之一 ， 根據諾貝爾的遺囑 ， 每年表彰 「 在文學領域創作出具理想傾向之最佳作品者 」 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['梁武帝蕭衍在位時間近48年 ， 在南北朝皇帝中名列第一 。']['在世界各地的一些國家 ， 在近代及現代在親屬或摯友死後有戴黑色袖章的習慣 ， 表示哀思 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['1950年9月 ， 于化虎作爲民兵代表 ， 出席了在北京召開的全國戰鬥英雄代表會議 ， 被評爲 “ 全國民兵英雄 ” 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['第二十屆世界青年日從8月15日星期一到8月21日星期日在科隆舉辦 。']['公海艦隊自沉事件 （ Selbstversenkung der Kaiserlichen Hochseeflotte in Scapa Flow ） 又稱爲 「 第十一節之辱 」 是指1919年6月21日德國海軍公海艦隊於斯卡帕灣的一次集體自沉行動 。']\n",
            "\n",
            "\n",
            "['承接自臺北帝國大學時代標本室的收藏 ， 館內收藏爲數衆多的百年以上之老標本與珍貴標本 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['拜耳命名法 （ Bayer designation ） 是一種恆星命名法 ， 它以一個希臘字母做前導 ， 後面伴隨著拉丁文所有格的星座名稱 。']['猶太教是一神論的宗教 ， 其主要經典是包括妥拉 （ 摩西五經 ） 在內的塔納赫 （ 即希伯來聖經 ， 基督教稱爲舊約聖經 ） ， 以及包括口傳律法 （ 密西拿 ） 、 口傳律法註釋 （ 革馬拉 ） 以及聖經註釋 （ 米德拉什 ） 在內的塔木德 ， 對信奉猶太教的猶太人而言 ， 猶太教是和以色列人立約的關係 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['高安公寓 ， 原名阿麥侖公寓  ， 位於上海市徐彙區高安路14號 ， 高安路 、 康平路路口的西北側 ， 與康平路1號住宅隔路相望 。']\n",
            "['所謂材料的電性能是由材料中電荷 （ 包括離子 、 電子 、 空穴等 ） 的分佈及移動所決定的 。']\n",
            "['因其陡峭的外觀被選爲日本三大奇形之一 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['食物通常不適用於這個定義 ， 儘管它們也可以對生物物種產生生理效應 。']['人氏是舊石器時期河套附近一個父系氏族 ， 他們以打獵爲生 ， 喫捕獲的獵物 ， 過着茹毛飲血的生活 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['海王星是太陽系八大行星中距離太陽最遠的 ， 體積第四大的 ， 但質量是第三大的行星 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['法輪功 ， 又名法輪大法 、 法輪佛法 ， 是由李洪志於1992年5月在中華人民共和國吉林省首次公開傳出的氣功修煉法 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['“ Aion ” ， 第五張專輯 ， 於1990年出版 ， 這張專輯顯示出樂隊對中近東音樂元素的注意與興趣 。']\n",
            "\n",
            "['徐太志  ， 韓國男歌手 、 作詞家 、 作曲家 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1952年 ， 有人在壽命超過十億年的紅巨星中發現了鍀 - 98 ， 讓人們認識到恆星可以製造重元素 。']['之後彈劾議案會交由憲法法院審理 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['另一種觀點是 ， “ 新 ” 應該意味著宗教的形成較新 。']['1948年 ， 60歲的艾略特被授予他一生中最大的榮譽 — — 諾貝爾文學獎 。']['哈比人  ， 是托爾金的奇幻小說中出現的一種虛構民族 ， 體型嬌小爲其特色 ， 但並非現實世界的矮人或侏儒 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['作爲世界上收入最多的演員之一 ， 他獲得了多項榮譽 ， 包含三次金球獎和榮譽金棕櫚獎 ， 以及三次奧斯卡金像獎和英國電影學院獎提名 。']\n",
            "\n",
            "\n",
            "['罩杯的別稱 ， 指鋼圈內衣包覆乳房而呈碗形的部分 。']Extracting evidence_list for the eval mode ...['車站名稱的由來是鄰近的白山神社內存有國家指定的天然紀念物 「 蟲川大杉 」 （ ) 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['《 - { zh : 午夜巴黎 ; zh - hans : 午夜巴黎 ; zh - hant : 午夜巴黎 ; zh - hk : 情迷午夜巴黎 ; zh - mo : 情迷午夜巴黎 ; zh - tw : 午夜 · 巴黎 ; } - 》  是2011年由伍迪 · 艾倫編劇並執導的一部以法國巴黎爲背景的浪漫喜劇和奇幻電影 。']['現在一般認爲起源於酒神祭祀 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['臺灣織布株式會社 ， 於日治臺灣設立的紡織公司 ， 創辦人爲小元富太郎 。']['全州也是聯合國教科文組織指定的美食城市之一 ， 每年舉行 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['軟件開發是一項包括需求獲取 、 開發規劃 、 需求分析和設計 、 編程實現 、 軟件測試 、 版本控制的系統工程 。']\n",
            "['李祖德 （ Tsu - Der Lee ) 生於臺北市 ， 畢業於臺北醫學大學牙醫學系 ， 由專業醫師轉爲經營者 ， 後又投身於創投業 、 當選北醫董事長 、 引進瑞士醫材產業等']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['著名前練馬師簡炳墀爲松柏塱原居民 ， 並獲選爲松柏塱村村長 。']['共和黨 （ Republican Party ） 是美國的一個政黨 ， 又被稱作大老黨 （ Grand Old Party ， 縮寫爲GOP ） ， 與民主黨並列爲美國兩大主要政黨 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2016年4月29日 ， 因涉嫌操縱證券市場及內幕交易犯罪 ， 被公安機關依法批准逮捕 。']['朱比特 （ Iuppiter ） ， 又譯 - { zh - cn : 朱比特 ; zh - hk : 朱庇特 ; zh - tw : 朱比特 ; zh - hant : 朱庇特 } - 、 朱皮特 ， 是古羅馬神話中的衆神之王 ， 相對應於古希臘神話的宙斯 （ 希臘語 ： Ζεύς ） ， 西方天文學對木星的稱呼以其命名 。']\n",
            "['已知人類在賽普勒斯的活動足跡最早可以追溯至西元前 10,000 年 ， 此一時期的遺址有喬伊魯科蒂亞 ， 爲新石器時代保存至今依然完好的建築群 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['1995年 ， 以劇情片 《 我的美麗與哀愁 》 出道 ； 同年以愛情片 《 少女小漁 》 獲得第40屆亞太影展最佳女主角 ； 主打歌 《 爲愛癡狂 》 獲得第32屆金馬獎最佳電影歌曲獎 。']['此外菲律賓 、 越南及中華人民共和國皆主張擁有太平島主權 ， 但均從未進行過實際控制 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['護照或護證  是一個國家或地區的政府發放給本國公民或國民的一種旅行證件 ， 用於證明持有人的身分與國籍 ， 以便其出入本國及在外國旅行 ， 同時亦用於請求有關外國當局給予持照人通行便利及保護 。']['熱帶海洋性氣候 ， 終年高溫多雨 ， 是一種分佈在赤道周圍的熱帶雨林氣候 。']\n",
            "\n",
            "['茲德內克 · 莫拉維克  ， 捷克天文學家 ， 以發現大量小行星聞名 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['TI於1950年代初開始研究晶體管 ， 同時也製造了世界上第一個商用硅晶體管 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['由於他被指與基地組織有聯繫 ， 被中國及美國官方列爲恐怖分子 。']\n",
            "['北理工擁有19個學院 ， 近200個科研機構 ， 兩家校辦產業和一個產業園區北理工科技園 ， 更擁有5個國家級重點實驗室 ， 是歷史上曾爲首批副部級高校 ， 也是中國首批設立研究生院的高校之一 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['按含三鍵的多少分別稱單炔烴 、 二炔烴等 :) 。']['這層殼會分節以利於運動 ， 猶如騎士的甲冑 。']['大衆汽車 （ Volkswagen ， 縮寫VW ） ， 是一家總部位於德國沃爾夫斯堡的汽車製造公司 ， 爲大衆集團的核心企業及原始品牌 ， 也是該集團最暢銷品牌及全球第一大汽車製造商 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['倫勃朗 、 彼得 · 保羅 · 魯本斯以及伊利亞 · 列賓等也是素描大師 。']['然而由於日本戰敗 ， 此一行政區劃始終未正式實施 ， 僅作爲一般稱呼 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在羅馬 ， 維納斯的紀念日定在每年的四月 ， 帝國時期的羅馬對維納斯的崇拜尤爲盛行 。']\n",
            "\n",
            "['SETI@home 官方2005年3月中旬發佈消息 ， 逐漸停止SETI Classic （ 即舊平臺 ） 的計算 ， 全面轉入BOINC計算平臺 ， 數據轉換預計在2個月之內完成 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']['1999年8月底在澳洲等地區的支持下通過公投決定獨立 ， 2002年5月20日零時獨立 ， 2002年9月27日正式加入聯合國 ， 成爲第191個聯合國會員國 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['納尼氏囊鰓鯰 ， 爲輻鰭魚綱鯰形目囊鰓鯰科的其中一種 ， 爲熱帶淡水魚 ， 分佈於亞洲孟加拉淡水流域 ， 體長可達 10.9 公分 ， 棲息在底層水域 ， 生活習性不明 。']['塘湖站是一個京滬線上的鐵路車站 ， 位於山東省濟寧市微山縣塘湖鄉 ， 建於1942年 ， 目前爲四等站 ， 郵政編碼爲277603 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1861年梵高開始接受教育 ， 在學習語言包括法語 、 德語及英語表現不錯 ， 但在1868年3月中斷學業 ， 並在1869年7月在國際藝術品交易商公司見習 。']['2007年 ， 張五常指出 《 勞動合同法 》 的推出是對中國的 「 大災難 」 ， 十多年來沒有減少批評之 ， 亦因此對中國前景的看法變得較保守 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['英格蘭教會  ， 也譯作英格蘭國教會 、 英國國教會 、 英國教會 、 英格蘭聖公會或英國聖公會 ， 是基督新教聖公宗的教會之一 ， 16世紀英格蘭宗教改革時期 ， 由英格蘭國王亨利八世領導 ， 由神學家托馬斯 · 克蘭麥 、 理查德 · 胡克等研究教義而開創的基督教會 ， 至今作爲英國英格蘭的國教 。']Extracting evidence_list for the eval mode ...\n",
            "['臺灣大學卡通漫畫研究社 ( NTUCCC ） ， 簡稱臺大卡漫社 ， 是國立臺灣大學的動漫畫社團 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['較重的夸克會通過一個叫粒子衰變的過程 ， 來迅速地變成上或下夸克 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['這使阿姆斯特丹成爲荷蘭的商業首都和歐洲頂級金融中心之一 ， 依照 （ GaWC ） 所公佈之 《 世界級城市 》 名單中，阿姆斯特丹屬於ALPHA級別的國際都市。這座城市也是荷蘭的文化之都。許多大型荷蘭機構的總部都設在該市，包括飛利浦、阿克蘇諾貝爾、Booking.com 、 TomTom和ING等 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['420年 ， 劉裕篡位建立劉宋 ， 開啓南北朝時代 ， 東晉亡 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李祖德 （ Tsu - Der Lee ) 生於臺北市 ， 畢業於臺北醫學大學牙醫學系 ， 由專業醫師轉爲經營者 ， 後又投身於創投業 、 當選北醫董事長 、 引進瑞士醫材產業等']\n",
            "['中華人民共和國政府制定的統一 、 規範的通用手語 ， 主要在中國大陸使用 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['鳳凰 ， 亦稱鶠 、 丹鳥 、 火鳥 、 鶤雞 、 威鳳 ， 是中國古代傳說中的百鳥之王 ， 在中華文化中的地位和龍相同 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['此外 ， 他一度爲世界上最高薪的足球運動員 ， 年薪曾經逾6500萬美元 ， 根據於2009年發表的一份估算 ， 其資產總額高達 1.25 億元英鎊 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['勃列日涅夫於1952年加入至中央委員會 ， 於1957年成爲政治局的正式成員 。']['熟石灰在一升水中溶解 1.56 克 ， 它的飽和溶液稱爲石灰水 ， 呈鹼性 ， 與二氧化碳產生化學反應後 ， 會產生碳酸鈣 （ 石灰石 ， CaCO3 ） ， 碳酸鈣加熱後進行熱分解反應 ， 又形成氧化鈣 （ CaO ） 和二氧化碳 （ CO2 ） 。']\n",
            "['許多文化中都有幾何學的發展 ， 包括許多有關長度 、 面積及體積的知識 ， 在西元前六世紀泰勒斯的時代 ， 西方世界開始將幾何學視爲數學的一部份 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['劍橋大學是一個由成員學院 ( College ) 、 學術學院 ( School ) 、 專業學院 ( Faculty ) 、 與學系 ( Department ) 組成的學院聯邦制學校 。']['中英兩國對前訂條約詮釋各執一辭 ， 大清水師登上英艇 「 亞羅 」 號搜捕海盜 ， 因而發生衝突 ， 引發英法聯軍之役 （ 1856至1858年 ） ， 結果兩國於1858年簽訂 《 天津條約 》 ， 英國得派遣外交代表到中國 ， 戰事暫告平息 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2009年9月11日 ， 臼井儀人因登山意外身亡 ， 本漫畫成爲未完成之遺作 ； 其後於2010年由長期協助臼井儀人的助手組成的 「 臼井儀人 ＆ UY Studio 」 續載 ， 並取名爲 《 新蠟筆小新 》 （ 新クレヨンしんちゃん ） 。']\n",
            "['鯡形目 （ 學名 ： Clupeiformes ） 是脊索動物門輻鰭魚綱中的一個目 ， 其中包括許多在漁業上非常重要的成群魚類如鯡魚等 。']Extracting evidence_list for the eval mode ...['前451年 ， 頒佈了十二銅表法 ， 解除了平民不受法律保護的局面 ， 在各方面限制騎士階級和元老院的司法專橫 ， 保障平民的生命財產 ， 這也標誌著羅馬法的誕生 。']\n",
            "\n",
            "\n",
            "['威廉 · 亨利 · 哈里森 （ William Henry Harrison ) 是美國軍官兼政治家 ， 曾於1841年當上第九任美國總統 ， 但就職僅31天就因病去世 ， 是首位任內逝世的美國總統 ， 也是任職時間最短的美國總統 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['殖民地特色是規章明顯異於宗主國 ， 但是宗主國的文化 、 經濟等綜合實力皆優於被殖民地的地區 ， 因故具備強大的影響力 ； 宗主國通過向殖民地輸出文化 、 資本 、 技術 ， 進行建設 ， 甚至頒佈法律 ， 控制殖民地區 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['鐒是在1961年 ， 由阿伯特 · 吉奧索等人在美國加利福尼亞柏克萊的勞倫斯放射實驗室中 ， 利用硼轟擊鐦合成 。']['2013年 ， 小麥是世界上總產量第三的糧食作物 （ 7.13 億噸 ） ， 僅次於玉米 （ 8.44 億噸 ） 和稻 （ 7.45 億噸 ） 。']['意外式暗殺 ： 透過蓄意製造車禍 、 空難 、 疾病 、 火災 、 失足 、 溺水 、 電擊 、 食物中毒 、 親友互殘 、 甚至不惜製造無辜傷亡 ， 以製造目標不幸遭遇意外逝世的假象 ， 令執法部門無法追查 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['攻擊對象主要是盟軍艦船 ， 尤其是航空母艦 。']\n",
            "['2006年入選 《 福布斯 》 全球十大慈善之星 。']['1991年 ， 日本子公司伊藤洋華堂收購了公司70 % 的股份後 ， 於2005年改組爲日本7 - Eleven的子公司 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['巴勃羅 · 魯伊斯 · 畢加索 （ Pablo Ruiz Picasso ) ， 西班牙著名的藝術家 、 畫家 、 雕塑家 、 版畫家 、 舞臺設計師 、 作家和前法國共產黨黨員 ， 出名於法國 ， 和喬治 · 布拉克同爲立體主義的創始者 ， 是20世紀現代藝術的主要代表人物之一 ， 遺作逾兩萬件 。']\n",
            "['蟲川大杉站  位於新潟縣上越市浦川原區蟲川 ， 是北越急行北北線的車站 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['在人類演化的脈絡下 ， 「 人類 」 這個專有名詞指的是 「 人屬 」 ， 但人類演化的研究往往包括其他人科動物 ， 如南方古猿 ， 人屬是在大約230萬至240萬年前的非洲 ， 從南方猿人屬分支出來 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['徐太志  ， 韓國男歌手 、 作詞家 、 作曲家 。']\n",
            "['竹內瑪莉亞  ， 島根縣簸川郡大社町 （ 即今日的出雲市 ） 出生 ， 是日本的著名流行音樂創作歌手 ， 出道當時則是以偶像歌手身份定位 。']\n",
            "['現在希臘神話已經從很多藝術品上關於衆神和英雄故事的裝飾得到考古學上證明 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在接連醜聞與巨大開銷下 ， 美國政府於2010年8月 ， 決定撤出伊拉克 。']Extracting evidence_list for the eval mode ...\n",
            "['漢族總人口約14億 ， 佔世界人口約18 % ， 以人數而言 ， 目前是世界第一大單一族群 ， 其 [ [ 中國各民族生育率表 | 總和生育率 ] ] 低於世代更替水平 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['與此同時 ， 大清帝國宣佈放棄對朝鮮數百年來之宗主國地位 ， 導致日本於數年後兼併朝鮮 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']['不過 ， 中東是一個定義不清的地緣政治區域 ， 包含了跨區域國家伊朗和非洲國家埃及 ， 而西亞則是純粹的地理學名詞 ， 表示亞洲的西南端 ， 人口 3.3 億人 。']['經營版圖橫跨紡織 、 百貨 、 水泥等產業 ， 並創立遠東集團及多所學校 ， 遠東集團更是臺灣三大工業集團之一 ， 爲高度多角化經營的成功典範 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在烏克蘭內戰後 ， 烏克蘭蘇維埃社會主義共和國在1922年成爲了蘇聯創始加盟共和國之一 。']\n",
            "\n",
            "['該校設有小學部及中學部 ， 俱只收男生 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['打赤腳走在路上時一開始會有一種不自在感 ， 這種不自在感來自於心理作用 ， 因爲介意別人的眼光 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['勃列日涅夫於1952年加入至中央委員會 ， 於1957年成爲政治局的正式成員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2010年11月 ， 她參與新北市市長選舉 ， 敗給中國國民黨候選人朱立倫 。']\n",
            "Extracting evidence_list for the eval mode ...['全市總面積 19,078 平方千米 ， 常駐人口 460.0276 萬 （ 2020年 ） 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['客船是主要用於運送乘客的大型船隻 。']Extracting evidence_list for the eval mode ...\n",
            "['靈長大目 （ 學名 ： Archonta ） 或稱 魁獸大目 、 統獸大目 ， 是一個已被棄用的哺乳動物分類單元 ， 在原有的分類體系中稱爲靈長總目 、 魁獸總目或統獸總目 ， 後來學界根據基因序列分析將其與齧齒目 、 兔形目等一同歸爲真魁獸齧型總目 （ Euarchontoglires ） ， 因此靈長大目由總目降級爲大目 ， 而其舊稱靈長總目 、 魁獸總目和統獸總目則成爲真魁獸齧型總目的通用簡稱 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['包拯  ， 字希仁 ， 廬州合肥 （ 今安徽省合肥市肥東 ） 人 ， 北宋人 ， 官至從二品樞密副使 、 朝散大夫 、 給事中 、 上輕車都尉 ， 封東海郡開國侯 、 食邑一千八百戶 、 食實封四百戶 ， 賜紫金魚袋 。']['馬純岱曾在新加坡邵氏馬來亞製片廠工作 ， 他執導的馬來語電影 《 馬六甲英雄傳 》 ， 在第7屆柏林國際電影節上獲得金熊獎提名 。']\n",
            "['由河源至太原市上蘭村爲上游 ， 穿行山地 ， 水土流失嚴重 ， 是洪水 、 泥沙的主要來源 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['漢初三傑 （ 也稱爲興漢三傑 ） 是指漢朝輔佐漢高祖劉邦得天下 ， 建立漢室江山的三位傑出的政治 、 軍事人才和功臣 ， 張良 、 韓信 、 蕭何 。']\n",
            "['敏迷龍曾擁有恐龍中最短的屬名 ， 之後由2004年發現於中國的肉食性恐龍寐龍 （ Mei ） 、 以及在2009年發現於蒙古國的足龍 （ Kol ） 、 2015年發表的奇翼龍 ( Yi ) 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['謝芳  ， 原名謝懷復 ， 女 ， 原籍湖南益陽 ， 生於湖北黃陂 ， 後遷居上海 ， 中國話劇 、 歌劇 、 戲曲 、 電影及電視劇演員 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['中華民國成立時 ， 將回族認定爲中國五大民族之一 ， 雖已經有區分出在新疆的維吾爾族與居住在中國其他地區的回民 ， 但其共同特徵仍爲信仰伊斯蘭教 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['由於蒙古土特產以前都通過河北省張家口市輸往內地 ， 張家口是蒙古貨物的集散地 ， 所以此類蘑菇被稱爲 “ 口蘑 ” ， 口即指張家口 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['竹內瑪莉亞  ， 島根縣簸川郡大社町 （ 即今日的出雲市 ） 出生 ， 是日本的著名流行音樂創作歌手 ， 出道當時則是以偶像歌手身份定位 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['文指典籍 ， 獻指人才 。']\n",
            "['- { zh - hans : 濺射 ; zh - hant : 濺射 } - （ sputtering ） ， 也稱濺鍍 （ sputter deposition / coating ） ， 是一種物理氣相沉積技術 ， 指固體靶 \" target \" （ 或源 \" source \" ） 中的原子被高能量離子 （ 通常來自等離子體 ） 撞擊而離開固體進入氣體的物理過程 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['領土與國家存亡有密切關係 ， 因爲領土是國家的一部份 ， 形成國家的必要條件 ， 國家行使主權的地域及顯示出國家獨有的主權的方式 。']\n",
            "['從狄更斯式的倫敦童年一直達到了電影工業的世界頂端 ， 查理 · 卓別林已成爲了一個文化偶像 。']['自從於1999年被雅虎收購以後 ， 逐漸以用戶的雅虎ID作分頁名稱 ， 把舊有的地方分頁慢慢淘汰 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['阿里山鄉 ， 位於嘉義縣']Extracting evidence_list for the eval mode ...['鄱陽湖是目前中國 “ 第一大淡水湖 ” ， 位於江西省北部 、 長江南岸 ， 鄱陽湖上承贛 、 撫 、 信 、 饒 、 修五河之水 ， 下接中國第一大河 — — 長江 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['隨後於1987年被改編成同名電影 ， 由方中信 、 夏文汐主演 。']\n",
            "\n",
            "['1998年大選後 ， 印度人民黨領導的政黨聯盟 ， 即阿塔爾 · 比哈里 · 瓦傑帕伊總理領導的全國民主聯盟奪得最多議席 ， 持續了一年時間 ， 直到該屆政府垮臺 。']Extracting evidence_list for the eval mode ...\n",
            "['杜鵑科 （ 學名 ： Cuculidae ） 在動物分類學上是鳥綱鵑形目中的唯一科 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['錇 - 249輻射的是低能電子 ， 所以相對安全 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['有些人在家中會赤腳不穿拖鞋 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['它位於豪登省北部城市普利托里亞 。']\n",
            "['2020年因電影重新上映 ， 票房上修爲 316.8 億日圓 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在2022年發佈的QS世界大學排名和泰晤士高等教育世界大學排名中 ， 愛大分別位列世界第15和第29位 。']['大豆含有大量的植酸 、 α - 亞麻酸及異黃酮 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['塞班島 （ Saipan ） ， 爲美國自治邦北马里亞納群島面積最大的島嶼 ， 也是邦內近九成人口所在地 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['高雄縣 ， 爲中華民國已經廢止的一個行政區劃 ， 位於臺灣西南部 ， 與臺南縣 、 臺南市 、 高雄市 、 屏東縣 、 臺東縣 、 花蓮縣 、 南投縣 、 嘉義縣相鄰 。']\n",
            "\n",
            "['《 魂斷藍橋 》 （ Waterloo Bridge ） 是美國黑白電影 ， 由米高梅電影公司於1940年出品 ； 導演是茂文 · 李洛埃 （ Mervyn LeRoy ） ， 女主角是主演 《 亂世佳人 》 的費雯 · 麗 （ Vivien Leigh ） 而男主角是羅伯特 · 泰勒 （ Robert Taylor ） 。']\n",
            "['單晶片 ， 全稱 - { zh - cn : 單片微型計算機 ; zh - tw : 單晶片微電腦 } - （ single - chip microcomputer ） ， 又稱 - { 微控制器 } - 單元 （ microcontroller unit ） ， 是把中央處理器 、 存儲器 、 定時 / 計數器 （ timer / counter ） 、 各種輸入輸出接口等都集成在一塊 - { zh - hans : 集成電路 ; zh - hant : 積體電路 ; } - 芯片上的微型計算機 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['狄安娜 ： 古羅馬女神 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['博羅金諾之戰  是拿破崙戰爭中規模最大 、 死傷者最多的單日戰役 ， 1812年9月7日於莫扎伊斯克鎮以西的博羅金諾村附近爆發 ， 由法皇拿破崙一世所領導的法軍對陣俄羅斯帝國的米哈伊爾 · 庫圖佐夫元帥率領的俄軍 ， 逾二十五萬士兵參與 ， 造成至少七萬人死傷 。']['朝鮮泡菜卡路里含量低 ， 富含纖維素 、 維生素A 、 B 、 C並含有對人體有益的乳杆益生菌 ， 曾在2012年被美國時代華納 《 健康雜誌 》 評爲世界五大最健康食品之一 ， 也有觀點認爲韓式泡菜與其它泡菜一樣含有亞硝酸鹽 ， 過度食用會引發胃癌 。']\n",
            "\n",
            "\n",
            "['裸子植物這個名稱源自希臘語 「 gymnospermos 」 ， 意指 「 裸露的種子 」 ， 因爲裸子植物的胚珠外圍沒有子房壁保護 ， 故稱做裸子植物 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['其西面臺灣海峽 、 東臨宜蘭縣 、 北接新北市 、 南與新竹縣爲界 ， 境內設籍人口約226萬人 ， 爲中華民國境內第五大直轄市 。']\n",
            "['例如明朝抗倭名將俞大猷 ， 因先祖俞敏跟從明太祖打天下 ， 以開國功臣襲泉州衛百戶官 ， 便從家鄉南直隸霍丘落戶當地 ， 故祖籍是南直隸霍丘 ， 籍貫爲福建泉州 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['歐陸舞曲 （ 英文 ： Eurodance ） ， 是在80年代後期到90年代中期流行於歐洲 、 大洋洲及南美洲的舞曲 。']\n",
            "['乙炔 ， 俗稱電石氣 ， 是炔烴化合物系列中體積最小的一員 ， 主要作工業用途 ， 特別是燒焊金屬方面 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['到之後的 《 黑水仙 》 （ 2001年 ） 、 《 中毒 》 （ 2002年 ） 以及 《 颱風 》 （ 2005年 ） 她亦擔任女主角 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['乾隆十二年 （ 1747年 ） 十二月廿九 ， 永琮因出痘夭折 。']\n",
            "\n",
            "['劉一德 （ Liu Yi - te ) ， 臺灣外省人第二代 ， 現任臺灣團結聯盟黨主席 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1966年其首都利奧波德維爾更名爲金沙薩 ， 使其國家簡稱改爲剛果 （ 金 ） 。']['傳統上所有天王星的英語名字都是以威廉 · 莎士比亞或亞歷山大 · 蒲柏的作品中的人物的名稱來命名的 ， 這個傳統是從約翰 · 赫歇爾開始的一直延用至今 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['而具有鏈狀的芳香烴一般稱之爲脂芳烴 （ arenes ） ， 常見的脂芳烴有甲苯 、 乙苯 、 苯乙烯等 。']\n",
            "\n",
            "\n",
            "['格奧爾格 · 費迪南德 · 路德維希 · 菲利普 · 康托爾 （ Georg Ferdinand Ludwig Philipp Cantor ) ， 出生於俄國的德國數學家 （ 波羅的海德國人 ） 。']Extracting evidence_list for the eval mode ...['香港的公營醫療是指由香港特區政府或公營機構 （ 主要爲醫院管理局及衞生署 ） 爲基層階層提供之安全網基本醫療 ， 旨在確保所有人皆可得到醫療服務 ， 是香港社會保障四大支柱之一 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['一般爲透明至微黃色 ， 在任何濃度下都能與水混溶並且放熱 。']\n",
            "\n",
            "['扁盤動物是身體結構最爲簡單的非寄生多細胞動物 ， 在動物界中相當孤立 ， 和其他動物類群之間缺乏關聯性 ， 這一點與多孔動物門的情況類似 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['蜘蛛俠百戰曱甴精是一套菲律賓電影 ， 講述兩位男主角在吞喫了異變的蜘蛛和曱甴 （ 蟑螂 ） 之後化身成爲蜘蛛俠和曱甴精 ， 但二人非但沒有利用能力來造福社會 ， 反而用來追女仔之餘 ， 更對鄰居造成各種破壞 。']['打赤腳走在路上時一開始會有一種不自在感 ， 這種不自在感來自於心理作用 ， 因爲介意別人的眼光 。']['全國民主聯盟 ( 縮寫作 NDA ） 是印度的一個右翼政黨聯盟 ， 成立於1998年 ， 由印度人民黨領導 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['是一種冷凍的甜品 ， 通常以鮮奶油或奶油等乳製品爲原料 ， 並加入水果或其他成分和香料混合製成 。']\n",
            "['迄今爲止 ， 他是唯一擁有哲學博士頭銜的美國總統 （ 法律博士銜除外 ） ， 也是唯一一名任總統以前曾在新澤西州擔任公職的美國總統 。']\n",
            "['創辦人爲遠東集團創辦人徐有庠先生 ， 現任董事長爲其子遠東企業董事長徐旭東先生 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['歐洲民族 主要是操印歐語系語言的族群 ， 其次是使用烏拉語系語言的族群 ， 巴斯克人是歐洲唯一使用孤立語言的族群 ， 在獨聯體國家中尚有一部分使用突厥語系 、 蒙古語系 、 高加索諸語言的族群 ， 在地中海還有使用亞非語系的族群 。']['豆瓣醬是由各種微生物相互作用豆製品 ， 產生複雜生化反應 ， 而釀造出來的一種發酵紅褐色調味料 ， 它是以黃豆或是蠶豆和麪粉爲主要生產原料 ， 同時 ， 又根據消費者的習慣不同 ， 在生產豆瓣醬中配製了香油 、 豆油 、 味精 、 辣椒等原料 ， 而增加了豆瓣醬的品種 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['生物化學 （ biochemistry ， 也作 biological chemistry ） ， 顧名思義是研究生物體中的化學進程的一門學科 ， 常被簡稱爲生化 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['普立茲公共服務獎主要授與的對象爲 ， 透過應用新聞資源而對公共服務有傑出貢獻的報紙或新聞網站 。']\n",
            "\n",
            "\n",
            "['果實 ， 是被子植物 （ 也稱開花植物 ） 花的部份組織衍生成的生殖器官 ， 通常在開花授粉之後 ， 以子房爲主體而形成 ， 其中包含有種子 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['屬於熱帶氣旋的一種 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['阿比讓 （ 或譯 - { zh : 阿必尚 ; zh - hans : 阿必尚 ; zh - hk : 阿必尚 ; zh - tw : 阿比讓 ; } - ， 法語 ： ， -LSB- abidʒɑ̃ -RSB- ） 是象牙海岸的最大都市 （ 港口 ） 和經濟首都 ， 也是象牙海岸實際上的行政中心 （ 象牙海岸名義上的首都是亞穆蘇克羅 ） 。']\n",
            "['然而後期改革過於激烈 ， 加上光緒皇帝有意通過維新派奪回權力 ， 且有維新派首領康有爲建議將慈禧太后囚禁 、 暗殺等傳聞 ， 加上以慈禧爲首的清朝當權保守勢力擔心變法中的計劃最終會導致中國被日本和英國瓜分 ， 步上朝鮮乙未事變的後塵 ， 因而發動了戊戌政變 ， 戊戌變法僅經歷了103日就告終 。']['獎盃 （ Trophy Cup ） 是對那些在某一領域中有傑出表現的人物所提供的一種獎勵物品 ， 特別是在大規模的巡迴體育競技運動中所常見 ， 運動選手通過公平的競賽活動來贏取勝利 ， 勝利者會得到主辦者給予某些物品作爲獎勵 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['柯伊伯帶是短週期彗星 ， 如哈雷彗星 ， 的來源地 。']\n",
            "['電影演員趙丹與第一任妻子葉露茜的女兒 。']Extracting evidence_list for the eval mode ...\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['按含三鍵的多少分別稱單炔烴 、 二炔烴等 :) 。']\n",
            "\n",
            "['承接自臺北帝國大學時代標本室的收藏 ， 館內收藏爲數衆多的百年以上之老標本與珍貴標本 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['淄博是溝通中原地區和山東半島的咽喉要道 ， 是山東省重要的交通樞紐城市 ， 也是 “ 齊國故都 ” 、 “ 陶瓷之都 ” 、 石油化工基地 、 全國文明城市 、 足球起源地 。']['民間受 《 三國演義 》 等傳統作品影響 ， 普遍認爲關羽與劉備 、 張飛義結金蘭 ， 關羽排行第二 ， 故又俗稱其爲關二爺 、 關二哥 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['古典制約 （ classical conditioning ， 又稱巴夫洛夫制約 、 反應制約 、 alpha制約 ） ， 是一種關聯性學習 。']\n",
            "['芝士 ， 又名 - { 乳酪 } - 、 - { zh : 奶酪 ; zh - cn : 乾酪 ; zh - hk : 奶酪 、 乾酪 ; zh - tw : 乾酪 ; zh - sg : 奶酪 、 乾酪 ; zh - my : 奶酪 、 乾酪 } - ， 音譯芝 - { } - 士 、 起 - { } - 司 、 起 - { } - 士 、 吉 - { } - 士 ， 是多種乳制芝士的通稱 ， 有各式各樣的味道 、 口感和形式 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "[\"馬爾地夫面積爲298平方公里 ， 爲亞洲面積最小的國家之一 ， 人口爲人 ， 馬累爲該國首都及最大城市 ， 傳統上稱之爲 「 國王之島 」 （ King 's Island ） 。\"]['本艦服役生涯中最重要的一戰是在黃海海戰中向日軍艦隊發起衝鋒 ， 並承受了日軍的沉重打擊 ， 突擊途中爆炸沉沒 。']['其中性狀又可以細分爲單位性狀和相對性狀 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1952年 ， 有人在壽命超過十億年的紅巨星中發現了鍀 - 98 ， 讓人們認識到恆星可以製造重元素 。']['勇度 · 烏東塔  ， 或簡稱勇度  是一名出現於漫威漫畫出版的美國漫畫書中的虛構人物 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['齊秦爲家中的麼子 ， 其兄名齊魯 ， 與他的姐姐齊豫同爲知名歌手 。']['劍橋大學 （ University of Cambridge ； 勳銜 ： Cantab ） 爲一所座落於英國劍橋郡劍橋市的研究型大學 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['文指典籍 ， 獻指人才 。']['現代晶體學研究主要通過分析晶體對各種電磁波束或粒子束的衍射圖像來進行 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['馬島長尾狸貓 （ 學名 ： ） ， 又名馬島獴 、 窩靈貓或隱肛狸 ， 英文名Fossa ， 是馬達加斯加特有的一種哺乳動物 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['它利用的是多餘的處理器資源 ， 不影響用戶正常使用計算機 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['銀川市 ， 簡稱銀 ， 古稱中興路 、 興慶府 、 懷遠鎮 、 寧夏省城 ， 是中華人民共和國寧夏回族自治區首府 ， 位於寧夏中北部 。']\n",
            "['荷馬史詩不僅具有在西方文學藝術上的重要價值 ， 它在歷史 、 地理 、 考古學和民俗學方面也提供給後世很多值得研究的東西 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['每隊出場5名隊員 ， 可將球向任何方向傳 、 投 、 拍 、 滾或運 ， 目的是將籃球投入對方球籃得分 ， 並阻止對方獲得控球權或得分 。']['劉文正  ， 臺灣男歌手 、 演員 、 主持人 、 製作人 ， 是1970 、 80年代華語流行樂壇的代表性男歌手 ， 他的歌曲流行於臺灣 、 中國大陸 、 東南亞 、 香港等地 ， 具有廣大的影響力 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['光宗不久因紅丸案暴斃 ， 熹宗繼承改元天啓 ， 天啓年間魏忠賢閹黨禍亂朝綱 ， 至崇禎帝即位後剷除閹黨 ， 但閹黨倒臺後 ， 黨爭又起 ， 政治腐敗以及連年天災 ， 導致國力衰退 ， 最終爆發大規模民變 。']['1936年軍隊發生叛亂反政府 ， 佛朗哥是主要參與者之一 ， 西班牙內戰爆發不久之後成爲國民軍大元帥 ， 並於1939年贏得內戰勝利統一全國 ， 成立獨裁政權 ， 以法西斯主義統治西班牙直到他在1975年逝世 ， 這段時間被稱爲 “ 佛朗哥時期 ” 。']['於1980年代及1990年代參與大量電影而爲人所熟悉 ， 並分別在1993年 、 1998年及2018年 ， 憑電影 《 八仙飯店之人肉叉燒包 》 、 《 野獸刑警 》 及 《 淪落人 》 ， 三度榮獲香港電影金像獎最佳男主角獎 ， 並在電影 《 想飛 》 、 《 無間道 》 與 《 頭文字D 》 的演出 ， 三度榮獲金馬獎最佳男配角獎 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['1912年4月10日 ， 鐵達尼號展開首航 ， 也是唯一一次的載客出航 ， 最終目的地爲紐約 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['肇慶是國家歷史文化名城 ， 明嘉靖四十三年 （ 1564年 ） 至清乾隆十一年 （ 1746年 ） ， 肇慶曾爲兩廣總督駐地長達180餘年 。']\n",
            "\n",
            "\n",
            "['計算機協會 （ Association for Computing Machinery ， 簡稱ACM ） 是一個世界性的計算機從業員專業組織 ， 創立於1947年 ， 是世界上第一個科學性及教育性計算機學會 ， 亦是現時全球最大的電腦相關學會 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['畢業生 ： 中國大陸1999年電視劇']\n",
            "['妊娠糖尿病可以用 、 運動來治療 ， 也可能透過注射胰島素來改善 ， 大部份的孕婦可以透過飲食以及運動來控血糖 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['然而由於日本戰敗 ， 此一行政區劃始終未正式實施 ， 僅作爲一般稱呼 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2004年2月13日被聯合國教科文組織地學部評選爲 「 世界地質公園 」 。']\n",
            "['因其陡峭的外觀被選爲日本三大奇形之一 。']['臺北賓館是位於臺灣台北市博愛特區內的官署建築 ， 門牌號碼爲凱達格蘭大道1號 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['汾河 ， 也稱汾水 ， 山西人稱爲 “ 母親河 ” ， 是黃河的僅次於渭河的第二大支流 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['熒光燈中的電流通過汞蒸氣產生波長很短的紫外線 ， 紫外線使熒光體發出熒光 ， 從而產生可見光 。']\n",
            "\n",
            "\n",
            "['梁朝偉亦曾是一名歌星 ， 於1990年代曾錄製多張唱片 ， 其中作品 《 一天一點愛戀 》 、 《 你是如此的難以忘記 》 成爲華語流行音樂名曲 。']Extracting evidence_list for the eval mode ...['在政治與軍事策略上 ， 暗殺的目的是藉由結束敵對勢力首腦的生命 ， 以期能在短時間內達到恫嚇或瓦解敵方士氣 、 混亂或癱瘓敵方指揮系統 。']\n",
            "\n",
            "['ILGA現在擁有超過1100個團體會員 ， 分佈在六大地區的110多個國家 ， 組織願景爲確保任何人不分其性傾向 、 性別認同 、 性別表達和性徵 ， 都能享有自由平等的人權 ， 並致力於實現這樣一種全球正義和平權的世界 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['成立於1861年11月1日 ， 是該州最早成立的十七個縣之一 。']\n",
            "['古與今並沒有確切的時間點來劃分 ， 漢朝時把周朝時叫作 “ 古 ” ， 而漢朝稱作 “ 今 ” ， 晉朝 、 宋朝時把漢朝時叫作 “ 古 ” ， 而晉朝時 、 宋朝時稱爲 “ 今 ” 。']Extracting evidence_list for the eval mode ...\n",
            "['已知人類在賽普勒斯的活動足跡最早可以追溯至西元前 10,000 年 ， 此一時期的遺址有喬伊魯科蒂亞 ， 爲新石器時代保存至今依然完好的建築群 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['1996年底以首張專輯 《 姊妹 》 正式出道 ， 在華語流行樂壇二十餘年來享有極崇高的影響力與地位 ， 並在華語地區享有 「 妹神 」 美譽 。']['當磁暴發生時 ， 在較低的緯度也會出現極光 。']\n",
            "['然而後期改革過於激烈 ， 加上光緒皇帝有意通過維新派奪回權力 ， 且有維新派首領康有爲建議將慈禧太后囚禁 、 暗殺等傳聞 ， 加上以慈禧爲首的清朝當權保守勢力擔心變法中的計劃最終會導致中國被日本和英國瓜分 ， 步上朝鮮乙未事變的後塵 ， 因而發動了戊戌政變 ， 戊戌變法僅經歷了103日就告終 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['大豆含有大量的植酸 、 α - 亞麻酸及異黃酮 。']['1919年至1922年期間 ， 麥克阿瑟出任西點軍校校長 ， 並進行一系列校務改革 ， 之後他被派至菲律賓服役 ， 於1924年還曾平定 「 」 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['現在 ， 貓成爲世界上最爲廣泛的寵物之一 ， 飼養率僅次於狗 ， 但同時也威脅着很多原生鳥類 、 齧齒類的生存 ， 是世界百大外來入侵種之一 。']\n",
            "['創辦 《 日本侵華研究 》 刊物 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['骨鰾類有6000多種魚 ， 世界上大部分淡水魚都屬於骨鰾類 ， 最大的達 4.5 米 ， 重達300千克 ， 最小的只有幾毫米 ， 過寄生生活 。']['漿果 （ Berry ） 是果實的一種類型 ， 屬於單果 ， 常見於分屬於不同科屬的多種植物 ， 例如茄科的番茄 、 茄子 、 馬鈴薯的果實 、 青椒 、 香蕉 、 忍冬科的忍冬 、 葡萄科的葡萄等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['根植於噪音爵士樂和50年代搖滾 ， 披頭士探索了各種音樂類型 ， 從流行謠曲到迷幻搖滾 ， 經常創新地運用經典元素 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['全市人口約520萬 ， 是俄羅斯人口第二大城市 、 以及世界上的最北端的居民超過100萬人的城市 。']['2022年 ， 民進黨在九合一選舉慘敗後 ， 她辭去民進黨主席一職 。']\n",
            "Extracting evidence_list for the eval mode ...['StatCounter在2018年8月的數據表示 ， 在桌面操作系統中 ， macOS的使用份額爲 12.65 % ， 次於Windows的 82.51 % 位居第二 。']\n",
            "\n",
            "['他最著名的作品多半是他在生前最後兩年創作的 ， 期間梵谷的作品乏人問津 ， 深陷於精神疾病和貧困中 ， 最後導致他在37歲那年自殺 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['單晶片 ， 全稱 - { zh - cn : 單片微型計算機 ; zh - tw : 單晶片微電腦 } - （ single - chip microcomputer ） ， 又稱 - { 微控制器 } - 單元 （ microcontroller unit ） ， 是把中央處理器 、 存儲器 、 定時 / 計數器 （ timer / counter ） 、 各種輸入輸出接口等都集成在一塊 - { zh - hans : 集成電路 ; zh - hant : 積體電路 ; } - 芯片上的微型計算機 。']\n",
            "\n",
            "['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']['六和敬 ， 又稱六慰勞法 、 六可憘法 、 六和 、 六和精神 ， 佛教術語 ， 爲追求菩薩道的修行者在團體生活中要遵循的六種生活態度 ， 也是佛教僧團共住時需遵循的六種生活方法 ：']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['也稱 “ 虛擬教研室 ” 。']['周文王認爲 ， 「 爻 」 ， 皎也 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['藤本弘曾長期與安孫子素雄 （ 筆名藤子不二雄Ⓐ ） 合作 ， 兩人相識於高岡市立定冢小學校 ， 並一起繪畫漫畫 ， 投稿到報刊中以藤子不二雄筆名創作 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['長頸鹿是衆多國家公園和主題公園的常客 ， ， 動物園中的長頸鹿有 1,600 多頭 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['該樂隊於1981年在澳洲成立 ， 並於1983年與4AD簽下合約 ， 翌年推出樂隊的處女作 — — 同名專輯 “ Dead Can Dance ” 。']['這個故事由征途中的一系列片段組成 ， 全書的大部分由每個章節引入一種新生物或荒野地點的形式構成 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['許世友出身貧苦農村 ， 少年時曾在少林寺出家並學習中國武術 ， 紅軍時期在紅四方面軍任職 ， 曾任紅四軍軍長 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['貝尼託 · 帕勃羅 · 胡亞雷斯 · 加西亞 （ 西班牙語 ： Benito Pablo Juárez García ， 胡亞雷斯或譯作華雷斯 ) 是一名墨西哥政治人物及民族英雄 ， 曾擔任總統達五個任期之久 ， 被稱爲墨西哥的林肯 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['輕機槍 （ Light machine gun ， 簡稱LMG ） 是相較於通用機槍輕型的一種機槍 ， 可以由一個士兵所操作使用 ， 由於輕機槍一般裝備到步兵分隊或步兵班 ， 有些國家軍隊定位爲機槍手 。']\n",
            "\n",
            "['臺灣天文觀測的研究發展 ， 官方現以中央研究院天文及天文物理研究所和中華民國天文學會爲主 ， 另外部份大學均有天文研究單位或觀測所 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']['1878年起 ， 爲英國所管理 ， 直至於1959年獲得獨立 ， 隔年成爲大英國協會員國 。']['他在爲1974年重拍的 《 大亨小傳 》 （ The Great Gatsby ） 寫劇本的同時 ， 爲喬治 · 盧卡斯 （ George Lucas ） 的突破性電影 《 美國風情畫 》 （ American Graffiti ） 做製片 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['社會規範是由社會學家指出 「 人們共同認可及遵守的行爲標準 」 的一個普遍現象 。']['其前身是日本人所成立的 「 臺灣協會 」 發行之 《 臺灣協會會報 》 。']\n",
            "\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['能夠讓召喚者得到心儀的人之愛 ， 無論是男人還是女人的愛 ， 直到召喚者滿意爲止 。']['京畿道  是位於朝鮮半島中部的一個韓國行政道 ， 北隔着三八線與朝鮮民主主義人民共和國相鄰 ， 東爲江原道 ， 南與忠清北道和忠清南道接壤 ， 西與中華人民共和國隔海而望 。']['雞 （ 學名 ： Gallus gallus domesticus ） ， 是原雞屬原雞中被人類馴化後而成的亞種 ， 家雞最初被馴化成爲家禽的目的是提供廉價優質的動物蛋白質食物來源 ， 是家畜及家禽中數量最多 ， 分佈也最廣的 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['醫療上有用作清潔消毒 ， 和用來消滅真菌之用 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['艾根訴加拿大案 （ Egan v. Canada ） [ 1995 ] 2 S.C.R. 513是加拿大最高法院圍繞同性戀權利而作出的一個判決 。']\n",
            "['李賦寧  ， 男 ， 祖籍陝西蒲城 ， 生於江蘇南京 ， 中國教育家 、 翻譯家 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['這個故事由征途中的一系列片段組成 ， 全書的大部分由每個章節引入一種新生物或荒野地點的形式構成 。']\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['因此 ， 研究宗教哲學的人並不需要有任何宗教信仰 。']\n",
            "\n",
            "['臺東飛行場 ， 爲日治時期位於臺東郡卑南莊利家 （ 今 臺東市 ） 的一座機場 ， 在二戰後廢止 。']Extracting evidence_list for the eval mode ...\n",
            "['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['元素週期表中超過四分之三的化學元素都屬於金屬 ， 其中較爲一般人所知的有金 、 銀 、 銅 、 鐵 、 鋁 、 錫 、 鉛 、 鋅 、 鈉 、 鈣等 。']\n",
            "['現存最古老的食譜來自公元前15世紀的巴比倫 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['塘湖站是一個京滬線上的鐵路車站 ， 位於山東省濟寧市微山縣塘湖鄉 ， 建於1942年 ， 目前爲四等站 ， 郵政編碼爲277603 。']['曾是常用消毒藥劑的紅藥水中就含有溴和汞 。']\n",
            "Extracting evidence_list for the eval mode ...['創辦 “ 日本侵華研究協會 ” 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['1980年代末入行 ， 90年代紅遍澳 、 港 、 臺 、 陸 、 新加坡 、 馬來西亞及亞洲外國等地區 ， 更到過日本發展 ， 有 「 玉女掌門人 」 的美譽 ， 近年來樂於當兼職藝人 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['前身爲臺灣日治時期的臺北一中 、 三中及四中 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['林鈺婷 Rita ， 臺灣新竹竹東客家人 ， 山狗大後生樂團團長及主唱 ， 也是電視廣播主持人及配音員等 。']['汾河 ， 也稱汾水 ， 山西人稱爲 “ 母親河 ” ， 是黃河的僅次於渭河的第二大支流 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['保羅 · 阿德里安 · 莫里斯 · 狄拉克 ， OM ， FRS （ Paul Adrien Maurice Dirac ) ， 又譯狄喇克 ， 英國理論物理學家 ， 量子力學的奠基者之一 ， 曾經主持劍橋大學的盧卡斯數學教授席位 ， 並在佛羅里達州立大學度過他人生的最後十四個年頭 。']\n",
            "Extracting evidence_list for the eval mode ...['各島泥土不厚 、 風浪較大 ， 僅釣魚臺上有淡水溪流 。']['室女座中亮於 5.5 等的恆星有58顆 ， 最亮星爲角宿一 （ 室女座α ） ， 視星等爲 0.98 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['他的宣示招致了波斯和奧斯曼帝國的迫害和監禁 ， 最終被囚禁於巴勒斯坦 （ 今以色列 ） 的阿卡24年 ， 並在那裏去世 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['鹼石灰可通過熟石灰在濃氫氧化鈉溶液中反應制得 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['學校的研究人員在第二次世界大戰及冷戰期間 ， 致力開發電腦 、 雷達及慣性導航系統技術 ； 戰後的防禦性科技研究使學校得以進一步發展 ， 教職員人數及校園面積在的帶領下有所上升 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['世界上最早建立的自然保護區爲美國加利福尼亞州西邁山谷的紅杉樹保護區 ， 在1864年建立 。']\n",
            "['海伍德 · “ 伍迪 ” · 艾倫 （ Heywood \" Woody \" Allen ) ， 本名艾倫 · 斯圖爾特 · 柯尼斯堡 （ Allen Stewart Konigsberg ） ， 美國電影導演 、 編劇 、 演員 、 喜劇演員 、 作家 、 劇作家和音樂家 ， 其職業生涯已逾50年 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['依照現行八二憲法和 《 中華人民共和國全國人民代表大會組織法 》 的規定 ， 全國人大代表每屆任期爲5年 ， 即從每屆全國人大舉行第一次會議開始 、 到下屆全國人大舉行第一次會議爲止 ； 補選代表的任期 ， 從補選產生之日到本屆人大任期屆滿爲止 。']\n",
            "['其代表作漫畫 《 丁丁歷險記 》 享譽全球 ， 至今在歐洲仍然不斷重版 ， 在中國也是二十世紀八十年代少數幾部能夠在商店中找到的外國連環畫之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['琴筒一端蒙以蛇皮 ， 這是二胡獨特音色的來源 ， 一些高級的二胡則採用蟒蛇皮 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['荷馬史詩是古希臘文學中最早的一部史詩 ， 也是最受歡迎 、 最具影響力的文學作品 。']\n",
            "Extracting evidence_list for the eval mode ...['2014年 ， 京杭大運河作爲大運河的一部分 ， 被列入世界遺產名錄 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['葡萄的生長沒有呼吸躍變 （ 非更年性 ） ， 其果實成簇聚集在一起 。']\n",
            "['它於1917年根據約瑟夫 · 普利策的遺囑而成立 ， 這是一位因爲報紙出版而發家的富豪 ； 這個獎項由哥倫比亞大學負責管理 。']['後來很多傳說相信這個杯子具有某種神奇的能力 ， 如果能找到這個聖盃而喝下其盛過的水就將返老還童 、 死而復生並且獲得永生 ， 這個傳說廣泛延續到很多文學 、 影視 、 遊戲等作品中 ， 比如亞瑟王傳說中 ， 就有人說他終其一生的最大目標就是找到這個聖盃 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['市政府站位於臺灣台北市信義區 ， 當地地名 「 興雅 」 ， 爲臺北捷運板南線 （ 南港線 ） 的捷運車站 。']\n",
            "['幹興元年 （ 1022年 ） 二月 ， 真宗崩 ， 仁宗即帝位 ， 時年13歲 ， 由嫡母劉太后攝政 ； 1023年改年號爲天聖 ； 1033年 ， 劉太后歸政 ， 仁宗親政 ； 1063年駕崩於汴梁皇宮中 ， 享年54歲 ， 在位41年 ， 爲宋朝在位時間最長的皇帝 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']['史東的許多電影都集中在20世紀末期有爭議的美國政治問題上 ， 而在上映時都被認爲是具爭議的 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['在艾伯特15歲時就派當在萊比錫市立圖書館工作 ， 並在當地學習了短期的神學 。']\n",
            "\n",
            "['2004年和2012年的金星凌日探測對於尋找太陽系外行星以及探測系內行星環境等方面的研究都有所助益 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雖然所有的細胞在代謝時都會產生熱量 ， 大部分的爬行動物不能產生足夠的熱量以保持體溫 ， 因此被稱爲冷血動物或變溫動物 （ 鳥類 、 棱皮龜則是例外 ） 。']['1971年王振鐸根據史書記載 ， 複製馬鈞的 “ 黃帝指南車 ” 成功 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['摩納哥地處法國南部 ， 除了靠地中海的南部海岸線之外 ， 全境北 、 西 、 東三面皆由法國包圍 ， 主要是由摩納哥舊城和隨後建立起來的周遭地區組成 。']\n",
            "['2004年 ， 辦公室主任尹建國 。']Extracting evidence_list for the eval mode ...['電子科大是工商管理碩士協會成員 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['熱帶海洋性氣候 ， 終年高溫多雨 ， 是一種分佈在赤道周圍的熱帶雨林氣候 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1912年4月10日 ， 鐵達尼號展開首航 ， 也是唯一一次的載客出航 ， 最終目的地爲紐約 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['國民政府軍事委員會爲中國國民黨主導之中華民國國民政府最高軍事機關 。']['1982年 ， 嵩山被中國國務院列爲第一批國家級風景名勝區 。']['沈鈞儒  ， 字秉甫 ， 號衡山 ， 男 ， 江蘇蘇州人 ， 祖籍嘉興 。']['李商隱  ， 字義山 ， 號玉谿生 、 樊南生 ， 祖籍隴西狄道 （ 今甘肅省臨洮縣 ） ， 祖輩遷滎陽 （ 今河南鄭州 ） ， 晚唐詩人 ， 和杜牧合稱 “ 小李杜 ” ， 與溫庭筠合稱爲 “ 溫李 ” ， 與同時期的段成式 、 溫庭筠風格相近 ， 且都在家族裏排行十六 ， 故並稱爲三十六體 。']\n",
            "\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "\n",
            "['在2022年發佈的QS世界大學排名和泰晤士高等教育世界大學排名中 ， 愛大分別位列世界第15和第29位 。']['罩杯的別稱 ， 指鋼圈內衣包覆乳房而呈碗形的部分 。']['2006年國際足協世界盃 （ 2006 FIFA World Cup ） 爲國際足協第十八屆舉行的世界盃足球賽 ， 於2006年6月9日至7月9日於德國十二個城市舉行 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['因爲國旗當中存在清真言 ， 所以索馬利蘭國旗同沙烏地阿拉伯國旗一樣 ， 也沒有降半旗的制度 。']\n",
            "\n",
            "['父親是努斯拉特 - 奧德 - 杜烏拉 · 菲魯茲 · 米爾扎 （ Nosrat - od - Dowleh Firouz Mirza ） ， 1859年出生 ， 1939年11月逝世 ， 享年80歲 。']['1940年考入國立中央大學工學院電機工程系 ， 1944年畢業 ， 1945年任電機系助教 ， 1949年任國立南京大學 （ 該年國立中央大學改名國立南京大學 ） 電機系講師 、 南京大學校務委員會常委 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['總統制又稱爲總統共和制 ， 是共和制政體的一種 ， 由行政首長領導一個獨立於立法部門的行政部門 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['土衛十一又稱爲 「 艾比米修斯 」 （ Epimetheus ） ， 是土星的一顆內側衛星 ， 它的專屬名稱艾比米修斯源自神話 ， 是普羅米修斯的兄弟 。']['該大學創立於1867年 ， 本是美國商人丹尼爾 · 德魯爲衛理公會成立的神學院 ， 1928年擴大成爲文理學院 ， 1955年開始接受研究生入學 。']\n",
            "\n",
            "['維基一詞出自 「 Wikipedia 」 的中文譯名 「 維基百科 」 ， 是維基媒體基金會 （ Wikimedia Foundation , Inc. ） 的一個商標 ， 用於所有由維基媒體基金會營運 、 使用wiki引擎發展的中文站點上 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['氰化鋰強烈水解生成氰化氫 ， 水溶液呈強鹼性 。']\n",
            "\n",
            "['古賽爾戰役 （ Al - Qusayr offensive ） 是開始於2013年4月4日 ， 由敘利亞政府軍和真主黨所發動的戰役 ， 目標攻佔古賽爾附近被敘利亞反對派佔領的所有村莊 ， 使古賽爾逐漸陷入包圍 ， 進而攻打 。']Extracting evidence_list for the eval mode ...\n",
            "['起點爲金山 、 終點爲基隆 ， 部份班次繞駛至龜吼漁港 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['， 坐落於蘇格蘭首府愛丁堡市 ， 是一所成立於1583年的公立研究型大學 。']['於元朝 （ 1271年 – 1368年 ） 末至正三年 （ 1343年 ） 由元朝丞相脫脫和阿魯圖先後主持 ， 與 《 遼史 》 、 《 金史 》 同時修撰 。']\n",
            "\n",
            "['沒有留下著作 ， 其思想和生平被記述於後來的學者 （ 主要是他的學生柏拉圖 ） 和同時代的劇作家阿里斯托芬的劇作中 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['- { zh - cn : 形而上學 ， 港澳臺簡稱形上學 ; zh - tw : 形上學 } - ， 也稱爲形之上學 、 元物理學 （ Metaphysics ） ， 在古希臘時期是指研究存在和事物本質的學問 。']Extracting evidence_list for the eval mode ...['漢趙 （ 304年 － 329年 ） ， 又稱前趙 ， 是南匈奴人劉淵所建的君主制政權 ， 都平陽郡 （ 今山西臨汾西北 ） ， 爲十六國時期建立的第一個政權 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['分佈在中國大陸的雲南等地 ， 生長於海拔 1,400 米至 3,200 米的地區 ， 常生於山谷林下 、 河邊灌叢和高山草甸中 ， 目前尚未由人工引種栽培 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['任大理寺丞 ， 一年中判決了大量的積壓案件 ， 涉及到一萬七千人 ， 無冤訴者 。']['愛丁堡大學在2021年英國政府的研究卓越框架 （ Research Excellence Framework ） 中 ， 其研究實力位居英國第4位 ， 僅次於牛津大學 、 倫敦大學學院和劍橋大學 。']['一般認爲商業行爲成立的條件有以下幾點 ：']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1:1 的共振 （ 有著相似軌道半徑的天體 ） 在特殊的情況下 ， 造成太陽系大天體將共享軌道的小天體彈射出去 ； 這是清除鄰居最廣泛應用的機制 ， 而此一效果也應用在目前的行星定義中 。']['煉武臺站  是位於韓國忠清南道論山市煉武邑的一個車站 ， 啓用日期爲1958年5月15日 ， 屬於江景線 。']\n",
            "['西門站位於臺灣台北市萬華區 、 中正區交界處 ， 爲臺北捷運板南線 、 松山新店線交會的捷運車站 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['杜鵑科 （ 學名 ： Cuculidae ） 在動物分類學上是鳥綱鵑形目中的唯一科 。']\n",
            "['《 天鵝湖 》  原爲柴可夫斯基於1875年 - 1876年間爲莫斯科帝國歌劇院所作的芭蕾舞劇 ， 於1877年3月4日 （ 俄羅斯舊曆2月20日 ） 在莫斯科大劇院首演 ， 之後作曲家將原作改編成了在音樂會上演奏的 《 天鵝湖 》 組曲 ， 組曲出版於1900年11月 。']['太行山又名五行山 、 王母山 、 女媧山 ， 或作太形山 。']\n",
            "Extracting evidence_list for the eval mode ...['中華民國國會全面改選是指1990年代發生在臺灣的政治改革 ， 終止了自中華民國政府遷臺後40餘年從未全面改選過的中央民意代表 （ 國會議員 ） 所組成的 「 萬年國會 」 ； 並以中華民國自由地區 （ 即臺灣 、 澎湖 、 金門與馬祖等島嶼 ） 爲範圍進行中央民意代表定期改選 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['它在2005年3月被命名爲Hegemone （ 赫革摩涅 ， 是卡里忒斯 （ 美惠三女神 ） 之一 ， 宙斯 （ Jupiter ） 的女兒 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1999年8月底在澳洲等地區的支持下通過公投決定獨立 ， 2002年5月20日零時獨立 ， 2002年9月27日正式加入聯合國 ， 成爲第191個聯合國會員國 。']\n",
            "['穆斯林認爲 《 古蘭經 》 是指導穆罕默德聖人奉行使命的奇蹟 ， 證明他的先知身份 ， 而他亦是自亞當以來最後一位接收啓示的先知 。']['古希臘神話是歐洲文明的搖籃 ， 對西方文化 、 藝術 、 文學和語言有著明顯而深遠的影響 。']Extracting evidence_list for the eval mode ...['其中在2012年10月8日 （ 農曆壬辰年八月廿三日 ， 星期一 ， 黃大仙誕 ） 開光的黃大仙雕像 ， 像高 3.22 米 ， 自稱是當時世界最大的金身白玉黃大仙雕像 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['於1990年被英國女王伊麗莎白二世授予爵士稱號 。']['諾貝爾獎 （ Nobel priset ， Nobel prisen ， Nobel Prize ） ， 是根據瑞典化學家阿爾弗雷德 · 諾貝爾遺囑於1901年開始每年頒發的5個獎項 ， 包括 ： 物理 、 化學 、 生理學或醫學 、 文學 、 和平 。']['繼它們的成功之後 ， 他着手把約瑟夫 · 康拉德 （ Joseph Conrad ） 的小說 《 黑暗之心 》 （ Heart of Darkness ） 拍攝爲電影 ， 把影片中的時間背景設定在越戰期間 ， 電影取名爲 《 現代啓示錄 》 。']['在20世紀後期 ， 由於納米比亞本土的政治家起義以及政治代表尋求獨立 ， 導致聯合國在1966年承擔對西南非的直接責任 ， 但南非保持了事實上的統治 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['爲了解其後乃至現代中國歷史 、 中國社會與中國文化 ， 三國時代很重要 ， 不可忽視 ； 在世界歷史上 ， 中國是一個極具獨特文化之國家 ， 許多特徵都是起源於三國時代 ， 例如紙張之普及使用 。']['西奧多 · 羅斯福在總統任期內 ， 對國內的主要貢獻是建立資源保護政策 ， 保護了森林 、 礦物 、 石油等資源 ； 建立公平交易法案 ， 推動了勞工與資本家和解 。']\n",
            "\n",
            "['軟件一般是通過某種或數種程序設計語言 、 在特定的計算機平臺上實現的 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "\n",
            "['中國人民解放軍武漢軍區是中國人民解放軍1955年3月7日設立的一個大軍區 ， 下轄湖北省軍區 、 河南省軍區 。']['目前12月25日因爲這天也是基督教的聖誕節 ， 部份企業與基督新教 、 天主教會學校仍會宣佈放假 。']['婆羅洲全境由印尼 、 馬來西亞及汶萊三國管轄 ， 是世界上管轄國家最多的島嶼 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['因爲身爲家中唯一男丁的哥哥早逝 ， 基於延續馬家香火的想法 ， 將二姐的兒子過繼給未婚的馬世莉扶養 。']\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']Extracting evidence_list for the eval mode ...['狻猊  ， 又寫作狻麑 ， 是中國古代文獻中記錄的一種獸類 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['貨幣代碼採行ISO 4217標準編爲TWD ， 符號爲NT $ 或NTD ， 並使用NT $ 100 、 NTD100之類方法表示 （ 中間無空格 ） 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['翌年 ， 他在1981年以黨外人士身分當選爲臺北市議員 ， 並在1985年得以連任 。']\n",
            "['張谷英村位於湖南省岳陽市以東的渭洞筆架山下 ， 地處岳陽縣 、 平江縣 、 汨羅市三縣市交匯處 ， 距離長沙市 、 岳陽市分別約150公里和70公里 ， 爲中國保存最爲完整的江南民居古建築群落 ， 至今已存在了500多年 。']['古賽爾戰役 （ Al - Qusayr offensive ） 是開始於2013年4月4日 ， 由敘利亞政府軍和真主黨所發動的戰役 ， 目標攻佔古賽爾附近被敘利亞反對派佔領的所有村莊 ， 使古賽爾逐漸陷入包圍 ， 進而攻打 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['航空交通管制 （ 英文 ： Air traffic control ， 縮寫 ： ATC ） 是指由在地面的航空交通管制員協調和指導空域或機場內不同航空器的航行路線和飛航模式以防止飛航器在地面或者空中發生意外和確保他們均可以運作暢順 ， 達至最大效率 。']\n",
            "['在西方世界中表示一般記成 “ AH ” （ Anno Hegirae ， 意即 \" 遷徙了多少年 \" ） ， 類似於基督紀年的 “ AD ” 和世界紀元的 “ AM ” 。']\n",
            "['茲德內克 · 莫拉維克  ， 捷克天文學家 ， 以發現大量小行星聞名 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['科學院站  是聖彼得堡地鐵基洛夫 － 維堡線的一個車站 ， 開通於1975年12月31日 。']['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']['2016年的人口普查結果顯示 ， 以波士頓領銜的大波士頓擁有480萬人口 ， 乃全美第十大的大都會區 ； 以波士頓爲中心的聯合統計區則擁有820萬人口 ， 乃全美第六大 。']['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['2006年8月10日 ， 由講談社出版 。']['該校前身爲1989年創立的元智工學院 ， 並於1997年升格大學 。']\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...['吳健雄是美國物理學會的第一位女性會長 ， 常被人稱爲 “ 中國的居里夫人 ” 、 “ 物理研究的第一女士 ” 、 “ 核子研究的女王 ” 以及 “ 世界最傑出的女性實驗物理學家之一 ” 。']\n",
            "\n",
            "\n",
            "['敏迷龍曾擁有恐龍中最短的屬名 ， 之後由2004年發現於中國的肉食性恐龍寐龍 （ Mei ） 、 以及在2009年發現於蒙古國的足龍 （ Kol ） 、 2015年發表的奇翼龍 ( Yi ) 。']['在國家科委的支持下 ， 袁隆平的水稻研究在1970 － 2000年代屢有突破 ， 使他獲聯合國多個獎項和獲封澳門科技大學榮譽博士 ， 於2019年更獲頒共和國勳章 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['後來該片集於1993年重拍成四集OVA 。']\n",
            "['該教的宇宙觀將人類歷史分成青陽 、 紅陽 、 白陽三期 ， 含有末劫救贖思想 ， 相信無極老母派遣彌勒佛 （ 轉世爲祖師路中一 ） 拯救凡間 。']\n",
            "\n",
            "\n",
            "['奧黛麗 · 赫本晚年淡出影壇投身公益 ， 1988年至1993年期間擔任 ， 她曾經出訪埃塞俄比亞 、 厄瓜多爾等許多國家探訪貧困兒童 ， 致力爭取兒童權利 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['現有的生物被分入古菌 、 細菌 、 真核3個域 ， 且只有在真核生物中還有 “ 界 ” 的分法 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['地處杭嘉湖平原中東部 ， 全境地勢平坦 。']\n",
            "\n",
            "['1963年 ， 金髮起 「 向華盛頓進軍 」 行動 ， 在林肯紀念堂前發表 《 我有一個夢 》 演講 ， 成爲美國曆史上最負名望的演說家 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['萼板 （ Thecal plate ） 不規則排列 ， 萼板之間有外露的縫孔 （ Sutural pore ） 作爲外呼吸孔 。']\n",
            "['《 葛底斯堡演說 》 （ Gettysburg Address ） 是第16任美國總統亞伯拉罕 · 林肯最著名的演說 ， 也是美國曆史上爲人引用最多之政治性演說 。']\n",
            "Extracting evidence_list for the eval mode ...['荷馬史詩不僅具有在西方文學藝術上的重要價值 ， 它在歷史 、 地理 、 考古學和民俗學方面也提供給後世很多值得研究的東西 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['加布艾萊西島是蘇格蘭的島嶼 ， 位於大西洋海域 ， 屬於內赫布里底群島的一部分 ， 面積 1.42 平方公里 ， 最高點海拔高度110米 ， 島上無人居住 。']\n",
            "['常用於古典音樂及爵士樂 ， 小號中的是銅管樂器家族中音域最高的樂器之一 ， 而的音高比常見的降B調小號要低一個八度 。']Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['1988年 ， 根據 《 國務院機構改革方案 》 ， 成立了人事部及勞動部 ， 勞動人事部被撤銷 。']['城市規劃是城市建設及管理的依據 ， 位於城市管理之規劃 、 建設 、 運作三個階段之首 ， 是城市管理的龍頭 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['最短路徑問題是圖論研究中的一個經典算法問題 ， 旨在尋找圖 （ 由結點和路徑組成的 ） 中兩結點之間的最短路徑 。']Extracting evidence_list for the eval mode ...\n",
            "['進入微軟公司後 ， 先後主持了Visual J++、.Net ， C# 和 TypeScript 。']['2010臺北國際花卉博覽會 ， 簡稱臺北花博 、 臺北國際花博 ， 2010年11月6日至2011年4月25日間於中華民國臺北市舉行的國際園藝博覽會 ， 是臺灣第一個正式獲得國際園藝家協會及國際展覽局認證授權舉辦的A2 / B1級國際園藝博覽會 ， 也是臺灣歷史以來第三次舉辦大型博覽會 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['內燃機的燃燒氣體同時也是工作介質 ， 比如汽油機中 ， 汽油燃燒後的氣體直接推動活塞做功 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['澳門特別行政區2010年度勳章 、 獎章和獎狀名單于2010年12月19日由澳門特別行政區行政長官崔世安簽署 ， 並於 《 澳門特別行政區政府公報 》 公佈 ， 共有38位人士 、 團體 、 機構分別獲授勳 ， 表揚他們在個人成就 、 社會貢獻或服務澳門特別行政區方面有傑出的表現 。']\n",
            "['穆斯林認爲 《 古蘭經 》 是指導穆罕默德聖人奉行使命的奇蹟 ， 證明他的先知身份 ， 而他亦是自亞當以來最後一位接收啓示的先知 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['他向大貴族宣稱 “ 朕即國家 ” ， 並把他們集中在凡爾賽宮居住 ， 將整個法國的官僚機構集中於他的周圍 ， 以此強化法國國王的軍事 、 財政和機構的決策權 。']\n",
            "\n",
            "['2005年7月8日 ， 德國Verden市法院認定他製造震盪波蠕蟲 ， 四次改變數據和三次對計算機實施破壞有罪 ， 判處21個月的緩刑 ， 在緩刑期間必須完成30個小時的感化工作 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['鹼石灰可通過熟石灰在濃氫氧化鈉溶液中反應制得 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['慶長八年 （ 1603年 ） 受後陽成天皇詔封爲徵夷大將軍 ， 並在江戶開創幕府 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "\n",
            "['後來 ， 沒有細胞核的細菌 ， 終被獨立爲一界 ， 再後來真菌也因爲沒有葉綠體 ， 被分出植物界 ， 也成爲獨立的一界 ， 最後自立爲界的是古細菌 。']\n",
            "Extracting evidence_list for the eval mode ...['水果酒 （ Fruit wine ） ， 以植物果實爲原料 ， 發酵而成的酒精飲料 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['它是一種無色到淡黃色的液體具有類似於香茅油一樣的柑橘味 。']['另在澳門 、 臺灣及美國有派駐記者 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['虛擬實境 （ virtual reality ， 縮寫VR ） ， 簡稱爲虛擬技術 ， 也稱虛擬環境 ， 是利用電腦模擬產生一個三維空間的虛擬世界 ， 提供使用者關於視覺等感官的模擬 ， 讓使用者感覺彷彿身歷其境 ， 可以即時 、 沒有限制地觀察三維空間內的事物 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在1983年2月遭受腎衰竭後 ， 安德羅波夫的健康開始迅速惡化 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['該教的宇宙觀將人類歷史分成青陽 、 紅陽 、 白陽三期 ， 含有末劫救贖思想 ， 相信無極老母派遣彌勒佛 （ 轉世爲祖師路中一 ） 拯救凡間 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['鎮江市 ， 簡稱鎮 ， 古稱京口 、 潤州 ， 是中華人民和國江蘇省下轄的地級市 ， 位於江蘇省南部 ， 長江南岸 。']\n",
            "\n",
            "['葉巒 ， 成化十七年進士 、 明朝政治人物 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['冰島的首都是雷克雅維克 ， 也是冰島的最大城市 ， 首都附近的西南地區人口占全國的三分之二 ， 即24萬人左右 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['最短路徑問題是圖論研究中的一個經典算法問題 ， 旨在尋找圖 （ 由結點和路徑組成的 ） 中兩結點之間的最短路徑 。']\n",
            "['萼板 （ Thecal plate ） 不規則排列 ， 萼板之間有外露的縫孔 （ Sutural pore ） 作爲外呼吸孔 。']Extracting evidence_list for the eval mode ...\n",
            "['車站名稱的由來是鄰近的白山神社內存有國家指定的天然紀念物 「 蟲川大杉 」 （ ) 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺北市私立道明外僑學校  ， 簡稱道明學校 ， 又稱道明國際學校 ， 是臺北市一所專門讓外國僑民就讀的高級中學 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['1912年4月10日 ， 鐵達尼號展開首航 ， 也是唯一一次的載客出航 ， 最終目的地爲紐約 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['木星是顆巨行星 ， 質量是太陽的千分之一 ， 但卻是太陽系其他行星質量總和的 2.5 倍 。']\n",
            "['兒玉利國 ， 日本鹿兒島縣人 ， 明治時代官員 ， 日軍海軍少將 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['全買 （ OBUYING ） ， 全名全買大賣場或全買生鮮超級市場 ， 是臺灣雲嘉南地區一家連鎖經營20年的老字號大賣場與生鮮超市 ， 以 「 長頸鹿 」 爲代表象徵與特色 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['能夠讓召喚者得到心儀的人之愛 ， 無論是男人還是女人的愛 ， 直到召喚者滿意爲止 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺中市立啓明學校 （ Taichung Minicipal Taichung Special Education School for The Visually impaired ） 簡稱中明 ， 是臺灣台中市后里區一所專收視覺障礙生的公立特殊教育學校 ， 1968年獨立創校 。']\n",
            "['冠蕉鵑是唯一現存的鳥類含有大量綠色色素的 ， 雖然鸚鵡也呈綠色 ， 但這些並非色素 ， 而是結構生色的 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['研揚大樓  ， 爲國立臺灣科技大學公館校區之教學研究大樓 。']\n",
            "\n",
            "['神農氏 ， 又稱烈山氏 、 連山氏 、 炎帝 ， 相傳生存年代在夏朝以前 ， 現存文字記載多出現在戰國以後 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['「 Q.E.D. 」 可以在證明的尾段寫出 ， 以顯示證明所需的結論已經完整了 。']['1995年 ， 以劇情片 《 我的美麗與哀愁 》 出道 ； 同年以愛情片 《 少女小漁 》 獲得第40屆亞太影展最佳女主角 ； 主打歌 《 爲愛癡狂 》 獲得第32屆金馬獎最佳電影歌曲獎 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['虛擬教研中心 ： 利用虛擬教研模式組織起來的跨時空的教研群體或團隊 。']\n",
            "['2001年6月25日被公佈爲全國重點文物保護單位 ， 2003年被評爲中國歷史文化名村 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['第二十屆世界青年日從8月15日星期一到8月21日星期日在科隆舉辦 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['西印度群島的原住民發現這種植物的提取物可以令魚麻醉安靜 ， 讓他們可以徒手抓魚 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['StatCounter在2018年8月的數據表示 ， 在桌面操作系統中 ， macOS的使用份額爲 12.65 % ， 次於Windows的 82.51 % 位居第二 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['新竹市  是中華民國臺灣省的市 ， 位於臺灣西北部 ， 爲新竹都會區的中心城市 ， 1718年建城 ， 是北臺灣閩南人最早建立的城市 。']\n",
            "['正壬醇 （ 1 - 壬醇 ） 是一種直鏈脂肪醇 ， 含有九個碳原子 ， 分子式爲CH3 ( CH2 ) 8OH 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['基隆市  是位於臺灣東北部的都市 ， 爲中華民國實際管轄區域的三個市之一 ， 古名雞籠 ， 以谷灣之天然港灣著名 ， 乃臺灣最北端的都市 ， 位於市中心的基隆港則是北臺灣首要航運樞紐 ， 因而有臺灣頭 、 臺灣北玄關之稱 ， 亦因氣候多雨而別稱雨港 、 雨都 ， 與高雄並列爲臺灣兩大港市 。']\n",
            "['花旗銀行 （ Citibank , N.A. ， 2002年前香港曾稱之爲萬國寶通銀行 ） 是花旗集團屬下的一家零售銀行 ， 其主要前身是1812年6月16日成立的 「 紐約城市銀行 」 （ City Bank of New York ） ， 經過兩個世紀的發展 、 收購 ， 已經成爲美國以資產計第三大銀行 ， 也是一間在全球近一百六十個國家及地區設有分支機構的國際級銀行 ， 總部位於紐約市格林威治街390號 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['能夠讓召喚者得到心儀的人之愛 ， 無論是男人還是女人的愛 ， 直到召喚者滿意爲止 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['1982年與山下達郎結婚 ， 育有一女 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['阿馬蒂亞 · 庫馬爾 · 森 ， CH （ अमर्त्य कुमार सेन ， Amartya Sen ， 又譯爲沈恩 ) 以對福利經濟學的貢獻 ， 獲得諾貝爾經濟學獎 （ 1998年 ） ， 後獲得印度政府頒發 （ 1999年 ） 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['最明顯的改變是在1965年當新的移民法 「 1965年移民法 」 通過後 ， 廢除了原爲了限制亞洲移民而設立的各國移民輸出限制 ， 而改成以家庭因素 、 及專業技術爲考量的新移民政策 。']['《 明報 》 是香港的中文報紙 ， 由武俠小說泰斗查良鏞 （ 筆名金庸 ） 和沈寶新在1959年5月20日創立 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['該書不僅對研究突厥語言學有重要價值 ， 而且對研究古代中亞地區諸突厥部落的歷史 、 文化 、 地理 、 文學 、 民俗 、 社會情況等 ， 也提供了大量極其珍貴的資料 ， 具有很高的學術價值 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['該校自創校以來一直採用英文爲主要教學語言 ， 現在共有十所學術學院 ， 作爲跨學術領域的綜合大學 ， 其以法律學 、 社會學 、 哲學 、 國際關係 、 建築學 、 新聞學 、 社會科學 、 政治學 、 文學及教育學等見長 。']\n",
            "\n",
            "['每隊每輪上場隊員2名 （ 從第7屆開始增加到3名 ） 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2020年因電影重新上映 ， 票房上修爲 316.8 億日圓 。']\n",
            "['影評人羅傑 · 埃伯特形容艾倫爲 「 影壇瑰寶 」 （ a treasure of the cinema ） 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['中華人民共和國勞動人事部是中華人民共和國國務院曾經有的一個組成部門 ， 爲1982年將國家勞動總局 、 國家人事局 、 國務院科技幹部局 、 國家編制委員會合並而成 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['計算機協會 （ Association for Computing Machinery ， 簡稱ACM ） 是一個世界性的計算機從業員專業組織 ， 創立於1947年 ， 是世界上第一個科學性及教育性計算機學會 ， 亦是現時全球最大的電腦相關學會 。']['2004年中旬爲止 ， 世界上共有49個島國 ， 由於許多獨立與半獨立自治區都是位於島嶼之上且隨時都有可能改變政府狀態 ， 因此這數字很有機會因此而增減 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['塘湖站是一個京滬線上的鐵路車站 ， 位於山東省濟寧市微山縣塘湖鄉 ， 建於1942年 ， 目前爲四等站 ， 郵政編碼爲277603 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1945年4月12日 ， 羅斯福因腦溢血在佐治亞州逝世 ， 死後由當時的美國副總統哈里 · 杜魯門接任美國總統 。']['元素週期表中超過四分之三的化學元素都屬於金屬 ， 其中較爲一般人所知的有金 、 銀 、 銅 、 鐵 、 鋁 、 錫 、 鉛 、 鋅 、 鈉 、 鈣等 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['許世友出身貧苦農村 ， 少年時曾在少林寺出家並學習中國武術 ， 紅軍時期在紅四方面軍任職 ， 曾任紅四軍軍長 。']['大韓民國的行政區劃主要將全國劃分爲1個特別市 （ 首爾市 ） 、 1個特別自治市 （ 世宗市 ） 、 6個廣域市 、 8個道 （ 不含 「 以北五道 」 ） 、 以及1個 （ 濟州道 ） ， 這17個一級行政區稱爲 「 廣域地方自治團體 」 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['昭武九姓 ， 亦稱九姓胡 ， 中國南北朝 、 隋 、 唐時期對西域河中的月氏民族和國家 ， 以及其西遷至蔥嶺一帶後建立的一系列國家之統稱 ， 爲維吾爾族與回族等中國西北少數民族的族源之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在醫學教育上 ， 標準化病人可讓醫學院學生學習問診的技巧 ， 練習由病人口述的症狀判斷可能的疾病 ， 以及學習醫生和病人溝通的技巧 ， 臺灣也在2006年開始引入此一教學方式 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李賦寧  ， 男 ， 祖籍陝西蒲城 ， 生於江蘇南京 ， 中國教育家 、 翻譯家 。']\n",
            "['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['排卵 （ Ovulation ） 是女性卵巢內的卵泡破裂 ， 釋放次級卵母細胞的過程 ， 這個卵母細胞會離開卵巢 ， 由輸卵管進入子宮 ， 是女性月經週期的一部份 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['西 ， 方位上的西方 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['馬丁 · 路德 · 金恩遇刺案 （ 英文 ： Assassination of Martin Luther King , Jr. ） 是指1968年4月4日 ， 美國民權運動領袖馬丁 · 路德 · 金恩在美國田納西州孟菲斯旅館內遭槍擊亡故 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['紐約市則是美國人口最多的城市 。']\n",
            "Extracting evidence_list for the eval mode ...['小行星是由岩石或金屬組成 ， 圍繞著太陽運動的小天體 。']\n",
            "\n",
            "['元素週期表中超過四分之三的化學元素都屬於金屬 ， 其中較爲一般人所知的有金 、 銀 、 銅 、 鐵 、 鋁 、 錫 、 鉛 、 鋅 、 鈉 、 鈣等 。']Extracting evidence_list for the eval mode ...\n",
            "['《 穀梁傳 》 是 《 春秋穀梁傳 》 的簡稱 ， 是一部對 《 春秋 》 的註解 ， 與 《 左傳 》 、 《 公羊傳 》 同爲解說 《 春秋 》 的三傳之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['嫘祖是傳說中的北方部落首領黃帝軒轅氏的元妃 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['計算機協會 （ Association for Computing Machinery ， 簡稱ACM ） 是一個世界性的計算機從業員專業組織 ， 創立於1947年 ， 是世界上第一個科學性及教育性計算機學會 ， 亦是現時全球最大的電腦相關學會 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['地處銀川平原中部 ， 西倚賀蘭山 ， 東靠鄂爾多斯高原 。']\n",
            "\n",
            "['樂山大佛開鑿於唐代開元元年 （ 713年 ） ， 完成於貞元十九年 （ 803年 ） ， 先後歷經3位負責人 ， 歷時約九十年 。']Extracting evidence_list for the eval mode ...\n",
            "[\"拿督 （ Datuk , Dato ' ） ， 源自於古馬來語 ， 是一個常見於馬來西亞 、 印度尼西亞和汶萊的稱號 ， 乃是對有地位和崇高名望者的一種尊稱 。\"]\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['黏菌與卵菌 （ 水黴菌 ） 在歷史上曾因形態相似而歸屬真菌界 ， 但分子支序顯示它們與真菌的親緣關係甚遠 ， 期和真菌的相似性是趨同演化的結果 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...['“ 石器時代 ” 的稱呼被考古學家用來表示冶金時代以前的漫長時期 ， 這段時間約佔人類歷史的99 % 以上 ， 在這段時期中各種石器的使用遠比用其它 （ 更軟的 ） 材料所制的工具多 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['臺灣土地銀行 ， 簡稱土地銀行 、 土銀 ， 是一家臺灣大型銀行 ， 爲臺灣 「 八大行庫 」 之一 ， 總行位於臺北市中正區館前路 ， 資產在中華民國本國銀行排名第五 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['一行禪師  ， 俗名張遂 ， 法號敬賢 ， 號大慧禪師 ， 也稱爲沙門一行 、 一行阿闍梨 ， 唐人還呼爲一公 ， 魏州昌樂 （ 今河南省濮陽市南樂縣 ） 人 ， 唐朝比丘 、 天文學家 、 曆法學家 、 數學家 、 風水學家 。']['它是火星較小和較外側的已知衛星 ， 另一顆是火衛一 ， 火衛二與火星的距離是 23460 km ， 以 30.3 小時的週期環繞火星 ， 軌道速度爲每秒 1.35 公里 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']\n",
            "['墨家邏輯是中國古代第一個邏輯學體系 ， 全球三大古典邏輯體系之一 ， 主要以三物論爲代表 ， 三物分別爲故 、 理 、 類 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['表現主義 （ 法語 ： Expressionnisme ） 是20世紀初流行於法國 、 德國 、 奧地利 、 北歐和俄羅斯的文學和藝術流派 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['面向東方時 ， 北就在左邊 。']['鑽石 （ 古希臘語 ： ἀδάμας ； 法語 、 德語 ： Diamant ； Diamond ； Алмаз ） ， 爲五種樞要寶石的一種 ， 化學和工業應用中稱爲金剛石 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['在20世紀後期 ， 由於納米比亞本土的政治家起義以及政治代表尋求獨立 ， 導致聯合國在1966年承擔對西南非的直接責任 ， 但南非保持了事實上的統治 。']['女人於是被迷惑 ， 違背命令喫了果子 ， 女人喫了果子後又把果子給她的丈夫亞當喫 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['藥理學上 ， 藥物指用於預防 、 治療 、 診斷疾病或增強體格或改善精神狀態的化學物質 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['其代表作漫畫 《 丁丁歷險記 》 享譽全球 ， 至今在歐洲仍然不斷重版 ， 在中國也是二十世紀八十年代少數幾部能夠在商店中找到的外國連環畫之一 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['格但斯克 （ Gdańsk ； 西里西亞語 ： Dànzica ； 卡舒比語 ： Gduńsk ； Dantiscum ） ， 德語稱但澤 （ Danzig ） ， 是波蘭波美拉尼亞省的省會 ， 也是該國北部沿海地區的最大城市和最重要的海港 。']\n",
            "Extracting evidence_list for the eval mode ...['則是被動語態 。']\n",
            "['在他統治時期 ， 經過他與盧福瓦侯爵的努力建設下 ， 使法蘭西王國的陸軍常備軍接近四十萬 ， 幾乎與歐洲列強常備軍總和相當 ， 這些軍隊裝備了大量的燧發槍和中世紀早期卡賓槍使法蘭西陸軍在17世紀稱雄歐陸 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['納尼氏囊鰓鯰 ， 爲輻鰭魚綱鯰形目囊鰓鯰科的其中一種 ， 爲熱帶淡水魚 ， 分佈於亞洲孟加拉淡水流域 ， 體長可達 10.9 公分 ， 棲息在底層水域 ， 生活習性不明 。']Extracting evidence_list for the eval mode ...\n",
            "['依照印度佛教史而論概念內涵 ， 上座部與大衆部是因爲戒律議題的分歧而分裂 ， 不是因爲修行目標是要成佛或成阿羅漢的分歧而分裂 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['南京大學 ， 簡稱南大 ， 位於中國南京市 ， 該校歷史或可追溯至三國吳永安元年 （ 258年 ） ， 歷史上曾歷經多次變遷 ， 亦是中國第一所集教學和研究於一體的現代大學 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['人口總數約246萬 ， 在臺灣各都市排名第四 、 人口密度則位居第一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['國家的陸地面積約 234.5 萬平方公里 ， 是非洲第2大 （ 僅次於阿爾及利亞 ） 、 暨世界第11大的國家 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['他的父親是日本人 ， 母親是菲律賓人 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']['漫畫哆啦A夢的作者藤子 · F · 不二雄即出生於此 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['強弱危機分析 （ SWOT Analysis ） ， 又稱優劣分析法 、 SWOT分析法或道斯矩陣 ， 是一種企業競爭態勢分析方法 ， 是市場營銷的基礎分析方法之一 ， - { zh - hans : 通過 ; zh - hant : 透過 } - 評價自身的優勢 （ Strengths ） 、 劣勢 （ Weaknesses ） 、 外部競爭上的機會 （ Opportunities ） 和威脅 （ Threats ） ， 用以在制定發展戰略前對自身進行深入全面的分析以及競爭優勢的定位 。']['50年代 ， 即早期的環境監測主要採用分析化學的方法對污染物進行分析 ， 但由於環境污染物含量低 （ 通常是ppm或ppb級別 ） 、 變化快 ， 實際上是分析化學的發展 ， 被稱爲污染源監測階段 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['臺中市立臺中女子高級中等學校 ， 簡稱臺中女中 、 中女中 、 中女 ， 創立於1919年 ， 位於臺中市的一所普通型高級中等學校 ， 是臺中市唯二的兩所女子中學 ， 另一所爲曉明女中 。']['身爲一位浸信會牧師 ， 金在他職業生涯早期就已開始投入民權運動 ， 曾領導1955年聯合抵制蒙哥馬利公車運動 ， 並在1957年協助建立 （ SCLC ） 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['曾在三一學院 、 劍橋大學擔任哲學教授 ， 他寫了許多著作 ， 其中包括經典著作 《 西方哲學史 》 。']\n",
            "['2000年簽約英皇娛樂 ； 2002年與同公司的劉思惠及蔣雅文組成3T （ 英皇第二代三小花 ） 推出 《 少女蝶 》 合輯EP入行 ， 其後爲三人中最突出的一位 ， 於同年率先推出首張個人同名EP 《 Yumiko The Debut EP 》 ， 收錄其首本名曲 《 相對溼度 》 ； 翌年推出 《 舞吧!舞吧 ! 》']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['是杉崎由綺琉於1997年在角川書店 《 月刊Asuka 》 雜誌上連載的少女漫畫 ， 在2003年被改編爲電視動畫全26話 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['雄性雉雞的羽毛非常多樣 ， 從幾乎白色到幾乎黑色都有 ， 這是由於馴化及與綠雉雜交 ， 加上放生不同的亞種雜交的結果 ； 雌鳥則維持棕色 、 灰色的樣子 ， 在各個大陸的外貌差別不大 。']['屋大維死於公元14年 ， 享年75歲 ， 據推測是自然死亡 ， 但也有傳言聲稱他的妻子利維亞毒死了他 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['對已婚女性的尊稱 。']['研揚大樓  ， 爲國立臺灣科技大學公館校區之教學研究大樓 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['這種奇數的紀年法來自於耶穌紀元後 ， 其中的1年通常表示 “ 吾主之年 ” （ year of our lord ） ， 因此一世紀從公元1年到公元100年 ， 而二十世紀則從公元1901年到公元2000年 ， 因此2001年是二十一世紀的第一年 。']['沙丁魚 （ 學名 ： Sardina pilchardus ） ， 又稱薩丁魚 、 鰮 、 鰮和鰯 ， 是沙丁魚屬的唯一物種 ， 屬於鯡形目鯡科 ， 本種小者長二寸 ， 大者尺許 ， 下顎較上顎略長 ， 齒不顯 ， 背蒼腹白 ， 肉美 ， 多用來製爲沙丁魚罐頭 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['議會常用來指稱民主國家的立法機關 ， 由於其運作內容很大部分來自人民的意向 ， 因此亦被稱爲 「 民意機關 」 ； 而國家層級的議會 ， 被稱爲國家議會 ， 簡稱 「 國會 」 。']\n",
            "['果實 ， 是被子植物 （ 也稱開花植物 ） 花的部份組織衍生成的生殖器官 ， 通常在開花授粉之後 ， 以子房爲主體而形成 ， 其中包含有種子 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在柏拉圖的 《 對話 》 一書中記載了蘇格拉底在倫理學領域的貢獻 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['鯡形目 （ 學名 ： Clupeiformes ） 是脊索動物門輻鰭魚綱中的一個目 ， 其中包括許多在漁業上非常重要的成群魚類如鯡魚等 。']['隋文帝楊堅和文獻皇后獨孤伽羅的次子 ， 唐高祖的表弟 ， 是隋朝第二位皇帝 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['2021年12月 ， 成爲全球權威音樂雜誌 《 Rolling Stone 》 於1967年創辦至今 ， 首次爲歌手個人出版特刊的第一人 。']['現在一般認爲起源於酒神祭祀 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['淄博是溝通中原地區和山東半島的咽喉要道 ， 是山東省重要的交通樞紐城市 ， 也是 “ 齊國故都 ” 、 “ 陶瓷之都 ” 、 石油化工基地 、 全國文明城市 、 足球起源地 。']\n",
            "['畢業後應邀到哈佛大學任經濟學助理教授 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['小艾伯特 · 阿諾德 · “ 阿爾 ” · 戈爾 （ Albert Arnold \" Al \" Gore , Jr. ) ， 是一名美國政治家 ， 曾於1993年至2001年間在總統比爾 · 克林頓執政時期擔任副總統 。']\n",
            "['隨後安得拉邦則將政府機構移置阿馬拉瓦蒂 ， 但仍然視海得拉巴爲法定首府 。']Extracting evidence_list for the eval mode ...\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['臺灣總督府民政部  ， 是臺灣總督府轄下擔當行政與司法的部局 。']\n",
            "['因爲身爲家中唯一男丁的哥哥早逝 ， 基於延續馬家香火的想法 ， 將二姐的兒子過繼給未婚的馬世莉扶養 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['以發現者意大利解剖學家馬爾皮吉 （ Marcello Malpighi ） 命名 。']\n",
            "Extracting evidence_list for the eval mode ...['該校設有小學部及中學部 ， 俱只收男生 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['此後定居法國 ， 在1939年加入法國國籍 ， 1940年法國被納粹德國佔領 ， 康定斯基沒有選擇前往美國定居 ， 他在1944年逝於']\n",
            "['他的第一部小說 《 蘭貝斯的麗莎 》 ( 1897年 ) 首期銷售速度驚人 ， 以至於他放棄了醫學 ， 轉而全職寫作 。']\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "['曾在三一學院 、 劍橋大學擔任哲學教授 ， 他寫了許多著作 ， 其中包括經典著作 《 西方哲學史 》 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...['內燃機 （ Internal combustion engine ， 縮寫爲ICE ） 是熱機的一種 ， 能將燃料的化學能轉化動能 。']\n",
            "\n",
            "['慕容復 ， 金庸武俠小說 《 天龍八部 》 反派角色 ， 一心想復興燕國 ， 因此名爲慕容復 ， 爲金庸小說內武功絕頂的高手 ， 也被不少人視之實爲主角之一 。']Extracting evidence_list for the eval mode ...\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['藥物 （ drug ） 廣義上指可以對人或其他機體產生已知生物效應的物質 ， 主要是用以改變人類 、 動植物 、 微生物的生理功能和生化代謝 ， 達到治療 、 預防 、 診斷疾病的醫學目的 ， 少數被用來消遣娛樂 。']['謝長廷 （ 英語 ： Frank C.T Hsieh ) ， 中華民國政治人物 、 律師 ， 臺北大稻埕人 ， 現任臺北駐日經濟文化代表處代表 （ 法律上的官銜爲中華民國駐日本國特命全權大使 ） ， 曾任行政院院長 、 高雄市市長 、 立法委員及臺北市議員 ， 亦曾代表民進黨競選中華民國正副總統落敗 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['蔡元培25歲成進士 ， 被點翰林院庶吉士 ， 是中華民國首任教育總長 ， 1916年至1927年任北京大學校長 ， 革新北大 ， 開 「 學術 」 與 「 自由 」 之風 ； 1920年至1930年 ， 蔡元培同時兼任中法大學校長 。']['惠靈頓 （ Wellington 、 毛利語 ： Te Whanganui - a-Tara 或 Poneke ） 是新西蘭的首都 ， 位於新西蘭北島島的西南端 ， 庫克海峽和雷穆塔卡山脈之間 ， 它是新西蘭的第三大城市 ， 與悉尼和墨爾本一起成爲大洋洲的文化中心 。']\n",
            "\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在實踐中的重要性源於它把易於構造的NFA轉換成了更有效執行的DFA 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['他經常演一些不安的 、 煩躁的以及帶有神經質的角色 ， 並且堅持將這些角色與現實中的自己完全區分開 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "\n",
            "['諾維亞語 （ Novial ） 是一門國際輔助語言 ， 用於不同母語人群之間的交流 。']Extracting evidence_list for the eval mode ...\n",
            "['要評價競爭力 ， 需要確定一個比較競爭力的群體 ， 根據目標時間在競爭群體中的表現評價它 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['主要由最高車速 、 加速時間 、 最大坡度三方面指標來評價 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['京劇四大名旦 ， 一般而言是指民國十六年 （ 1927年 ） 由北京 《 順天時報 》 評選出的梅蘭芳 、 程硯秋 、 尚小云 、 荀慧生 、 徐碧雲五位著名的京劇旦角中的前四位 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['非營利組織 （ nonprofit organization ， 簡寫爲NPO ） 是指不以營利爲目的組織或團體 ， 其核心目標通常是支持或處理個人關心或者公衆關注的議題或事件 ， 因此其所涉及的領域非常廣 ， 從藝術 、 慈善 、 教育 、 政治 、 公共政策 、 宗教 、 學術 、 環保等 ， 分別擔任起彌補社會需求與政府供給間的落差 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['單晶片 ， 全稱 - { zh - cn : 單片微型計算機 ; zh - tw : 單晶片微電腦 } - （ single - chip microcomputer ） ， 又稱 - { 微控制器 } - 單元 （ microcontroller unit ） ， 是把中央處理器 、 存儲器 、 定時 / 計數器 （ timer / counter ） 、 各種輸入輸出接口等都集成在一塊 - { zh - hans : 集成電路 ; zh - hant : 積體電路 ; } - 芯片上的微型計算機 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['楊梅區  ， 舊稱 「 楊梅壢 」 ， 位於中華民國桃園市南部 ， 可分爲楊梅 、 埔心 、 富岡 、 高山頂四個區域 ， 人口約有 17.6 萬人 ， 並以客家人爲主要族群 ， 約佔全區人口數七成左右 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['維克多 · 馬裏 · 雨果 （ Victor Marie Hugo ， -LSB- viktɔʁ maʁi yɡo -RSB- ) ， 法國浪漫主義文學的代表人物和19世紀前期積極浪漫主義文學運動的領袖 ， 法國文學史上卓越的作家 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['早年參加學生運動 ， 後留學蘇聯莫斯科中山大學 ， 精通俄語與政治理論 ， 爲 “ 二十八個半布爾什維克 ” 之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在接連醜聞與巨大開銷下 ， 美國政府於2010年8月 ， 決定撤出伊拉克 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雖然有許多歐陸舞曲在北美洲造成轟動 ， 一般而言 ， 歐陸舞曲在北美洲比較不流行 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['汕頭大學 （ Shantou University ， 縮寫 ： STU ） ， 簡稱汕大 ， 位於中華人民共和國廣東省汕頭市 ， 是1981年經國務院批准成立的綜合性大學 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['這種纖維比頭髮稍粗 ， 這樣細的纖維要有折射率截然不同的雙重結構分佈 ， 是一個非常驚人的技術 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雙翅目 （ 學名 ： Diptera ） 包括蚊 、 蠅 、 虻等昆蟲 ， 包含200多個科的近16萬現生物種以及近4000種化石 ， 是昆蟲綱中居於鞘翅目 、 鱗翅目和膜翅目之後的第四大目 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['顯微鏡泛指將微小不可見或難見物品之影像放大 ， 而能被肉眼或其他成像儀器觀察之工具 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雄性和雌性從外表看不太出來 ， 只是雄性在身材方面顯得有力 ， 在行爲及表情上顯的較爲主動 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['查理曼於814年去世 ， 當了皇帝超過十三年 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['廣域地方自治團體以下爲二級行政區 ， 分爲屬於 「 基礎地方自治團體 」 的75個自治市 、 82個郡 、 15個 、 69個自治區 ， 以及不具地方自治團體身分的32個一般區 、 2個 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['大學是提供教學和研究條件 ， 授權頒發副學位和學位的高等教育機構 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['最早取材於唐代詩人元稹所寫的傳奇 《 會真記 》 （ 又名 《 鶯鶯傳 》 ） ， 後被元代王實甫改編爲雜劇 （ 此前有宋人趙令畤以此題材作 《 商調蝶戀花鼓子詞 》 ， 金人董解元作 《 西廂記諸宮調 》 ， 然影響力均遠不如雜劇 《 西廂記 》 ） ， 被稱爲 「 元雜劇的壓卷之作 」 ， 對中國的語言 、 文化等各個方面皆頗有影響 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['除了強調環境保護與自然生態保育 ， 在人爲飼養活體的態度也十分嚴謹 ， 不但獲得大量外匯和資訊優勢 ， 其動物保護法律管束 、 生命教育水準也是首屈一指的 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['雀百靈屬 （ 學名 ： Eremopterix ） 在生物分類學上是雀形目百靈科中的一個屬 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['天衛三 （ 緹坦妮雅 、 Titania 、 -LSB- link - entaɪˈteɪniə -RSB- ） 是天王星最大的衛星 ， 也是太陽系內第八大的衛星 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['花茶是利用茶善於吸收異味的特點 ， 將有香味的鮮花和新茶一起悶 ， 茶吸收香味後再把乾花篩除 ， 以此方法制成的茶葉 ， 並不是拿花瓣做成茶 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['由於曹魏盤踞中原 ， 人口爲三國當中最多 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['骨鰾類有6000多種魚 ， 世界上大部分淡水魚都屬於骨鰾類 ， 最大的達 4.5 米 ， 重達300千克 ， 最小的只有幾毫米 ， 過寄生生活 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['晉元帝司馬睿  ， 字景文 ， 東晉第一位皇帝 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['人氏是舊石器時期河套附近一個父系氏族 ， 他們以打獵爲生 ， 喫捕獲的獵物 ， 過着茹毛飲血的生活 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['新華網  ， 由新華社主辦 ， 是新華社的官方網站 ， 由北京總網和分佈於中國各地的30多個地方頻道及新華社的十多家子網站聯合組成 ， 屬世界範圍內最重要的中文新聞網站之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['托馬斯 · 索塞克 是捷克的一位足球運動員 ， 場上司職中場 ， 現在效力於英超球隊西漢姆聯 ， 亦是捷克國家足球隊成員之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['隋朝結束自魏晉南北朝以來的分裂局面 ， 奠定日後大唐盛世的基礎 ， 對中國歷史的意義重大 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['癌細胞只有一個簡單的定義 ： 不死的細胞 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['公元前1500年至前600年左右 ， 《 吠陀經 》 問世 ， 這是印歐語系諸民族中最爲古老的一部文學著作 ， 在其中 ， 印度神話初次較有系統組合起來 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['林鈺婷 Rita ， 臺灣新竹竹東客家人 ， 山狗大後生樂團團長及主唱 ， 也是電視廣播主持人及配音員等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['特產包括青梅 、 香蕉 、 甘蔗 、 凍頂烏龍茶 、 紹興酒 、 竹藝品 、 南投陶 、 花卉 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['其直徑約爲3公里 ， 軌道平均半徑爲 23,703 Mm ， 軌道週期爲 745.500 地球日 ， 與黃道間的軌道傾角爲153 ° （ 與木星赤道151 ° ） ， 運轉方向爲逆行 ， 軌道離心率爲 0.4077 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['鄭集  ， 男 ， 四川南溪人 ， 中國著名的生物化學家 、 教育家 ， 營養學家 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['披頭士樂隊 （ The Beatles ） 是1960年在利物浦組建的一支英國搖滾樂隊 ， 在華語地區亦稱爲 “ 甲殼蟲樂隊 ” 、 “ - { zh - hans : 披頭四樂隊 ; zh - tw : 披頭士樂隊 ; zh - hk : 披頭士樂隊 } - ” 等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['有同國家的操同一語言的人群承認屬於同一族群 ， 也有人更願意強調自己的國別 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['他在慶曆二年考中進士 ， 先後於江蘇 、 浙江 、 安徽 、 河南等地爲官 ， 這二十年中他廣泛地接觸了社會生活 ， 對社會上的各種問題有了比較深刻的認識 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李商隱  ， 字義山 ， 號玉谿生 、 樊南生 ， 祖籍隴西狄道 （ 今甘肅省臨洮縣 ） ， 祖輩遷滎陽 （ 今河南鄭州 ） ， 晚唐詩人 ， 和杜牧合稱 “ 小李杜 ” ， 與溫庭筠合稱爲 “ 溫李 ” ， 與同時期的段成式 、 溫庭筠風格相近 ， 且都在家族裏排行十六 ， 故並稱爲三十六體 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['整個套件包含Netscape Navigator （ 網頁瀏覽器 ） 、 Netscape Messenger （ 電子郵件客戶端軟體 ） 、 Netscape Collabra （ 新聞群組軟體 ） 、 Netscape Netcaster （ 資料收訊軟體 ） 、 Netscape Conference （ 線上會議軟體 ） 等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['妙義山是位於群馬縣下仁田町 、 富岡市 、 安中市的一座標高1104米的山峯 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['史蒂夫 · 保羅 · 賈伯斯 （ Steven Paul Jobs ) ， 通稱史蒂夫 · 賈伯斯 （ Steve Jobs ） ， 是一名美國發明家 、 企業家 、 - { zh - cn : 營銷家 ; zh - tw : 行銷師 } - ， 蘋果公司聯合創始人之一 ， 曾任董事長 、 行政總裁職位 ， NeXT創辦人及首席執行官 ， 也是彼思動畫創辦人並曾任行政總裁 ， 2006年爲華特迪士尼公司董事會成員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['清治末期 ， 基隆因航運地理位置優越 、 加上週邊有豐富的煤礦蘊藏 ， 清廷於1875年正式設治 、 並將原名 「 雞籠 」 更改爲 「 基隆 」 ， 開啓了都市發展的歷史 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['它利用的是多餘的處理器資源 ， 不影響用戶正常使用計算機 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['埃歐表面大部分的平原都被硫磺和二氧化硫的霜覆蓋著 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['埃德加 · 愛倫 · 坡 （ Edgar Allan Poe ； ） ， 美國作家 、 詩人 、 編輯與文學評論家 ， 被尊崇是美國浪漫主義運動要角之一 ， 以懸疑及驚悚小說最負盛名 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['大部分錫克教徒居住在印度旁遮普邦 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺灣台中市泰安服務區 ， 屬中山高速 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1996年樂團解散後 ， 徐太志隱退 ， 前往美國 ， 於兩年後回國 ， 開始個人演藝活動 ， 個人風格逐漸轉回早期參與Sinawe樂團時代的搖滾樂 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['高行健  ， 江西贛州出生 ， 法籍華裔劇作家 、 小說家 、 畫家 、 戲劇和電影導演 、 攝影家 ， 1980年代末前往歐洲 ， 現爲法國公民 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在人類的文化中 ， 家牛一般佔有很高的地位 ， 早期臺灣的農家子弟禁喫牛肉 ， 1949年的 《 印度憲法 》 與 《 尼泊爾憲法 》 甚至禁止宰牛 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['於1990年被英國女王伊麗莎白二世授予爵士稱號 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['亞伯拉罕 （ אַבְרָהָם ， 意爲 “ 多國之父 ” ； Abraham ； إبراهيم ） ， 原名作亞伯蘭 （ 希伯來語 ： אַבְרָם ） 或亞巴郎 （ Abram ， 意爲 “ 崇高之父 ” ） ， 是亞伯拉罕諸教 （ 猶太教 、 基督教和伊斯蘭教等宗教 ） 的先知 ， 是從地上衆生中所揀選並給予祝福的人 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2012年以韓國男子團體EXO及中國分隊EXO - M成員在中韓兩地出道 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['相傳 “ 神農嘗百草 ” 、 教人醫療與農耕 ， 中國人視之爲傳說中的農業和醫藥的發明者 、 守護神 ， 尊稱爲 「 藥王 」 、 「 五穀王 」 、 「 五穀先帝 」 、 「 神農大帝 」 等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['隋文帝的次子楊廣爭奪長子楊勇的太子位獲勝 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['屋大維死於公元14年 ， 享年75歲 ， 據推測是自然死亡 ， 但也有傳言聲稱他的妻子利維亞毒死了他 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺中市立啓明學校 （ Taichung Minicipal Taichung Special Education School for The Visually impaired ） 簡稱中明 ， 是臺灣台中市后里區一所專收視覺障礙生的公立特殊教育學校 ， 1968年獨立創校 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['SETI@home 官方2005年3月中旬發佈消息 ， 逐漸停止SETI Classic （ 即舊平臺 ） 的計算 ， 全面轉入BOINC計算平臺 ， 數據轉換預計在2個月之內完成 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['類星體的高解析影像 ， 特別是哈伯太空望遠鏡 ， 已經證明類星體是發生在星系的中心 ， 一些類星體的宿主星系是強烈的交互作用星系或合併中的星系 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['因爲國旗當中存在清真言 ， 所以索馬利蘭國旗同沙烏地阿拉伯國旗一樣 ， 也沒有降半旗的制度 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['核分裂時 ， 大部分的分裂中子均是一分裂就立即釋出 ， 稱爲瞬發中子 ， 少部分則在之後 （ 一至數十秒 ） 才釋出 ， 稱爲延遲中子 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['司馬光  ， 字君實 ， 號迂叟 ， 通稱司馬相公 ， 陝州夏縣涑水鄉 （ 今山西省夏縣 ） 人 ， 北宋政治家 、 文學家 、 史學家 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['馬克思主義政治經濟學 ， 也稱馬克思主義經濟學 ， 馬克思在其著作 《 資本論 》 中闡述了該學說的基本觀點 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['晶體是原子 、 離子或分子按照一定的週期性 ， 在結晶過程中 ， 在空間排列形成具有一定規則的幾何外形的固體 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在作爲 “ 黑馬 ” 贏得1844年的選舉後 ， 他是第一位沒有尋求連任而直接退休的總統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['桑吉巴蝴蝶魚 ， 爲輻鰭魚綱鱸形目蝴蝶魚科的其中一種 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['但即使到達爾文死後一百多年的今日 ， 寒武紀大爆發依舊是科學界的一大謎題 ， 尚待更多的科學證據出土 ， 也許就能窺見當時的實際情況 ， 找出真正的原因 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['象通稱大象 ， 是象科 （ 學名 ： Elephantidae ） 動物的通稱 ， 現存最大的陸生動物 ， 屬於長鼻目 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['新華網  ， 由新華社主辦 ， 是新華社的官方網站 ， 由北京總網和分佈於中國各地的30多個地方頻道及新華社的十多家子網站聯合組成 ， 屬世界範圍內最重要的中文新聞網站之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['明鄭爲臺灣歷史上第一個漢族 （ 閩南民系 ） 政權 ， 鄭成功登陸臺灣後將該地改名爲 「 東都 」 ， 仿照中國的郡縣制 ， 將赤崁地方更名 “ 東都明京 ” 設置承天府爲地方府治行政中心 ， 作爲臺灣最高的行政機構 ， 效仿明朝中央官制 ， 設六官 ； 府城分爲東安 、 西安 、 宋南 、 鎮北四坊 ， 各設首領 ， 管理事務 ； 府之下設二縣 ， 承天府以北叫天興縣 、 以南叫萬年縣 ， 同時各種民生工作也逐步展開 ， 例如 ： 查戶口 、 報田產 、 徵勞力 、 納稅銀等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2016年4月4日 ， 姚明與前NBA球星沙奎爾 · 奧尼爾和艾倫 · 艾弗森一同入選奈史密斯籃球名人紀念堂 ， 他也是首位入選也是迄今爲止唯一入選名人堂的亞洲球員 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['是在以前Uniform Grocery Product Code Council （ 統一商品碼理事會 ） 的基礎上重新組建的 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['懷化市 ， 別稱鶴城 、 五溪 ， 是中華人民共和國湖南省下轄的地級市 ， 位於湖南省西部 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['電動機與其它發動機 （ 比如熱機 、 液壓發動機 、 氣壓發動機和噴氣發動機等 ） 的原理差別在於能量轉換的方式不同 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['北歐人相傳每當雷雨交加時就是索爾乘坐馬車出來巡視 ， 因此稱呼索爾爲 “ 雷神 ” 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['以前西方人根本不相信是直接畫的 ， 但現在工藝已經公開 ， 製作場所可以參觀 ， 立即成爲世界著名的工藝品 ， 爲了適應各種欣賞習慣 ， 現在也有在內畫壺中繪製油畫的 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['但受王立軍事件影響 ， 2012年3月15日 ， 薄熙來被解除中共重慶市委書記職務 ， 同年4月10日被停止中共中央委員和政治局委員職務 ， 接受中共中央紀委調查 ， 並於同年9月28日被開除黨籍 、 公職並以其涉嫌犯罪問題及犯罪問題線索被移送司法機關處理 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1979年美麗島事件發生後遭到政府逮捕 ， 隔年軍事審判期間遭遇林宅血案 ， 母親和雙胞胎女兒遇害 ， 長女林奐均重傷 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['法國在經歷三十年戰爭與法西戰爭的洗禮後的海軍實力在他掌權初期僅不到20艘軍艦 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['李祖德 （ Tsu - Der Lee ) 生於臺北市 ， 畢業於臺北醫學大學牙醫學系 ， 由專業醫師轉爲經營者 ， 後又投身於創投業 、 當選北醫董事長 、 引進瑞士醫材產業等']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['而奧迪的四環標誌 ， 當中四個環分別代表了組成汽車聯盟 （ Auto Union ） 的四家汽車廠牌 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['近年來 ， 共和黨除了提倡保守的經濟政策外 ， 在社會議題上也偏保守 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['它利用的是多餘的處理器資源 ， 不影響用戶正常使用計算機 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['忒堤斯 （ 古希臘語 ： Τηθύς ） 希臘神話中的提坦神之一 ， 海神俄刻阿諾斯的姐姐與妻子 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['它是一種無色到淡黃色的液體具有類似於香茅油一樣的柑橘味 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['手工藝跟大批量生產的機械製造方式不同 ， 通常通過一定的藝術構思 ， 以手工作坊的方式加工製作 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['英格蘭教會  ， 也譯作英格蘭國教會 、 英國國教會 、 英國教會 、 英格蘭聖公會或英國聖公會 ， 是基督新教聖公宗的教會之一 ， 16世紀英格蘭宗教改革時期 ， 由英格蘭國王亨利八世領導 ， 由神學家托馬斯 · 克蘭麥 、 理查德 · 胡克等研究教義而開創的基督教會 ， 至今作爲英國英格蘭的國教 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['費雯 · 麗在電影 《 亂世佳人 》 中飾演女主角 ， 郝思嘉成爲影史上最難以超越的角色之一 ， 1998年她被美國電影學會遴選爲 「 AFI百年百大女明星 」 第16名 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['茶葉蛋 ， 又名茶蛋 、 鹽茶蛋 、 五香蛋 、 五香茶葉蛋 、 茶雞蛋 ， 是一種歷史悠久的蛋類料理 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['19世紀最後三十年裏 ， 它成爲法國藝術的主流 ， 並影響到整個歐美畫壇 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['近數十年來 ， 十大建設 、 鐵路電氣化 、 環島鐵路計劃 、 高速公路路網一一完工 ， 加上國內航空路網的建構 ， 以及各大城市軌道交通系統 、 南北高速鐵路的興築 ， 今日臺灣交通的面貌逐漸完成 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1965年因遭中華民國政府沒收其家族財產 ， 以及逮捕其家人入獄即將判處死刑的威脅之下 ， 廖於是向中華民國政府投降返臺 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['其最有名的著作 《 科學革命的結構 》 （ The Structure of Scientific Revolutions ， 1962年 ） ， 爲當代的科學思想研究建立了一個廣爲人知的討論基礎 ； 不論是贊成或是批評 ， 因此可以說是最有影響力的科學史及科學哲學家 ， 其著作也被引用到科學史之外的其他廣泛領域中 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['民國85年 （ 1996年 ） 2月1日 ， 奉臺灣省政府覈准 ， 正式籌備建校 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['肥胖的主要治療方式有飲食計劃和運動 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['此外 ， 當事人可能不會想在現實中實現這些幻想 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['停雲詩社爲臺灣古典詩社之一 ， 成立於1979年 ， 以國立臺灣師範大學國文學系教授爲主幹 ， 多爲外省籍人士 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['父親是努斯拉特 - 奧德 - 杜烏拉 · 菲魯茲 · 米爾扎 （ Nosrat - od - Dowleh Firouz Mirza ） ， 1859年出生 ， 1939年11月逝世 ， 享年80歲 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['後來天文學家西門 · 馬裏烏斯建議以希臘神話中神的斟酒者 、 宙斯的愛人蓋尼米德爲之命名 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['靜脈注射劑型的藥物 ， 在一分鐘內就會生效 ， 並持續半小時到一小時左右 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['其最大特點是排水設施完整 ， 採光 、 通風 、 防火設施完備 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['近代廣東得益於貿易經濟 ， 還有大量外省移民與南洋香料 ， 進一步提高了食物的豐富度 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['銀川市 ， 簡稱銀 ， 古稱中興路 、 興慶府 、 懷遠鎮 、 寧夏省城 ， 是中華人民共和國寧夏回族自治區首府 ， 位於寧夏中北部 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['分佈於中國大陸的西藏 、 雲南 、 四川等地 ， 生長於海拔 2,450 米至 3,500 米的地區 ， 多生長在高山針葉林中 、 灌叢中 、 巖壁縫隙及溝邊草地上 ， 目前尚未由人工引種栽培 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['手工藝 ， 指的是純手工製作的工藝 ， 用簡單的工具 ， 通常跟藝術有關 ， 其創作需要技巧 、 熟練度 ， 以及一定程度的美感 ， 媒材可能包括編織 、 陶藝 、 紙藝 、 繡縫 、 木作與其他 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['費茲傑羅被認爲是1920年代 「 迷惘的一代 」 的人 ， 他最著名的小說爲 《 大亨小傳 》 ， 此書堪稱美國社會縮影的經典代表 ， 描述1920年代美國人在歌舞昇平中空虛 、 享樂 、 矛盾的精神與思想 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['花旗銀行 （ Citibank , N.A. ， 2002年前香港曾稱之爲萬國寶通銀行 ） 是花旗集團屬下的一家零售銀行 ， 其主要前身是1812年6月16日成立的 「 紐約城市銀行 」 （ City Bank of New York ） ， 經過兩個世紀的發展 、 收購 ， 已經成爲美國以資產計第三大銀行 ， 也是一間在全球近一百六十個國家及地區設有分支機構的國際級銀行 ， 總部位於紐約市格林威治街390號 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['阿美族 （ 阿美語 ： Amis 、 Pangcah ） 是臺灣原住民的一個族群 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['現代晶體學研究主要通過分析晶體對各種電磁波束或粒子束的衍射圖像來進行 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['成龍是少數能夠在荷里活取得成功的中國演員 ， 他的多部電影如 《 火拼時速 》 系列都在美國獲得票房成功 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['奧迪公司 （ 英語和Audi AG ） 是一家德國汽車公司 ， 主要從事豪華汽車 、 超級跑車 、 大型重機的設計 、 研發 、 製造和銷售 ， 爲福斯集團全資的子公司 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['阿曼蘇丹國 （ سلطنة عُمان ） ， 簡稱阿曼  ， 是位於西南亞 ， 阿拉伯半島東南沿海的國家 ， 北部與阿拉伯聯合酋長國接壤 ， 西面毗鄰沙地阿拉伯 ， 西南靠近也門 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在人類的文化中 ， 家牛一般佔有很高的地位 ， 早期臺灣的農家子弟禁喫牛肉 ， 1949年的 《 印度憲法 》 與 《 尼泊爾憲法 》 甚至禁止宰牛 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['林鈺婷 Rita ， 臺灣新竹竹東客家人 ， 山狗大後生樂團團長及主唱 ， 也是電視廣播主持人及配音員等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2005年7月8日 ， 德國Verden市法院認定他製造震盪波蠕蟲 ， 四次改變數據和三次對計算機實施破壞有罪 ， 判處21個月的緩刑 ， 在緩刑期間必須完成30個小時的感化工作 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['洞裏薩湖 （ -LSB- ɓəŋ tɔnlei saːp -RSB- ； ， 漢字 ： 湖海 、 壺海 ） ， 別名金邊湖 ， 又譯洞 - { 裏 } - 湖 ， 中國史籍作淡洋 、 淡水洋 、 淡水湖 ， 華人稱之爲大魚湖 、 太湖 ， 是位於柬埔寨西北部的湖泊 ， 屬湄公河水系 ， 爲東南亞最大的淡水湖 ， 也是世界上最多樣化和最具生產力的生態系統之一 ， 具有極高的生物多樣性 ， 於1997年被聯合國教科文組織指定爲生物圈保護區 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2020年1月11日因臺聯在第10屆立委選舉僅獲 0.35 % 的政黨票 ， 排名落居第11位 ， 雖仍爲全國第六大黨 ， 劉一德請辭黨主席因未獲準而留任 ， 但預計進入二年的冬眠狀態 ， 使臺聯面臨泡沫化的危機 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['氰化鋰強烈水解生成氰化氫 ， 水溶液呈強鹼性 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['勇度 · 烏東塔  ， 或簡稱勇度  是一名出現於漫威漫畫出版的美國漫畫書中的虛構人物 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['臺中市立啓明學校 （ Taichung Minicipal Taichung Special Education School for The Visually impaired ） 簡稱中明 ， 是臺灣台中市后里區一所專收視覺障礙生的公立特殊教育學校 ， 1968年獨立創校 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['北航創辦於1952年10月25日 ， 原名北京航空學院 ， 地處海淀區學院路 ， 是20世紀50年代著名的八大學院之一 ； 如今北航在北京市昌平區的沙河高教園區建有分校區 ， 在北京市房山區竇店鎮以及青島市 、 合肥市 、 昆明市等地建有研究院 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['總統制又稱爲總統共和制 ， 是共和制政體的一種 ， 由行政首長領導一個獨立於立法部門的行政部門 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['與楊過之間四離四合 ， 生死相隨 ， 爲救楊過性命 ， 縱身跳下斷腸崖 ， 在谷底生活十六年 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1861年梵高開始接受教育 ， 在學習語言包括法語 、 德語及英語表現不錯 ， 但在1868年3月中斷學業 ， 並在1869年7月在國際藝術品交易商公司見習 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['北理工擁有19個學院 ， 近200個科研機構 ， 兩家校辦產業和一個產業園區北理工科技園 ， 更擁有5個國家級重點實驗室 ， 是歷史上曾爲首批副部級高校 ， 也是中國首批設立研究生院的高校之一 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['金馬獎 （ 英語譯名 ： Golden Horse Awards ， 簡寫爲GHA ） 是臺灣的電影獎 ， 也是歷史最悠久的華語電影獎 ， 爲華語電影業界最具影響力與代表性的電影獎之一 ， 有 「 華語的奧斯卡金像獎 」 的美譽 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['作爲英格蘭足球聯賽系統的組成部分 ， 英超的每支球隊均需與同級別的全部其它球隊進行主客場制對賽 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['天衛三覆蓋著無數直徑達326公里 （ 203 英里 ） 的撞擊坑 ， 但隕石坑的數量並不如天衛四多 ， 並表示天王星的五個衛星它可能經歷了早期的內源性表面重生事件 ， 抹去較舊的 、 嚴重坑坑窪窪的表面 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['卡爾 · 吉德 · 央斯基 （ Karl Guthe Jansky ) 是一位美國無線電工程師 ， 也是無線電天文學先驅 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['墨索里尼的屍體在1946年被法西斯主義支持者盜挖 ， 後被意大利政府扣押10年 ， 直到1957年才交還給他的遺孀雷切爾 · 墨索里尼 ， 並在他的故鄉普雷達皮奧下葬 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['三苗主要分佈於長江中下游一帶 ， 戰國時人認爲上古三苗部落位於古洞庭湖與鄱陽湖之間 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['2005年6月 ， 行政院原住民委員會爲了保護達悟族的飛魚節文化 ， 規定每年3 － 6月蘭嶼外海6海里內的海域禁止10噸以上漁船捕魚 ， 也不準使用流刺網 、 追逐網 、 毒魚 、 炸魚等破壞性捕魚手段 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['19世紀最後三十年裏 ， 它成爲法國藝術的主流 ， 並影響到整個歐美畫壇 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['三苗主要分佈於長江中下游一帶 ， 戰國時人認爲上古三苗部落位於古洞庭湖與鄱陽湖之間 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['莎士比亞在埃文河畔斯特拉特福出生長大 ， 18歲時與安妮 · 哈瑟維結婚 ， 兩人共生育了三個孩子 ： 蘇珊娜 、 雙胞胎哈姆內特和朱迪思 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['在費城做郵政多年後 ， 富蘭克林於1753年成爲殖民地郵政代理總長 ， 建立首個全國通訊系統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['本校是日治時期臺灣島內唯一升大學的管道 ， 入學競爭十分激烈 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['News世界大學排名中位居全球前十名 、 英國前五名 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['資興市位於中國湖南省南部 ， 爲郴州市代管縣級市 ； 1984年12月撤縣設市 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['； 此外 ， 由於min系符號之概念 ， 故配合複數使用時仍應維持min ， 不得加s 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['威廉 · 亨利 · 哈里森 （ William Henry Harrison ) 是美國軍官兼政治家 ， 曾於1841年當上第九任美國總統 ， 但就職僅31天就因病去世 ， 是首位任內逝世的美國總統 ， 也是任職時間最短的美國總統 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['白話字於1850年代由東南亞傳播至廈門 ， 由多位傳教士改良之 ， 當中以打馬字版本 （ 設計上以廈門話爲基礎 ） 最多人使用而成當今主流寫法 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['汾河 ， 也稱汾水 ， 山西人稱爲 “ 母親河 ” ， 是黃河的僅次於渭河的第二大支流 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['1912年4月10日 ， 鐵達尼號展開首航 ， 也是唯一一次的載客出航 ， 最終目的地爲紐約 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['麗臺科技是全球知名的電腦及智慧醫療研發製造商 、 NVIDIA長期合作伙伴 ， 以 「 研究創新 、 品質至上 」 爲不變的信念 ， 推出產品涵蓋GeForce顯示卡 、 Quadro專業繪圖卡 、 AI工作站 / 伺服器 、 AI管理軟體 、 桌面虛擬化Zero Client / Thin Client方案 、 智慧醫療 / 健康照護及大數據解決方案等 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['滯洪池 （ detention basin ） 是於河溪湖泊周邊 、 鄰接處或支流上開挖出的低窪區域 ， 平時的水位通常較低甚至時常乾涸 ， 功能是在暴雨來臨時將突增的地表逕流暫時儲存 ， 以控制洪水的蔓延速度 ， 降低尖峯流量對下游低勢地區所帶來的傷害 。']\n",
            "Extracting evidence_list for the eval mode ...\n",
            "['斯科特 · 布赫茲 （ Scott Buchholz ) 是一位澳洲政治人物 ， 他的黨籍是昆士蘭自由國家黨 。']\n"
          ]
        }
      ],
      "source": [
        "TEST_DATA = load_json(\"data/test_doc5sent5.jsonl\")\n",
        "TEST_PKL_FILE = Path(\"data/test_doc5sent5.pkl\")\n",
        "\n",
        "if not TEST_PKL_FILE.exists():\n",
        "    test_df = pd.DataFrame(TEST_DATA)\n",
        "    test_df = test_df.parallel_apply(partial(\n",
        "        join_with_topk_evidence,\n",
        "        mapping=mapping,\n",
        "        topk=EVIDENCE_TOPK,\n",
        "        mode=\"eval\",\n",
        "    ), axis=1)\n",
        "    # test_df = join_with_topk_evidence(\n",
        "    #     pd.DataFrame(TEST_DATA),\n",
        "    #     mapping,\n",
        "    #     mode=\"eval\",\n",
        "    #     topk=EVIDENCE_TOPK,\n",
        "    # )\n",
        "    test_df.to_pickle(TEST_PKL_FILE, protocol=4)\n",
        "else:\n",
        "    with open(TEST_PKL_FILE, \"rb\") as f:\n",
        "        test_df = pickle.load(f)\n",
        "\n",
        "test_dataset = AicupTopkEvidenceBERTDataset(\n",
        "    test_df,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=MAX_SEQ_LEN,\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tqIjlht8yCMA"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d66814d1e8cd4d679a14af48c0b474b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Predicting:   0%|          | 0/31 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ckpt_name = \"val_acc=0.5000_model.375.pt\"  #@param {type:\"string\"}\n",
        "model = load_model(model, ckpt_name, CKPT_DIR)\n",
        "predicted_label = run_predict(model, test_dataloader, device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Gl9I3ZWW4pHo"
      },
      "outputs": [],
      "source": [
        "predict_dataset = test_df.copy()\n",
        "predict_dataset[\"predicted_label\"] = list(map(ID2LABEL.get, predicted_label))\n",
        "predict_dataset[[\"id\", \"predicted_label\", \"predicted_evidence\"]].to_json(\n",
        "    f\"submission/{ckpt_name[:14]}_{OUTPUT_FILENAME}\",\n",
        "    orient=\"records\",\n",
        "    lines=True,\n",
        "    force_ascii=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
