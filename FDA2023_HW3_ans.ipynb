{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Document retrieval of the AICUP2023 competition\n",
    "The goal of this homework is to implement a document retrieval system for the AICUP2023 competition using the [`TF-IDF`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TF-IDF?\n",
    "- TF: Term Frequency, the occurring time ($n$) of `a term` $t$ appears in `a document` $d$.\n",
    "$$\\textrm{TF}_{t,d} =  \\frac{n_{t,d}}{\\sum_k n_{k,d}}$$\n",
    ", where $\\sum_k n_{k,d}$ indicates summation of all terms in the document $d$.\n",
    "- IDF: Inverse Document Frequency, how frequent `a term` $t$ appears in all documents.\n",
    "    - Document Frequency $\\textrm{DF}_t = \\log\\frac{|\\{d: t\\in d\\}|}{|D|}$, where $|\\{d: t\\in d\\}|$ is the number of documents that `a term` t appears, and $|D|$ indicates total number of documents.\n",
    "    - $\\Rightarrow \\textrm{IDF}_t = \\log\\frac{|D|}{|\\{d: t\\in d\\}|}$\n",
    "- $\\textrm{TFIDF}_{t,d} = \\textrm{TF}_{t,d} \\times \\textrm{IDF}_t$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can TF-IDF do?\n",
    "- TF-IDF can be used to transform a document or a sentence into a `vector`.\n",
    "- There are other ways to transform a document or a sentence into a vector, such as [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html), [`BERT`](https://huggingface.co/docs/transformers/training), [`Sentence-BERT`](https://github.com/UKPLab/sentence-transformers), etc.\n",
    "- **This homework only asks you to implement the TF-IDF method.** You can try other methods in your final project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of TF-IDF using `scikit-learn`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Use [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assume we have 4 documents in a list\n",
    "corpus = [\n",
    "    \"this is the first document\",\n",
    "    \"this document is the second document\",\n",
    "    \"and this is the third one\",\n",
    "    \"is this the first document\",\n",
    "]\n",
    "# Assume we want some important words in the vocabulary\n",
    "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"count\", CountVectorizer(vocabulary=vocabulary)),  # Get the count of each word\n",
    "        (\"tfidf\", TfidfTransformer(norm=None)),  # Get the tfidf of each word\n",
    "    ]\n",
    ").fit(corpus)\n",
    "\n",
    "# Print the count of each word\n",
    "print(pipe[\"count\"].transform(corpus).toarray())\n",
    ">>> [[1, 1, 1, 1, 0, 1, 0, 0], # count of the first document\n",
    "     [1, 2, 0, 1, 1, 1, 0, 0], # count of the second document\n",
    "     [1, 0, 0, 1, 0, 1, 1, 1], # count of the third document\n",
    "     [1, 1, 1, 1, 0, 1, 0, 0]] # count of the fourth document\n",
    "\n",
    "# Print the IDF values of each word\n",
    "# Notice!! The shape of the IDF values is (n_features, ).\n",
    "# n_features is the number of words in the vocabulary.\n",
    "print(pipe[\"tfidf\"].idf_)\n",
    ">>> [1.         1.22314355 1.51082562 1.         1.91629073 1.\n",
    " 1.91629073 1.91629073]\n",
    "\n",
    "# Print the TF-IDF values\n",
    "# Notice!! The shape of the TF-IDF values is (n_documents, n_features).\n",
    "print(pipe.transform(corpus).toarray())\n",
    ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
    "  0.         0.        ]\n",
    " [1.         2.4462871  0.         1.         1.91629073 1.\n",
    "  0.         0.        ]\n",
    " [1.         0.         0.         1.         0.         1.\n",
    "  1.91629073 1.91629073]\n",
    " [1.         1.22314355 1.51082562 1.         0.         1.\n",
    "  0.         0.        ]]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Use [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) alone\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assume we have 4 documents in a list\n",
    "corpus = [\n",
    "    \"this is the first document\",\n",
    "    \"this document is the second document\",\n",
    "    \"and this is the third one\",\n",
    "    \"is this the first document\",\n",
    "]\n",
    "# Assume we want some important words in the vocabulary\n",
    "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
    "vectorizer = TfidfVectorizer(\n",
    "    vocabulary=vocabulary,\n",
    "    norm=None, # without normalization\n",
    ")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
    "  0.         0.        ]\n",
    " [1.         2.4462871  0.         1.         1.91629073 1.\n",
    "  0.         0.        ]\n",
    " [1.         0.         0.         1.         0.         1.\n",
    "  1.91629073 1.91629073]\n",
    " [1.         1.22314355 1.51082562 1.         0.         1.\n",
    "  0.         0.        ]]\n",
    "```\n",
    "- Example 2 is the same as Example 1, but we use `TfidfVectorizer` instead of `CountVectorizer` + `TfidfTransformer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to transform a document or a sentence into a vector?\n",
    "- Because we can use the `cosine similarity` to measure the similarity between each sentence and a Wikipedia article.\n",
    "- In our AICUP2023 task, we can find the most similar Wikipedia article for each `claim`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Delivery policies\n",
    "|Rules | Punishment | Note |\n",
    "|-|-|-|\n",
    "| Name the folder as `FDA_HW3_F12345678` and zip it | -5| |\n",
    "| Include `requirements.txt` in the folder | -5 | |\n",
    "| Use ChatGPT to predict the answers (not allowed in HW3) | -20 | You can use ChatGPT to help coding work. |\n",
    "| Results cannot be reproduced by re-running | -10 | We will run your code! |\n",
    "|Explicit rule-based corrections (e.g. predictions[0] = [\"天衛三\"]) | -20 |  |\n",
    "|Copy and paste the code from your classmates | -20 |  |\n",
    "|Modify the `do-not-modify cell` | -20 |  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Scoring rules\n",
    "|Rules | Scores|\n",
    "|-|-|\n",
    "| Perform document retrieval with scikit-learn `TF-IDF` by finishing all # TODO (each for +16) | +80 |\n",
    "| Document retrieval Precision >= 0.25 and Recall >= 0.6 (on the training set) | +10 |\n",
    "| Document retrieval Precision >= 0.4 and Recall >= 0.7 (on the training set) | +10 |\n",
    "| (Bonus) Document retrieval Precision >= 0.5 and Recall >= 0.7 (on the training set) | +10 |\n",
    "| (Bonus) Document retrieval Precision >= 0.6 and Recall >= 0.7 (on the training set) | +10 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "1. Join the AICUP2023 competition. Link: https://tbrain.trendmicro.com.tw/Competitions/Details/28\n",
    "2. Download the training data and the Wikipedia data.\n",
    "3. (This code will need memory about 5.7 GB, tested on Google Colab.)\n",
    "\n",
    "### Folder structure\n",
    "- Please follow the following folder structure.\n",
    "```bash\n",
    "FDA_HW3_F12345678/\n",
    "│\n",
    "├── data/ \n",
    "│ ├── public_train.jsonl # Downloaded from the AICUP2023 page\n",
    "│ └── wiki-pages # Downloaded from the AICUP2023 page and zipped\n",
    "│\n",
    "├── FDA2023_HW3.ipynb # This file\n",
    "├── hw3_utils.py\n",
    "└── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you use Google Colab\n",
    "# !pip install pandarallel\n",
    "# !pip install TCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import scipy\n",
    "\n",
    "# jieba.set_dictionary(\"dict.txt.big\")\n",
    "# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "# Adjust the number of workers if you want\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=3)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas() # for progress_apply\n",
    "\n",
    "from hw3_utils import (\n",
    "    load_json,\n",
    "    jsonl_dir_to_df,\n",
    "    calculate_precision,\n",
    "    calculate_recall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stopwords\n",
    "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n",
    "from TCSP import read_stopwords_list\n",
    "\n",
    "stopwords = read_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, stopwords: list) -> str:\n",
    "    import jieba\n",
    "    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): claim or wikipedia article\n",
    "        stopwords (list): common words that contribute little to the meaning of a sentence\n",
    "\n",
    "    Returns:\n",
    "        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = jieba.cut(text)\n",
    "\n",
    "    return \" \".join([w for w in tokens if w not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_docs_sklearn(\n",
    "    claim: str,\n",
    "    tokenizing_method: callable,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
    "    wiki_pages: pd.DataFrame,\n",
    "    topk: int,\n",
    ") -> set:\n",
    "    import numpy as np\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    tokens = tokenizing_method(claim)\n",
    "    claim_vector = vectorizer.transform([tokens])\n",
    "    similarity_scores = cosine_similarity(tf_idf_matrix, claim_vector)\n",
    "\n",
    "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
    "    similarity_scores = similarity_scores[:, 0]  # flatten the array\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    topk_sorted_indices = sorted_indices[:topk]\n",
    "\n",
    "    # Get the wiki page names based on the topk sorted indices \n",
    "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
    "\n",
    "    exact_matchs = []\n",
    "    # You can find the following code in our AICUP2023 baseline.\n",
    "    # Basically, we check if a result is exactly mentioned in the claim.\n",
    "    for result in results:\n",
    "        if (\n",
    "            (result in claim)\n",
    "            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n",
    "            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
    "            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n",
    "        ):\n",
    "            exact_matchs.append(result)\n",
    "        elif \"·\" in result:\n",
    "            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
    "            for split in splitted:\n",
    "                if split in claim:\n",
    "                    exact_matchs.append(result)\n",
    "                    break\n",
    "\n",
    "    return set(exact_matchs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function (you don't need to modify this)\n",
    "\n",
    "def get_title_from_evidence(evidence):\n",
    "    titles = []\n",
    "    for evidence_set in evidence:\n",
    "        if len(evidence_set) == 4 and evidence_set[2] is None:\n",
    "            return [None]\n",
    "        for evidence_sent in evidence_set:\n",
    "            titles.append(evidence_sent[2])\n",
    "    return list(set(titles))\n",
    "\n",
    "\n",
    "def save_results_to_markdown(results: dict, output_file=\"grid_search_results.md\"):\n",
    "    file_exists = Path(output_file).exists()\n",
    "\n",
    "    with open(output_file, \"a\") as f:\n",
    "        if not file_exists:\n",
    "            f.write(\"# Grid Search Results\\n\\n\")\n",
    "            f.write(\"| Experiment  | F1 Score | Precision | Recall |\\n\")\n",
    "            f.write(\"| ----------- | -------- | --------- | ------ | \\n\")\n",
    "\n",
    "        exp_name = results[\"exp_name\"]\n",
    "        f1 = results[\"f1_score\"]\n",
    "        prec = results[\"precision\"]\n",
    "        recall = results[\"recall\"]\n",
    "        f.write(f\"| {exp_name} | {f1:.4f} | {prec:.4f} | {recall:.4f} |\\n\")\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "wiki_path = \"data/wiki-pages\"\n",
    "min_wiki_length = 10\n",
    "num_of_samples = 500\n",
    "topk = 15\n",
    "min_df = 2\n",
    "max_df = 0.8\n",
    "use_idf = True\n",
    "sublinear_tf = True\n",
    "\n",
    "# Set up the experiment name for logging\n",
    "exp_name = (\n",
    "    f\"len{min_wiki_length}_top{topk}_min_df={min_df}_\"\n",
    "    + f\"max_df={max_df}_{num_of_samples}s\"\n",
    ")\n",
    "if sublinear_tf:\n",
    "    exp_name = \"sublinearTF_\" + exp_name\n",
    "if not use_idf:\n",
    "    exp_name = \"no_idf_\" + exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time running this cell will 34 minutes using Google Colab.\n",
    "wiki_cache = \"wiki\"\n",
    "target_column = \"text\"\n",
    "\n",
    "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
    "if wiki_cache_path.exists():\n",
    "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
    "else:\n",
    "    # You need to download `wiki-pages.zip` from the AICUP website\n",
    "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
    "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
    "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
    "\n",
    "    # tokenize the text and keep the result in a new column `processed_text`\n",
    "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
    "        partial(tokenize, stopwords=stopwords)\n",
    "    )\n",
    "    # save the result to a pickle file\n",
    "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    use_idf=use_idf,\n",
    "    sublinear_tf=sublinear_tf,\n",
    "    dtype=np.float64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages = wiki_pages[\n",
    "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
    "]\n",
    "corpus = wiki_pages[\"processed_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# fit_transform will do the following two steps:\n",
    "# 1. fit: learn the vocabulary and idf from the corpus\n",
    "# 2. transform: transform the corpus into a vector space\n",
    "# Note the result is a sparse matrix, which contains lots of zeros for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_json(\"data/public_train.jsonl\")[:num_of_samples]\n",
    "train_df = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995170             緹坦妮雅\n",
      "186143      仲夏夜之夢_(消歧義)\n",
      "197396           尼克·博特姆\n",
      "995169              奧伯隆\n",
      "9661                天衛三\n",
      "122018               磁層\n",
      "1136853      仲夏夜_(羅文專輯)\n",
      "857790         托馬斯·卡拉布羅\n",
      "380707         仙后_(消歧義)\n",
      "367848           小行星685\n",
      "81583               磁層頂\n",
      "213220     結婚進行曲_(孟德爾頌)\n",
      "757629     仲夏夜之夢_(門德爾松)\n",
      "350491           小行星593\n",
      "860607         天王星軌道探測器\n",
      "Name: id, dtype: object\n",
      "995092       桑氏遠洋鳥\n",
      "203406          翼展\n",
      "200382       漂泊信天翁\n",
      "192384     南方皇家信天翁\n",
      "200171       特島信天翁\n",
      "8207          信天翁科\n",
      "625819       黑背信天翁\n",
      "816800       灰背信天翁\n",
      "315437       黑眉信天翁\n",
      "228700       黑腳信天翁\n",
      "959856    智慧_(信天翁)\n",
      "8205         大信天翁屬\n",
      "229551          黃雀\n",
      "228766          大鵟\n",
      "8202          白嘴潛鳥\n",
      "Name: id, dtype: object\n",
      "40469       F.I.R.飛兒樂團\n",
      "131632           梵谷的左耳\n",
      "396998      建寧街道_(曲靖市)\n",
      "568010     紀念日_(炎亞綸專輯)\n",
      "724663     Better_Life\n",
      "280117            Real\n",
      "201270        浮雲_(吉他手)\n",
      "131596              阿沁\n",
      "385750             宋雨哲\n",
      "287769      宇宙人_(臺灣樂團)\n",
      "92218              俞飛鴻\n",
      "875021             任晙赫\n",
      "188359          阿姆迪·法耶\n",
      "163168             陳建寧\n",
      "1037398       讓-皮埃爾·費伊\n",
      "Name: id, dtype: object\n",
      "57448                  新潟\n",
      "238113              狄龍灣機場\n",
      "113562               機場快鐵\n",
      "1059144         巴勒斯坦國機場列表\n",
      "1167507              大阪機場\n",
      "325815                機場站\n",
      "228164            塞舌爾國際機場\n",
      "226958          沙姆沙伊赫國際機場\n",
      "630896           卡拉斯科國際機場\n",
      "200939               萊斯機場\n",
      "226961           古爾代蓋國際機場\n",
      "230197           烏隆他尼國際機場\n",
      "229672           馬賽普羅旺斯機場\n",
      "286037              揚迪納機場\n",
      "285027     國際民航組織機場代碼_(E)\n",
      "Name: id, dtype: object\n",
      "496271      黨委書記和校長列入中央管理的高校\n",
      "371952                地方所屬高校\n",
      "37399              廣東省高等學校列表\n",
      "300052     中華人民共和國副省部級以上單位列表\n",
      "663483            高校畢業生自主創業證\n",
      "298703             雲南省高等學校列表\n",
      "911167             衡陽市高等學校列表\n",
      "298692                北方民族大學\n",
      "94897              湖北省高等學校列表\n",
      "359273                中央部屬高校\n",
      "94902              陝西省高等學校列表\n",
      "824881                    長大\n",
      "41710                 中央音樂學院\n",
      "315084              九州職業技術學院\n",
      "1169053               高等職業學校\n",
      "Name: id, dtype: object\n",
      "67674     南京大學附屬中學_(消歧義)\n",
      "7469                南大附中\n",
      "7468            南京大學附屬中學\n",
      "136331        中央民族大學附屬中學\n",
      "7464          南京師範大學附屬中學\n",
      "17395         師範大學附屬中學列表\n",
      "255166               馬毓泉\n",
      "458080               陳墓漾\n",
      "928064    南京航空航天大學附屬高級中學\n",
      "783189               南京街\n",
      "393222               天場鄉\n",
      "393228               陳濤鄉\n",
      "393225               振東鄉\n",
      "56874               交大附中\n",
      "78814              中央體育場\n",
      "Name: id, dtype: object\n",
      "325517        毒魚豆\n",
      "1170346       毒魚柿\n",
      "1016827      藤井善隆\n",
      "1109029      針刺麻醉\n",
      "132185     安的列斯群島\n",
      "605276         木荷\n",
      "1123842        謝榮\n",
      "1170322       山生柿\n",
      "82981      桑特海峽戰役\n",
      "170276      麻醉科醫師\n",
      "1170351      加蓬烏木\n",
      "1135019     吸入性麻醉\n",
      "351124       安靜街道\n",
      "205266         毒魚\n",
      "350459      丹·奧斯曼\n",
      "Name: id, dtype: object\n",
      "9976             軟件開發\n",
      "37115            需求獲取\n",
      "1021885        面向特徵編程\n",
      "126909          迭代式開發\n",
      "867956         Fossil\n",
      "21827            需求分析\n",
      "802037       系統發展生命週期\n",
      "59451           系統分析員\n",
      "1152             軟件工程\n",
      "1167416        軟件項目管理\n",
      "1152867     需求_(產品開發)\n",
      "864297       版本控制軟件比較\n",
      "583270           軟件文檔\n",
      "738608     應用程式生命週期管理\n",
      "734759         軟件需求說明\n",
      "Name: id, dtype: object\n",
      "928855     國立臺灣大學應用力學研究所\n",
      "702497               鮑亦興\n",
      "987455               吳光鍾\n",
      "987326               劉佩玲\n",
      "987872               林啓萬\n",
      "236112               郭宗德\n",
      "993035               周昌弘\n",
      "977571               吳大峻\n",
      "1077222               陳衛\n",
      "928802         張培仁_(力學家)\n",
      "1019698              謝立信\n",
      "350853               李世昌\n",
      "1019678              沈漁邨\n",
      "1077330              胡培松\n",
      "661609               何文壽\n",
      "Name: id, dtype: object\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Perform the prediction for document retrieval\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# If you use Google Colab, do not use parallel_apply due to the memory limit.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_df[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m\"\u001b[39;49m\u001b[39mclaim\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m      5\u001b[0m \u001b[39m# train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m     partial(\n\u001b[0;32m      7\u001b[0m         get_pred_docs_sklearn,\n\u001b[0;32m      8\u001b[0m         tokenizing_method\u001b[39m=\u001b[39;49mpartial(tokenize, stopwords\u001b[39m=\u001b[39;49mstopwords),\n\u001b[0;32m      9\u001b[0m         vectorizer\u001b[39m=\u001b[39;49mvectorizer,\n\u001b[0;32m     10\u001b[0m         tf_idf_matrix\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m     11\u001b[0m         wiki_pages\u001b[39m=\u001b[39;49mwiki_pages,\n\u001b[0;32m     12\u001b[0m         topk\u001b[39m=\u001b[39;49mtopk,\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mget_pred_docs_sklearn\u001b[1;34m(claim, tokenizing_method, vectorizer, tf_idf_matrix, wiki_pages, topk)\u001b[0m\n\u001b[0;32m     14\u001b[0m tokens \u001b[39m=\u001b[39m tokenizing_method(claim)\n\u001b[0;32m     15\u001b[0m claim_vector \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform([tokens])\n\u001b[1;32m---> 16\u001b[0m similarity_scores \u001b[39m=\u001b[39m cosine_similarity(tf_idf_matrix, claim_vector)\n\u001b[0;32m     18\u001b[0m \u001b[39m# `similarity_scores` shape: (num_wiki_pages x 1)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m similarity_scores \u001b[39m=\u001b[39m similarity_scores[:, \u001b[39m0\u001b[39m]  \u001b[39m# flatten the array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1395\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[39m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m   1393\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[1;32m-> 1395\u001b[0m X_normalized \u001b[39m=\u001b[39m normalize(X, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m Y:\n\u001b[0;32m   1397\u001b[0m     Y_normalized \u001b[39m=\u001b[39m X_normalized\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1817\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1815\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a supported axis\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m axis)\n\u001b[1;32m-> 1817\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1818\u001b[0m     X,\n\u001b[0;32m   1819\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49msparse_format,\n\u001b[0;32m   1820\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1821\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthe normalize function\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1822\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m   1823\u001b[0m )\n\u001b[0;32m   1824\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1825\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:845\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(array):\n\u001b[0;32m    844\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 845\u001b[0m     array \u001b[39m=\u001b[39m _ensure_sparse_format(\n\u001b[0;32m    846\u001b[0m         array,\n\u001b[0;32m    847\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    848\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    849\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    850\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    851\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    852\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    853\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    854\u001b[0m     )\n\u001b[0;32m    855\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     \u001b[39m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    858\u001b[0m     \u001b[39m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    859\u001b[0m     \u001b[39m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:552\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    549\u001b[0m     spmatrix \u001b[39m=\u001b[39m spmatrix\u001b[39m.\u001b[39mastype(dtype)\n\u001b[0;32m    550\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m changed_format:\n\u001b[0;32m    551\u001b[0m     \u001b[39m# force copy\u001b[39;00m\n\u001b[1;32m--> 552\u001b[0m     spmatrix \u001b[39m=\u001b[39m spmatrix\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m    554\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    555\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(spmatrix, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_data.py:92\u001b[0m, in \u001b[0;36m_data_matrix.copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_data(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcopy(), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Gaberil0903\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1230\u001b[0m, in \u001b[0;36m_cs_matrix._with_data\u001b[1;34m(self, data, copy)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[39m\"\"\"Returns a matrix with the same sparsity structure as self,\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[39mbut with different data.  By default the structure arrays\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m(i.e. .indptr and .indices) are copied.\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m-> 1230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m((data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mcopy(),\n\u001b[0;32m   1231\u001b[0m                            \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mcopy()),\n\u001b[0;32m   1232\u001b[0m                           shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape,\n\u001b[0;32m   1233\u001b[0m                           dtype\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   1234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m((data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr),\n\u001b[0;32m   1236\u001b[0m                           shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform the prediction for document retrieval\n",
    "# If you use Google Colab, do not use parallel_apply due to the memory limit.\n",
    "\n",
    "train_df[\"predictions\"] = train_df[\"claim\"].apply(\n",
    "# train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        tf_idf_matrix=X,\n",
    "        wiki_pages=wiki_pages,\n",
    "        topk=topk,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4978260044049519\n",
      "Recall: 0.6675438596491229\n",
      "Results saved to grid_search_results.md\n"
     ]
    }
   ],
   "source": [
    "precision = calculate_precision(train, train_df[\"predictions\"])\n",
    "recall = calculate_recall(train, train_df[\"predictions\"])\n",
    "results = {\n",
    "    \"exp_name\": exp_name,\n",
    "    \"f1_score\": 2.0 * precision * recall / (precision + recall),\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "}\n",
    "\n",
    "# This helps you to adjust the hyperparameters\n",
    "save_results_to_markdown(results, output_file=\"grid_search_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e61355a6e26410b95e2ec499821c2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=986), Label(value='0 / 986'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4779389682176204\n",
      "Recall: 0.6322995382031905\n"
     ]
    }
   ],
   "source": [
    "# Do not modify this cell.\n",
    "# This cell is for your scores on the training set.\n",
    "\n",
    "train = load_json(\"data/public_train.jsonl\")\n",
    "train_df = pd.DataFrame(train)\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
    "# Perform the prediction for document retrieval\n",
    "train_df[\"predictions\"] = train_df[\"claim\"].parallel_apply(\n",
    "    partial(\n",
    "        get_pred_docs_sklearn,\n",
    "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
    "        vectorizer=vectorizer,\n",
    "        tf_idf_matrix=X,\n",
    "        wiki_pages=wiki_pages,\n",
    "        topk=topk,\n",
    "    )\n",
    ")\n",
    "precision = calculate_precision(train, train_df[\"predictions\"])\n",
    "recall = calculate_recall(train, train_df[\"predictions\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58f38e4893e5afed24ba0416c084b8b56093319a21f96223eaba66586e962656"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
